@article{WOOTEN2019482,
title = "An application for streamlined and automated ENDF Cross Section Analysis and visualization (EXSAN)",
journal = "Annals of Nuclear Energy",
volume = "129",
pages = "482 - 486",
year = "2019",
issn = "0306-4549",
doi = "https://doi.org/10.1016/j.anucene.2019.01.053",
url = "http://www.sciencedirect.com/science/article/pii/S0306454919300659",
author = "H. Omar Wooten",
keywords = "ENDF, Nuclear data, Photoatomic data, Visualization, Cross section, Isotopes",
abstract = "This article introduces EXSAN, a new open-sourced code developed for streamlined visualization and analysis of Evaluated Nuclear Data File (ENDF)-formatted nuclear data libraries. In addition to viewing cross section data, EXSAN provides the user with an intuitive interface for automated downloading of ENDF data libraries and processing entire libraries with the nuclear cross section code NJOY. Furthermore, EXSAN includes the capability to query a library and obtain rank-ordered lists of reaction-specific cross sections by cross section value and isotope. EXSAN was developed in Python, and it is available on GitHub."
}
@article{DIAZBLANCO201794,
title = "Energy analytics in public buildings using interactive histograms",
journal = "Energy and Buildings",
volume = "134",
pages = "94 - 104",
year = "2017",
issn = "0378-7788",
doi = "https://doi.org/10.1016/j.enbuild.2016.10.026",
url = "http://www.sciencedirect.com/science/article/pii/S0378778816312439",
author = "Ignacio {Díaz Blanco} and Abel Alberto {Cuadrado Vega} and Daniel {Pérez López} and Manuel {Domínguez González} and Serafín {Alonso Castro} and Miguel Ángel {Prada Medrano}",
keywords = "Visual analytics, Energy efficiency, Multiway analysis, Data cube",
abstract = "In this paper we propose a visual analytics approach based on data cube methods to provide an insightful analysis of how energy is being used in a group of public buildings according to many different factors. The analysis is done by means of a web-based visual interface featuring “live” coordinated views – histograms – that show the distribution of demand data, according to different attributes, under different scenarios defined by user-driven filters on these attributes. We use the crossfilter.js library to achieve real-time computation of data cube aggregations for constantly changing user-defined filters, resulting in a fluid visualization of demand parameters (active power, power factor, total harmonic distorsion, etc.) aggregated according to many different factors or dimensions such as time (hour, day of week, month, etc.), building or environment (outside temperature)."
}
@article{GONZALEZ2017489,
title = "Novel remote monitoring platform for RES-hydrogen based smart microgrid",
journal = "Energy Conversion and Management",
volume = "148",
pages = "489 - 505",
year = "2017",
issn = "0196-8904",
doi = "https://doi.org/10.1016/j.enconman.2017.06.031",
url = "http://www.sciencedirect.com/science/article/pii/S0196890417305757",
author = "I. González and A.J. Calderón and J.M. Andújar",
keywords = "Smart microgrid, Smart grid, Renewable energy sources, Hydrogen, Remote monitoring, Easy Java/Javascript Simulations",
abstract = "In the context of the future power grids – Smart Grids (SGs) – Smart MicroGrids (SMGs) play a paramount role. These ones are very specific portions of the SGs that deal with integration of small-rated distributed energy and storage resources closer to the loads – chiefly within the distribution domain. Data acquisition and monitoring tasks are vital functions that must be developed at every stage of the grid for a proper operation. This paper presents a remote monitoring platform (RMP) to monitor an experimental SMG. It integrates Renewable Energy Sources (RESs) (solar and wind) and hydrogen to operate in isolated regime. The RMP has been developed using the open-source authoring tool Easy Java/Javascript Simulations (EJsS). The interface has been designed to be intuitive and easy-to-use, providing real-time information of all the involved magnitudes over the network. Scalability, easy development, portability and cost effective are the main features of the proposed framework. The microgrid and the proposed monitoring platform are described and the successful results are reported. The remote user executes a ready-to-use file with low computational requirements and is enabled to graphically and numerically track the SMG behaviour. These results prove the suitability of the RMP as an effective means for continuous visualization of the coordinated energy flows of a real SMG."
}
@article{CHIN2018480,
title = "A Visual Analytics Platform and Advanced Visualization Tools for Interpreting and Analyzing Wind Energy Time-Series Data",
journal = "IFAC-PapersOnLine",
volume = "51",
number = "28",
pages = "480 - 485",
year = "2018",
note = "10th IFAC Symposium on Control of Power and Energy Systems CPES 2018",
issn = "2405-8963",
doi = "https://doi.org/10.1016/j.ifacol.2018.11.749",
url = "http://www.sciencedirect.com/science/article/pii/S2405896318334700",
author = "George Chin and Yousu Chen and Erin Fitzhenry and Blaine McGary and Meg Pirrung and Joe Bruce and Scott Winner",
keywords = "Wind, renewable energy systems, operators, visual pattern recognition, human-machine interface",
abstract = "The Bonneville Power Administration (BPA) connects nearly 5,100 MW of wind energy to its transmission system through dozens of wind energy projects located in the states of Oregon and Washington. These wind energy projects are producing large amounts of real-time data that need to be efficiently monitored and analyzed by BPA operators in order to combine and balance this variable source of energy with other energy resources to meet energy demands. To support wind energy monitoring, operations, and integration, visual analytics researchers at the Pacific Northwest National Laboratory have developed a visual analytics platform that is able to connect to existing wind operations data sources, fuse and integrate wind energy data, and efficiently serve integrated wind energy data to advanced web-based visualization tools. In addition, the visual analytics team has also developed a set of advanced visualization tools that are available through the visual analytics platform to directly support wind energy operations and analysis needs."
}
@article{BUCHER201943,
title = "From location tracking to personalized eco-feedback: A framework for geographic information collection, processing and visualization to promote sustainable mobility behaviors",
journal = "Travel Behaviour and Society",
volume = "14",
pages = "43 - 56",
year = "2019",
issn = "2214-367X",
doi = "https://doi.org/10.1016/j.tbs.2018.09.005",
url = "http://www.sciencedirect.com/science/article/pii/S2214367X18300887",
author = "Dominik Bucher and Francesca Mangili and Francesca Cellina and Claudio Bonesana and David Jonietz and Martin Raubal",
keywords = "Mobility tracking, Mode detection, Trajectory clustering, Eco-feedback, Sustainability",
abstract = "Nowadays, most people carry around a powerful smartphone which is well suited to constantly monitor the location and sometimes even the activity of its user. This makes tracking prevalent and leads to a large number of projects concerned with trajectory data. One area of particular interest is transport and mobility, where data is important for urban planning and smart city-related activities, but can also be used to provide individual users with feedback and suggestions for personal behavior change. As part of a large-scale study based in Switzerland, we use activity tracking data to provide people with eco-feedback on their own mobility patterns and stimulate them to adopt more energy-efficient mobility choices. In this paper we explore the opportunities offered by smartphone based activity tracking, propose a general framework to exploit location data to foster more sustainable mobility behavior, describe the technical solutions chosen and discuss a range of outcomes in terms of user perception and sustainability potential. The presented approach extracts mobility patterns from users’ trajectories, computes credible alternative transport options, and presents the results in a concise and clear way. The resulting eco-feedback helps people to understand their mobility choices, discover the most non-ecological parts of their travel behavior, and explore feasible alternatives."
}
@article{EWINGS2016132,
title = "Horace: Software for the analysis of data from single crystal spectroscopy experiments at time-of-flight neutron instruments",
journal = "Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment",
volume = "834",
pages = "132 - 142",
year = "2016",
issn = "0168-9002",
doi = "https://doi.org/10.1016/j.nima.2016.07.036",
url = "http://www.sciencedirect.com/science/article/pii/S016890021630777X",
author = "R.A. Ewings and A. Buts and M.D. Le and J. {van Duijn} and I. Bustinduy and T.G. Perring",
keywords = "Time-of-flight, Neutron spectroscopy, Multidimensional visualization",
abstract = "The Horace suite of programs has been developed to work with large multiple-measurement data sets collected from time-of-flight neutron spectrometers equipped with arrays of position-sensitive detectors. The software allows exploratory studies of the four dimensions of reciprocal space and excitation energy to be undertaken, enabling multi-dimensional subsets to be visualized, algebraically manipulated, and models for the scattering to simulated or fitted to the data. The software is designed to be an extensible framework, thus allowing user-customized operations to be performed on the data. Examples of the use of its features are given for measurements exploring the spin waves of the simple antiferromagnet RbMnF3 and ferromagnetic iron, and the phonons in URu2Si2."
}
@article{BENITEZ2016517,
title = "Dynamic clustering of residential electricity consumption time series data based on Hausdorff distance",
journal = "Electric Power Systems Research",
volume = "140",
pages = "517 - 526",
year = "2016",
issn = "0378-7796",
doi = "https://doi.org/10.1016/j.epsr.2016.05.023",
url = "http://www.sciencedirect.com/science/article/pii/S0378779616301754",
author = "Ignacio Benítez and José-Luis Díez and Alfredo Quijano and Ignacio Delgado",
keywords = "Dynamic clustering, Data mining, Load profiles",
abstract = "As the analysis of electrical loads is reaching data measured from low voltage power distribution networks, there is a need for the main agents involved in the operation and management of the power grids to segment the end users as a function of their shapes of daily energy consumption or load profiles, and to obtain patterns that allow to classify the users in groups based on how they consume the energy. However, this analysis is usually limited to the analysis of single days. Since the smart metering data are time series formed by sequential measurements of energy through each hour or quarter of hour of the day, and also through each day, thanks to the implementation of Advanced Metering Infrastructure (AMI) and the Smart Grid technologies, it becomes clear that the analysis of the data needs to be extended to consider the dynamic evolution of the consumption patterns through days, weeks, months, seasons, and even years. This is the objective of the present work. A new framework is presented that addresses the dynamic clustering, visualization and identification of temporal patterns in load profiles time series, fulfilling the detected gap in this area. The present development is a generic framework that allows the clustering and visualization of load profiles time series applying different classical clustering algorithms. A novel dynamic clustering algorithm is also presented, based on an initial segmentation of the energy consumption time series data in smaller surfaces, and the computation of a similarity measure among them applying the Hausdorff distance. Following, these developments are presented and tested on two dataset of energy consumption load profiles from a sample of residential users in Spain and London."
}
@article{LIU2017100,
title = "HybridVis: An adaptive hybrid-scale visualization of multivariate graphs",
journal = "Journal of Visual Languages & Computing",
volume = "41",
pages = "100 - 110",
year = "2017",
issn = "1045-926X",
doi = "https://doi.org/10.1016/j.jvlc.2017.03.008",
url = "http://www.sciencedirect.com/science/article/pii/S1045926X16301392",
author = "Yuhua Liu and Changbo Wang and Peng Ye and Kang Zhang",
keywords = "Clustering, Graph/network data, Hierarchy data, Focus + context techniques",
abstract = "Existing network visualizations support hierarchical exploration, which rely on user interactions to create and modify graph hierarchies based on the patterns in the data attributes. It will take a relatively long time for users to identify the impact of different attributes on the cluster structure. To address this problem, this paper proposes a visual analytical approach, called HybridVis, creating an interactive layout to reveal clusters of obvious characteristics on one or more attributes at different scales. HybridVis can help people gain social insight and better understand the roles of attributes within a cluster. First, an approximate optimal graph hierarchy based on an energy model is created, considering both data attributes and relationships among data items. Then a layout algorithm and a level-dependent perceptual view for multi-scale graphs are proposed to show the attribute-driven graph hierarchy. Several views, which interact with each other, are designed in HybridVis, including a graphical view of the relationships among clusters; a cluster tree revealing the cluster scales and the details of attributes on parallel coordinates augmented with histograms and interactions. From the meaningful and globally approximate optimal abstraction, users can navigate a large multivariate graph with an overview+detail to explore and rapidly find the potential correlations between the graph structure and the attributes of data items. Finally, experiments using two real world data sets are performed to demonstrate the effectiveness of our methods."
}
@article{HUTTON20122087,
title = "Experiment archive, analysis, and visualization at the National Ignition Facility",
journal = "Fusion Engineering and Design",
volume = "87",
number = "12",
pages = "2087 - 2091",
year = "2012",
note = "Proceedings of the 8th IAEA Technical Meeting on Control, Data Acquisition, and Remote Participation for Fusion Research",
issn = "0920-3796",
doi = "https://doi.org/10.1016/j.fusengdes.2012.07.009",
url = "http://www.sciencedirect.com/science/article/pii/S0920379612003274",
author = "Matthew S. Hutton and Stephen Azevedo and Richard Beeler and Rita Bettenhausen and Essex Bond and Allan Casey and Judith Liebman and Amber Marsh and Thomas Pannell and Abbie Warrick",
keywords = "NIF, National Ignition Facility, Scientific archive, Web-based visualization, Automated analysis, Dashboards",
abstract = "The National Ignition Facility (NIF) at the Lawrence Livermore National Laboratory is the world's most energetic laser, providing a scientific research center to study inertial confinement fusion and matter at extreme energy densities and pressures. A target shot involves over 30 specialized diagnostics measuring critical x-ray, optical and nuclear phenomena to quantify ignition results for comparison with computational models. The Shot Analysis and Visualization System (SAVI) acquires and analyzes target diagnostic data for display within a time-budget of 30min. Laser and target diagnostic data are automatically loaded into the NIF archive database through clustered software data collection agents. The SAVI Analysis Engine distributes signal and image processing tasks to a Linux cluster where computation is performed. Intermediate results are archived at each step of the analysis pipeline. Data is archived with metadata and pedigree. Experiment results are visualized through a web-based user interface in interactive dashboards tailored to single or multiple shot perspectives. The SAVI system integrates open-source software, commercial workflow tools, relational database and messaging technologies into a service-oriented and distributed software architecture that is highly parallel, scalable, and flexible. The architecture and functionality of the SAVI system will be presented along with examples."
}
@article{FIASCHETTI201867,
title = "Monitoring and controlling energy distribution: Implementation of a distribution management system based on Common Information Model",
journal = "International Journal of Electrical Power & Energy Systems",
volume = "94",
pages = "67 - 76",
year = "2018",
issn = "0142-0615",
doi = "https://doi.org/10.1016/j.ijepes.2017.06.029",
url = "http://www.sciencedirect.com/science/article/pii/S014206151730090X",
author = "L. Fiaschetti and M. Antunez and E. Trapani and L. Valenzuela and A. Rubiales and M. Risso and G. Boroni",
keywords = "Open architecture design, Common Information Model, Smart grid, Electrical engineering, Distribution management system, Computer-aided engineering, Power system metering",
abstract = "The management of renewable and distributed energy resources is changing the way in which electric distribution systems are being operated. Many companies commonly create several custom system solutions to manage power distribution. The functions provided by these systems are network visualization, state estimation, system control and data acquisition, among others. Despite their relevance, many mid-size or small distribution companies have problems to install one of these solutions because they are either very expensive, incompatible, or limited by difficulties in data exchange. As a possible solution to this problem, this paper presents the development of a Distribution Management System (DMS) based on open source technology. This system combines four main components: OpenDSS-framework, ActiveMQ-broker, applications for visualizing and editing electrical network, and Common Information Model (CIM). The tool is part of Tenergia R&D project 1This work is financed by R&D projects FITS 2013 – UREE10 – FONARSEC – Ministry of Science, Technology and Productive Innovation of Argentina, and PIDDEF 36/14 – Ministry of Defense of Argentina.1 and has a collection of applications designed to monitor and control the distribution networks located in Argentina."
}
@article{RUIZ2020101315,
title = "A case study on understanding energy consumption through prediction and visualization (VIMOEN)",
journal = "Journal of Building Engineering",
volume = "30",
pages = "101315",
year = "2020",
issn = "2352-7102",
doi = "https://doi.org/10.1016/j.jobe.2020.101315",
url = "http://www.sciencedirect.com/science/article/pii/S2352710219317978",
author = "L.G.B. Ruiz and M.C. Pegalajar and M. Molina-Solana and Yi-Ke Guo",
keywords = "Energy efficiency, Energy forecasting, Visualization, Mapbox, Energy monitoring",
abstract = "Energy efficiency has emerged as an overarching concern due to the high pollution and cost associated with operating heating, ventilation and air-conditioning systems in buildings, which are an essential part of our day to day life. Besides, energy monitoring becomes one of the most important research topics nowadays as it enables us the possibility of understanding the consumption of the facilities. This, along with energy forecasting, represents a very decisive task for energy efficiency. The goal of this study is divided into two parts. First to provide a methodology to predict energy usage every hour. To do so, several Machine Learning technologies were analysed: Trees, Support Vector Machines and Neural Networks. Besides, as the University of Granada lacks a tool to properly monitoring those data, a second aim is to propose an intelligent system to visualize and to use those models in order to predict energy consumption in real-time. To this end, we designed VIMOEN (VIsual MOnitoring of ENergy), a web-based application to provide not only visual information about the energy consumption of a set of geographically-distributed buildings but also expected expenditures in the near future. The system has been designed to be easy-to-use and intuitive for non-expert users. Our system was validated on data coming from buildings of the UGR and the experiments show that the Elman Neural Networks proved to be the most accurate and stable model and since the 5th hour the results maintain accuracy."
}
@article{CARNEIRO2019330,
title = "Influencing occupant's choices by using spatiotemporal information visualization in Immersive Virtual Environments",
journal = "Building and Environment",
volume = "150",
pages = "330 - 338",
year = "2019",
issn = "0360-1323",
doi = "https://doi.org/10.1016/j.buildenv.2019.01.024",
url = "http://www.sciencedirect.com/science/article/pii/S0360132319300320",
author = "Joao P. Carneiro and Ashrant Aryal and Burcin Becerik-Gerber",
keywords = "Information visualization, Spatiotemporal visualization, Immersive visualization, Occupant behavior, Feedback systems",
abstract = "Feedback systems are useful to increase occupant awareness and disrupt their wasteful behaviors. The study presented in this paper explores the opportunity for feedback system designers to use information visualization to guide occupants to a desired option without reducing the occupant's control over the environment. In contrast to previous studies, we utilize spatiotemporal information visualization, that not only contradicts with occupants' initial choices but also presents the consequences of their choices. The information visualization is presented in a 1 to 1 scale, where the visualization coexists with the occupants over the office space in an immersive virtual environment. In total, 80 participants' data were collected. Two visualizations were presented to the participants, one relating to energy consumption and another relating to light distribution in the room. The results show that the information visualizations made the participants reconsider their choices. Our results also show that the energy consumption information visualization was more effective on guiding the participants towards an option that supported by the visualization, compared to the visualization that focused on room lighting. The results support the idea of using information visualization to drive occupant behavior towards desired outcomes and could help in the design of future feedback systems targeted at influencing occupant behavior."
}
@article{VANSCHIJNDEL2018189,
title = "Mapping future energy demands for European museums",
journal = "Journal of Cultural Heritage",
volume = "31",
pages = "189 - 201",
year = "2018",
issn = "1296-2074",
doi = "https://doi.org/10.1016/j.culher.2017.11.013",
url = "http://www.sciencedirect.com/science/article/pii/S1296207416303788",
author = "A.W.M.J. {van Schijndel} and H.L.H. Schellen",
keywords = "Map, Museum, Energy, Simulation, Future, Climate change",
abstract = "In this paper we present a methodology for simulating and mapping energy needs for European museums for the recent past, near future 2021–2050 and far future 2071–2100. This approach consists of four recent developments: Firstly, climates files. The availability of the hourly based, European, external future A1B climate data from the European FP7 Climate for Culture project. Secondly, classification. An energy-based classification system of museums based on the quality of the building envelope and systems, representing wide ranges of museums. The latter consisted of 16 different museums equal to all combinations of four levels of building construction and four levels of climate control. Thirdly, building performance simulation. A multi-zone building energy and indoor climate model with a proved record of simulating a wide range of buildings, including museums. Seven performance indicators were used: The mean indoor temperature; the mean indoor relative humidity; the mean heating demand; the mean cooling demand; the mean humidification demand; the mean dehumidification demand; the total energy demand. Fourthly, a mapping tool. We used a mapping tool to produce European maps for sixteen museum types and five 30-year periods: recent past, near future, far future, near future minus recent past and far future minus recent past. The most important mapping results are included and discussed in this paper."
}
@article{IOANNIDIS201657,
title = "Occupancy driven building performance assessment",
journal = "Journal of Innovation in Digital Ecosystems",
volume = "3",
number = "2",
pages = "57 - 69",
year = "2016",
issn = "2352-6645",
doi = "https://doi.org/10.1016/j.jides.2016.10.008",
url = "http://www.sciencedirect.com/science/article/pii/S2352664516300219",
author = "Dimosthenis Ioannidis and Pantelis Tropios and Stelios Krinidis and George Stavropoulos and Dimitrios Tzovaras and Spiridon Likothanasis",
keywords = "Building performance assessment, Big data analytics, Visual analytics, Human presence, Occupancy extraction, Building occupancy visualization",
abstract = "In this paper, we focus on the building performance assessment using big data and visual analytics techniques driven by building occupancy. Building occupancy is a paramount factor in building performance, specifically lighting, plug loads and HVAC equipment utilization. Extrapolation of patterns from big data sets, which consist of building information, energy consumption, environmental measurements and namely occupancy information, is a powerful analysis technique to extract useful semantic information about building performance. To this end, visual analytics techniques are exploited to visualize them in a compact and comprehensive way taking into account properties of human cognition, perception and sense making. Visual Analytics facilitates the detailed spatiotemporal analysis building performance in terms of occupancy comfort, building performance and energy consumption and exploits innovative data mining techniques and mechanisms to allow analysts to detect patterns and crucial point that are difficult to be detected otherwise, thus assisting them to further optimize the building’s operation. The presented tool has been tested on real data information acquired from a building located at southern Europe demonstrating its effectiveness and its usability for building managers."
}
@article{XIAO2020110228,
title = "Unsupervised learning for feature projection: Extracting patterns from multidimensional building measurements",
journal = "Energy and Buildings",
volume = "224",
pages = "110228",
year = "2020",
issn = "0378-7788",
doi = "https://doi.org/10.1016/j.enbuild.2020.110228",
url = "http://www.sciencedirect.com/science/article/pii/S0378778820304412",
author = "Chunze Xiao and Fazel Khayatian and Giuliano Dall'O'",
keywords = "Unsupervised learning, Building performance, Dimensionality reduction, Data representation",
abstract = "Data visualization is an important resource for decision makers to obtain information from large datasets. Based on the data obtained from either predictions or measurements, different strategies are combined and tested to reduce the energy demand, whilst keeping the indoor comfort at suitable level. Although the information expressed from data representation can significantly influence the decisions, little research has focused on extracting features from building measurements. This paper provides an in-depth view into representation of building data, and applies three dimensionality reduction algorithms Principle Component Analysis (PCA), autoencoder and t-Distributed Stochastic Neighbour Embedding (t-SNE) on measurements from a teaching building. Results show that whilst PCA returns linear representations, it also has the least data compression, which can be useful for obtaining more general features. On the other hand, t-SNE returns the most compressed data, which is suitable for seeking large margins within a dataset. However, t-SNE may be unsuitable for datasets with recurring step-like temporal profiles. Autoencoder is the best overall option, as they capture the nonlinearities within a dataset whilst avoiding excessive data compression. Fine-tuning the hyperparameters of studied the algorithms, and the perils of relying on poorly tuned models is discussed at the end of the study."
}
@article{FREEDMAN2014176,
title = "A high-performance workflow system for subsurface simulation",
journal = "Environmental Modelling & Software",
volume = "55",
pages = "176 - 189",
year = "2014",
issn = "1364-8152",
doi = "https://doi.org/10.1016/j.envsoft.2014.01.030",
url = "http://www.sciencedirect.com/science/article/pii/S1364815214000401",
author = "Vicky L. Freedman and Xingyuan Chen and Stefan Finsterle and Mark D. Freshley and Ian Gorton and Luke J. Gosink and Elizabeth H. Keating and Carina S. Lansing and William A.M. Moeglein and Christopher J. Murray and George S.H. Pau and Ellen Porter and Sumit Purohit and Mark Rockhold and Karen L. Schuchardt and Chandrika Sivaramakrishnan and Velimir V. Vessilinov and Scott R. Waichler",
keywords = "Workflows, ASCEM, Akuna, Amanzi, Model calibration, Uncertainty analysis, Contaminant transport, Vadose zone",
abstract = "The U.S. Department of Energy (DOE) recently invested in developing a numerical modeling toolset called ASCEM (Advanced Simulation Capability for Environmental Management) to support modeling analyses at legacy waste sites. This investment includes the development of an open-source user environment called Akuna that manages subsurface simulation workflows. Core toolsets accessible through the Akuna user interface include model setup, grid generation, sensitivity analysis, model calibration, and uncertainty quantification. Additional toolsets are used to manage simulation data and visualize results. This new workflow technology is demonstrated by streamlining model setup, calibration, and uncertainty analysis using high performance computation for the BC Cribs Site, a legacy waste area at the Hanford Site in Washington State. For technetium-99 transport, the uncertainty assessment for potential remedial actions (e.g., surface infiltration covers) demonstrates that using multiple realizations of the geologic conceptual model results in greater variation in concentration predictions than when a single model is used."
}
@article{ARJUNAN2020115413,
title = "EnergyStar++: Towards more accurate and explanatory building energy benchmarking",
journal = "Applied Energy",
volume = "276",
pages = "115413",
year = "2020",
issn = "0306-2619",
doi = "https://doi.org/10.1016/j.apenergy.2020.115413",
url = "http://www.sciencedirect.com/science/article/pii/S0306261920309259",
author = "Pandarasamy Arjunan and Kameshwar Poolla and Clayton Miller",
keywords = "Building energy benchmarking, Building performance rating, Multiple linear regression, Gradient boosting trees, Feature interaction, Interpretable machine learning",
abstract = "Building energy performance benchmarking has been adopted widely in the USA and Canada through the Energy Star Portfolio Manager platform. Building operations and energy management professionals have long used this simple 1–100 score to understand how their building compares to its peers. This single number is easy to use but is created by potentially inaccurate multiple linear regression (MLR) models and lacks much further information about why a building achieves that score. This paper proposes a methodology that enhances the existing Energy Star calculation method by increasing accuracy and providing additional model output processing to help explain why a building is achieving a particular score. Two new prediction models were proposed and tested: multiple linear regression with feature interactions (MLRi) and gradient boosted trees (GBT). Both models performed better than a baseline Energy Star MLR model as well as four baseline models from previous benchmarking studies. This paper shows that for six building types, on average, the third-order MLRi models achieved a 4.9% increase in adjusted R2 and a 7.0% decrease in normalized root mean squared error (NRMSE) over the baseline MLR model. More substantially, the most accurate GBT models, on average, achieved a 24.9% increase in adjusted R2 and a 13.7% decrease in NMRSE against the baseline MLR model. In addition, a set of techniques was developed to help determine which factors most influence a building’s energy use versus its peers using SHapley Additive exPlanation (SHAP) values. The SHAP force visualization, in particular, offered an accessible overview of the aspects of the building that influenced the score that even non-technical users can interpret. This methodology was tested on the 2012 Commercial Building Energy Consumption Survey (CBECS)(1,812 buildings) and public data sets from the energy disclosure programs of New York City (11,131 buildings) and Seattle (2,073 buildings)."
}
@article{PEREIRA2020121018,
title = "Vinasse biogas energy and economic analysis in the state of São Paulo, Brazil",
journal = "Journal of Cleaner Production",
volume = "260",
pages = "121018",
year = "2020",
issn = "0959-6526",
doi = "https://doi.org/10.1016/j.jclepro.2020.121018",
url = "http://www.sciencedirect.com/science/article/pii/S0959652620310659",
author = "Isabela Zanon Pereira and Ivan Felipe Silva dos Santos and Regina Mambeli Barros and Hellen Luisa de {Castro e Silva} and Geraldo Lucio {Tiago Filho} and Ana Paula {Moni e Silva}",
keywords = "Vinasse, Biogas, Geographic information system, Economic viability and São Paulo",
abstract = "The alcohol industry contributes significantly to the Brazilian economy, having high power generation potential through vinasse, the main waste produced with a high organic load. The amount of sugarcane and the volume of vinasse produced were calculated. Also, the production of biogas from the cultivation area belonging to each municipality of the state was estimated. Economic viability was assessed using the net present value (NPV) and the Levelized Cost of Electricity (LCOE) parameters. According to the results obtained, energy potential, NPV and LCOE maps were generated from all municipalities, followed by the combination of the cartographic information with the generated energy data, obtaining several classes through the Jenks optimization method of ArcGis® software. Total electric power generation was determined to be 659 GWh/yr; 33% of the studied municipalities were identified as economically viable. The total energy output would be able to provide power to 295,702 inhabitants and 0.45% of the state’s energy demands. Power and viability maps were constructed for better visualization of the results."
}
@article{WILCOX2019250,
title = "A Big Data platform for smart meter data analytics",
journal = "Computers in Industry",
volume = "105",
pages = "250 - 259",
year = "2019",
issn = "0166-3615",
doi = "https://doi.org/10.1016/j.compind.2018.12.010",
url = "http://www.sciencedirect.com/science/article/pii/S0166361518303749",
author = "Tom Wilcox and Nanlin Jin and Peter Flach and Joshua Thumim",
keywords = "Big data, Smart grid, Meter data analytics",
abstract = "Smart grids have started generating an ever increasingly large volume of data. Extensive research has been done in meter data analytics for small data sets of electrical grid and electricity consumption. However limited research has investigated the methods, systems and tools to support data storage and data analytics for big data generated by smart grids. This work has proposed a new core-broker-client system architecture for big data analytics. Its implemented platform is named Smart Meter Analytics Scaled by Hadoop (SMASH). Our work has demonstrated that SMASH is able to perform data storage, query, analysis and visualization tasks on large data sets at 20 TB scale. The performance of SMASH in storing and querying large quantities of data are compared with the published results provided by Google Cloud, IBM, MongoDB, and AMPLab. The experimental results suggest that SMASH provides industry a competitive and easily operable platform to manage big energy data and visualize knowledge, with potential to support data-intensive decision making."
}
@article{LEX2019273,
title = "A cross-disciplinary path to healthy and energy efficient buildings",
journal = "Technological Forecasting and Social Change",
volume = "142",
pages = "273 - 284",
year = "2019",
note = "Understanding Smart Cities: Innovation ecosystems, technological advancements, and societal challenges",
issn = "0040-1625",
doi = "https://doi.org/10.1016/j.techfore.2018.07.023",
url = "http://www.sciencedirect.com/science/article/pii/S0040162518301999",
author = "Simon Westergaard Lex and Davide Calì and Morten {Koed Rasmussen} and Peder Bacher and Magnus Bachalarz and Henrik Madsen",
keywords = "Smart City, Collaboration, Indoor climate, Energy, Dynamic baseline, Reactive interventions",
abstract = "This paper complements existing Smart City taxonomies with a case study of concrete cross-boundary collaboration between actors from diverse disciplines and institutions. The paper explores technical, social and organizational aspects of indoor climate in public buildings in Copenhagen, and outlines a digital platform (skoleklima.dk/climify.org) for the visualization and evaluation of locally produced data. The platform is to improve temporarily challenging situations ‘right-in-time’, help to solve continuous problematic conditions in the buildings and provide a scientific data infrastructure for better political decision-making. Furthermore, the paper suggests that research in active public organizations (‘living labs’) unfolds in erratic and dynamic trajectories, and in order to attain comprehensive understanding and reach innovative solutions, involved actors need to explore and intertwine diverse technical, social, political and organizational circumstances. With an empirically outset, the paper thus opens for new contextual driven understandings of cross-boundary collaboration in Smart City development."
}
@article{GRANDERSON2019187,
title = "Integrating diagnostics and model-based optimization",
journal = "Energy and Buildings",
volume = "182",
pages = "187 - 195",
year = "2019",
issn = "0378-7788",
doi = "https://doi.org/10.1016/j.enbuild.2018.10.015",
url = "http://www.sciencedirect.com/science/article/pii/S0378778818314129",
author = "Jessica Granderson and Guanjing Lin and David Blum and Janie Page and Michael Spears and Mary Ann Piette",
keywords = "Energy management and information systems (EMIS), Fault detection and diagnostics (FDD), Optimization, Modelica, Physics-based modeling, Central cooling plant",
abstract = "Energy Management and Information Systems are a family of analytics technologies that include energy information systems, fault detection and diagnostics (FDD), and automated system optimization tools. Such systems have the potential to enable buildings to meet energy management goals of reducing total energy consumption and cost. Most current market offerings use data-driven and rule-based analytics. However, the use of physics-based models in the analytics offers potential improvements by providing an accurate estimation of outputs based on representation of the physical principles governing the building system behaviors. This also permits the use of design stage models to inform commissioning and operation. This paper describes the development and testing of a hybrid data-driven and physics model-based operational tool for energy efficiency in central cooling plants. The tool offers FDD functionality, setpoint optimization, and visualization of key performance parameters. It was demonstrated at a university campus in the mixed-humid ASHRAE Climate Zone 4A. Key performance metrics that were analyzed include plant electricity use reduction, plant model calibration, and system economics. Annual simulations indicate the tool can provide electricity savings of greater than 10% for approximately six months of the year, mainly during the winter season when wet bulb temperatures are low, though only 1.38% savings for the entire year. Additionally, over a 4-day period in April, recommended optimal setpoints were implemented, resulting in 17% savings versus metered baseline consumption. With respect to model calibration, the difference between model-predicted and measured parameters was less than 10% for 90% of data points acquired for three of six chillers, and for each ten cooling towers. Finally, the tool users reported that satisfaction with the capabilities was equal to or better than that with the preexisting BAS system."
}
@article{GOPALAKRISHNAN201937,
title = "Dynamic response of passive release of fungal spores from exposure to air",
journal = "Journal of Aerosol Science",
volume = "133",
pages = "37 - 48",
year = "2019",
issn = "0021-8502",
doi = "https://doi.org/10.1016/j.jaerosci.2019.04.005",
url = "http://www.sciencedirect.com/science/article/pii/S0021850219300448",
author = "Saranya Gopalakrishnan and Ravinder Arigela and Shashank Kumar Gupta and Ravikrishna Raghunathan",
keywords = ", Passive spore release, Optical particle sizer, Bioaerosols",
abstract = "Fungal spores are among the most predominant microorganisms present in ambient air and can potentially affect human health, vegetation and climate. However, a quantitative model representing passive release mechanism of such terrestrial filamentous fungi is still critically lacking. In this study, a combined experimental and mathematical modeling approach has been applied to characterize the dynamic response of passive release of spores upon exposure to air. The spore flux was observed to decrease with time when exposed to constant air velocity. The flux also displayed a cyclic behavior in response to velocity cycles. Based on the visual observations of the test surface that the conidiophore bend during air flow, the behavior of the fungal spores is explained in terms of the velocity gradient and the corresponding drag force acting on the conidiophores. The decrease in the observed spore flux with time was strongly correlated with the visual observations of the bending of the fungal conidiophore during airflow. These observations clearly suggest that the velocity and the drag on the spore corresponding to a bent conidiophore are smaller than that of a vertical conidiophore and consequently results in a smaller spore flux from a bent conidiophore. Based on the developed model, an important parameter, representing the energy required to aerosolize spores from the conidiophores was also determined (5.58 μJ for Penicillium chrysogenum). The model was refined to incorporate different scenario based on the experimental observations."
}
@article{BERGER20152913,
title = "CFD Post-processing in Unity3D",
journal = "Procedia Computer Science",
volume = "51",
pages = "2913 - 2922",
year = "2015",
note = "International Conference On Computational Science, ICCS 2015",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2015.05.476",
url = "http://www.sciencedirect.com/science/article/pii/S1877050915012843",
author = "Matthias Berger and Verina Cristie",
keywords = "CFD post-processing, Unity3D, urban climate, urban designs, visualization",
abstract = "In architecture and urban design, urban climate on is a strong design criterion for outdoor thermal comfort and building's energy performance. Evaluating the effect of buildings on the local climate and vice versa is done by computational fluid dynamics (CFD) methods. The results from CFD are typically visualized through post-processing software closely related to pre-processing and simulation software. The built-in functions are made for engineers and thus, it lacks user-friendliness for real-time exploration of results for architects. To bridge the gap between architect and engineer we propose visualizations based on game engine technology. This paper demonstrates the implementation of CFD to Unity3D conversion and weather data visualization."
}
@article{RIZKALLA2018997,
title = "Electromagnetic simulation for diagnosing damage to femoral neck vasculature: A feasibility study",
journal = "Journal of Orthopaedics",
volume = "15",
number = "4",
pages = "997 - 1003",
year = "2018",
issn = "0972-978X",
doi = "https://doi.org/10.1016/j.jor.2018.08.036",
url = "http://www.sciencedirect.com/science/article/pii/S0972978X18302368",
author = "James Rizkalla and Matthew Jeffers and Paul Salama and Maher Rizkalla",
abstract = "Background
Femoral neck fractures are common injuries managed by orthopedic surgeons across the world. From pediatrics to geriatrics, disruption of the blood supply to the femoral neck is a well-recognized source of morbidity and mortality, oftentimes resulting in avascular necrosis of the femoral head. This devastating complication occurs in 10–45% of femoral neck fractures. Therefore, it is vital for orthopedic surgeons provide efficient treatment of this injury, in order to optimize the patient's potential outcome and prevent long-term sequelae.
Methods
In this study, the anatomy of the proximal femur, including femoral metaphysis, femoral neck, vasculature, and femoral head, were simulated in COMSOL Finite Element Analysis (FEA) software. Electric fields were generated in a fashion that exploited disruptions within the vasculature of the femoral neck. This study was aimed at developing an alternative imaging modality for narrowing or disrupting the femoral neck's vasculature. The variables used for investigation included: frequency, penetration depth, and magnitude of the electrical energy. These variables, when combined, allowed for enhanced simulated visualization of the vasculature of the femoral neck and theoretically expedited diagnosis of obvious, or occult, femoral neck injury.
Results
Simulated blood vessels were developed in two-dimensions: the phi direction (circular), and z-direction. Two different frequencies, 3 GHz, and 5 GHz were considered, with 100-J energy pulses within blood vessels of 2.54 mm in diameter. The fat surrounding the bone to the outside surface body was simulated at 0.25 inch (0.65 cm). An additional model, with layered fat and skin above the vessels, was simulated at 2000J and successfully able to visualize the femoral neck's blood vessels. Results showed a distinguished E field across the blood boundary of nearly 170 V/M.
Conclusions
The electric field simulation data within the Phi and Z directions promises the feasibility of a subsequent practical model."
}
@article{TALLADA2020100391,
title = "CosmoHub: Interactive exploration and distribution of astronomical data on Hadoop",
journal = "Astronomy and Computing",
volume = "32",
pages = "100391",
year = "2020",
issn = "2213-1337",
doi = "https://doi.org/10.1016/j.ascom.2020.100391",
url = "http://www.sciencedirect.com/science/article/pii/S2213133720300457",
author = "P. Tallada and J. Carretero and J. Casals and C. Acosta-Silva and S. Serrano and M. Caubet and F.J. Castander and E. César and M. Crocce and M. Delfino and M. Eriksen and P. Fosalba and E. Gaztañaga and G. Merino and C. Neissner and N. Tonello",
keywords = "Apache Hadoop, Apache Hive, Data exploration, Data distribution, FITS, ASDF",
abstract = "We present CosmoHub (https://cosmohub.pic.es), a web application based on Hadoop to perform interactive exploration and distribution of massive cosmological datasets. Recent Cosmology seeks to unveil the nature of both dark matter and dark energy mapping the large-scale structure of the Universe, through the analysis of massive amounts of astronomical data, progressively increasing during the last (and future) decades with the digitization and automation of the experimental techniques. CosmoHub, hosted and developed at the Port d’Informació Científica (PIC), provides support to a worldwide community of scientists, without requiring the end user to know any Structured Query Language (SQL). It is serving data of several large international collaborations such as the Euclid space mission, the Dark Energy Survey (DES), the Physics of the Accelerating Universe Survey (PAUS) and the Marenostrum Institut de Ciències de l’Espai (MICE) numerical simulations. While originally developed as a PostgreSQL relational database web frontend, this work describes the current version of CosmoHub, built on top of Apache Hive, which facilitates scalable reading, writing and managing huge datasets. As CosmoHub’s datasets are seldomly modified, Hive it is a better fit. Over 60 TiB of cataloged information and 50×109 astronomical objects can be interactively explored using an integrated visualization tool which includes 1D histogram and 2D heatmap plots. In our current implementation, online exploration of datasets of 109 objects can be done in a timescale of tens of seconds. Users can also download customized subsets of data in standard formats generated in few minutes."
}
@article{HAM201315,
title = "EPAR: Energy Performance Augmented Reality models for identification of building energy performance deviations between actual measurements and simulation results",
journal = "Energy and Buildings",
volume = "63",
pages = "15 - 28",
year = "2013",
issn = "0378-7788",
doi = "https://doi.org/10.1016/j.enbuild.2013.02.054",
url = "http://www.sciencedirect.com/science/article/pii/S0378778813001485",
author = "Youngjib Ham and Mani Golparvar-Fard",
keywords = "Thermogaphy, Image-based 3D reconstruction, Computational fluid dynamics (CFD), Energy audits",
abstract = "Building energy performance simulation tools such as EnergyPlus, Ecotect, and eQuest are widely used to model energy performance of existing buildings and assess retrofit alternatives. Nevertheless, predictions from simulations typically deviate from actual measurements. Monitoring actual performance and measuring deviations from simulated data in 3D can help improve simulation accuracy through model calibrations, and in turn facilitate identification of energy performance problem. To do that, this paper presents Energy Performance Augmented Reality (EPAR) modeling that leverages collections of unordered digital and thermal imagery, in addition to computational fluid dynamics (CFD) models. First, users collect large numbers of digital and thermal imagery from the building under inspection using a single thermal camera. Through an image-based reconstruction pipeline, actual 3D spatio-thermal models are automatically generated and are superimposed with expected building energy performance models generated using CFD analysis through a user-driven process. The outcomes are EPAR models which visualize actual and expected models in a common 3D environment. Within the EPAR models, actual measurements and simulated results can be systematically compared and analyzed. The method is validated on typical residential and instructional buildings. The results demonstrate that EPAR models facilitate calibration of building energy performance models and support detection and analysis of building performance deviations."
}
@article{LIU20161710,
title = "A hybrid ICT-solution for smart meter data analytics",
journal = "Energy",
volume = "115",
pages = "1710 - 1722",
year = "2016",
note = "Sustainable Development of Energy, Water and Environment Systems",
issn = "0360-5442",
doi = "https://doi.org/10.1016/j.energy.2016.05.068",
url = "http://www.sciencedirect.com/science/article/pii/S0360544216306855",
author = "Xiufeng Liu and Per Sieverts Nielsen",
keywords = "ICT-solution, Smart meter data, Big data, Data analytics",
abstract = "Smart meters are increasingly used worldwide. Smart meters are the advanced meters capable of measuring energy consumption at a fine-grained time interval, e.g., every 15 min. Smart meter data are typically bundled with social economic data in analytics, such as meter geographic locations, weather conditions and user information, which makes the data sets very sizable and the analytics complex. Data mining and emerging cloud computing technologies make collecting, processing, and analyzing the so-called big data possible. This paper proposes an innovative ICT-solution to streamline smart meter data analytics. The proposed solution offers an information integration pipeline for ingesting data from smart meters, a scalable platform for processing and mining big data sets, and a web portal for visualizing analytics results. The implemented system has a hybrid architecture of using Spark or Hive for big data processing, and using the machine learning toolkit, MADlib, for doing in-database data analytics in PostgreSQL database. This paper evaluates the key technologies of the proposed ICT-solution, and the results show the effectiveness and efficiency of using the system for both batch and online analytics."
}
@article{YUFIT2019485,
title = "Operando Visualization and Multi-scale Tomography Studies of Dendrite Formation and Dissolution in Zinc Batteries",
journal = "Joule",
volume = "3",
number = "2",
pages = "485 - 502",
year = "2019",
issn = "2542-4351",
doi = "https://doi.org/10.1016/j.joule.2018.11.002",
url = "http://www.sciencedirect.com/science/article/pii/S2542435118305221",
author = "Vladimir Yufit and Farid Tariq and David S. Eastwood and Moshiel Biton and Billy Wu and Peter D. Lee and Nigel P. Brandon",
keywords = "battery degradation and failure, dendrite formation, synchrotron X-ray computed tomography, radiography, 3D imaging and quantification, FIB-SEM tomography, rechargeable zinc batteries",
abstract = "Summary
Alternative battery technologies are required to meet growing energy demands and address the limitations of present technologies. As such, it is necessary to look beyond lithium-ion batteries. Zinc batteries enable high power density while being sourced from ubiquitous and cost-effective materials. This paper presents, for the first time known to the authors, multi-length scale tomography studies of failure mechanisms in zinc batteries with and without commercial microporous separators. In both cases, dendrites were grown, dissolved, and regrown, critically resulting in different morphology of dendritic layer formed on both the electrode and the separator. The growth of dendrites and their volume-specific areas were quantified using tomography and radiography data in unprecedented resolution. High-resolution ex situ analysis was employed to characterize single dendrites and dendritic deposits inside the separator. The findings provide unique insights into mechanisms of metal-battery failure effected by growing dendrites."
}
@article{LI2020110281,
title = "Simulated Annealing Wrapped Generic Ensemble Fault Diagnostic Strategy for VRF System",
journal = "Energy and Buildings",
volume = "224",
pages = "110281",
year = "2020",
issn = "0378-7788",
doi = "https://doi.org/10.1016/j.enbuild.2020.110281",
url = "http://www.sciencedirect.com/science/article/pii/S0378778820311816",
author = "Zhengfei Li and Wentian Wei and Kuan Hu and Huanxin Chen and Yuzhou Wang and Qian Liu and Shuai Liu",
keywords = "VRF system, Simulated annealing algorithm, Ensemble learning, Refrigerant, Valves, Liquid floodback, Fault diagnosis",
abstract = "Variable refrigerant flow (VRF) systems have gained much attention and been widely used in commercial and residential buildings benefitting from their competitive advantages. However, after long-term operation in a complex environment, various faults may occur in the VRF systems, resulting in failure to meet users comfort requirements and even unnecessary increase in energy consumption. This paper proposes a simulated annealing wrapped generic ensemble fault diagnosis strategy for typical faults of VRF systems, such as refrigerant charge amount (RCA) faults, valve faults, and compressor liquid return (LF) faults. The simulated annealing algorithm based on random forest (SA-RF) is first utilized to perform feature selection process on the three kinds of fault datasets to select the optimal variables that can well characterize the fault states, which can improve the modeling efficiency while reducing the data dimensionality. Then five component learners and the proposed ensemble model based on them are established adopting the optimal variables as input variables. Through visualizing the error evolution and margin of the boosting models built in the first stage of the integration process, it was found that the boosting models can effectively avoid overfitting and most samples are correctly classified with high confidence. By comparing with the five component learners, it is concluded that the boosting strategy in the first stage can improve the diagnostic performance of the models, and the weighted voting integration strategy in the second stage can further improve the diagnostic performance of the model. The final ensemble model can effectively compensate for the deficiencies of each component learners and its diagnostic accuracy for the three fault data sets is as high as 95.37%, 99.36% and 98.3%, respectively, indicating that the model can be applied to diagnose the three types of faults in VRF system at the same time, showing a high versatility."
}
@article{CHIANG2015554,
title = "BIM-enabled Power Consumption Data Management Platform for Rendering and Analysis of Energy Usage Patterns",
journal = "Procedia Engineering",
volume = "118",
pages = "554 - 562",
year = "2015",
note = "Defining the future of sustainability and resilience in design, engineering and construction",
issn = "1877-7058",
doi = "https://doi.org/10.1016/j.proeng.2015.08.480",
url = "http://www.sciencedirect.com/science/article/pii/S1877705815021359",
author = "Cheng-Ting Chiang and Chun-Ping Chu and Chien-Cheng Chou",
keywords = "Building Information Model, Serious Game, Power Consumption Data Analysis ;",
abstract = "Using a visualization engine to analyze power meter generated data about energy consumption of a building can provide immediate and informative feedback for energy conservation research. This paper proposed a platform, real-time replay system (RTRS), to provide such a visualized and interactive environment through the Unity 3d game engine. RTRS combines two data sources: Revit, a building information model (BIM) tool, and WiFi power socket sensors into Unity 3d and replays electricity usage events. RTRS users can easily change the scene or jump to the given time frame identified as a wasteful operation, and the system will display potential energy conservation suggestions. In addition, RTRS users can set up event triggering parameters or query conditions against the sensor database inside RTRS. In overall, RTRS can provide an intuitive and easy-to-use platform for managing electricity usage data of a building, reducing the time and efforts for looking up information at the raw data level."
}
@article{WOODS2017290,
title = "Comparative Visualization of the RNA Suboptimal Conformational Ensemble In Vivo",
journal = "Biophysical Journal",
volume = "113",
number = "2",
pages = "290 - 301",
year = "2017",
issn = "0006-3495",
doi = "https://doi.org/10.1016/j.bpj.2017.05.031",
url = "http://www.sciencedirect.com/science/article/pii/S0006349517305702",
author = "Chanin T. Woods and Lela Lackey and Benfeard Williams and Nikolay V. Dokholyan and David Gotz and Alain Laederach",
abstract = "When a ribonucleic acid (RNA) molecule folds, it often does not adopt a single, well-defined conformation. The folding energy landscape of an RNA is highly dependent on its nucleotide sequence and molecular environment. Cellular molecules sometimes alter the energy landscape, thereby changing the ensemble of likely low-energy conformations. The effects of these energy landscape changes on the conformational ensemble are particularly challenging to visualize for large RNAs. We have created a robust approach for visualizing the conformational ensemble of RNAs that is well suited for in vitro versus in vivo comparisons. Our method creates a stable map of conformational space for a given RNA sequence. We first identify single point mutations in the RNA that maximally sample suboptimal conformational space based on the ensemble’s partition function. Then, we cluster these diverse ensembles to identify the most diverse partition functions for Boltzmann stochastic sampling. By using, to our knowledge, a novel nestedness distance metric, we iteratively add mutant suboptimal ensembles to converge on a stable 2D map of conformational space. We then compute the selective 2′ hydroxyl acylation by primer extension (SHAPE)-directed ensemble for the RNA folding under different conditions, and we project these ensembles on the map to visualize. To validate our approach, we established a conformational map of the Vibrio vulnificus add adenine riboswitch that reveals five classes of structures. In the presence of adenine, projection of the SHAPE-directed sampling correctly identified the on-conformation; without the ligand, only off-conformations were visualized. We also collected the whole-transcript in vitro and in vivo SHAPE-MaP for human β-actin messenger RNA that revealed similar global folds in both conditions. Nonetheless, a comparison of in vitro and in vivo data revealed that specific regions exhibited significantly different SHAPE-MaP profiles indicative of structural rearrangements, including rearrangement consistent with binding of the zipcode protein in a region distal to the stop codon."
}
@incollection{JANIPELLA2019143,
title = "Chapter 8 - Application of Geographic Information System in Energy Utilization",
editor = "Sunil Kumar and Rakesh Kumar and Ashok Pandey",
booktitle = "Current Developments in Biotechnology and Bioengineering",
publisher = "Elsevier",
pages = "143 - 161",
year = "2019",
isbn = "978-0-444-64083-3",
doi = "https://doi.org/10.1016/B978-0-444-64083-3.00008-7",
url = "http://www.sciencedirect.com/science/article/pii/B9780444640833000087",
author = "Ramesh Janipella and Vikash Gupta and Rucha V. Moharir",
keywords = "Geographic information system (GIS), Renewable energy, Site selection, Solid waste management, Waste to energy",
abstract = "The application of geographic information systems (GISs) in the field of renewable energy has gained a lot of attention in recent times. For visualizing, interpreting, and analyzing data a GIS can be used. This chapter mainly focuses on the application of GISs in energy generation from waste treatment processes like incineration, pyrolysis, gasification, etc. A GIS serves as an optimal decision support system for converting waste to energy (WTE) by incorporating various input spatial data sets. WTE is a promising alternative for solid waste management and it is a potential source for the production of renewable energy. GISs help in finding suitable locations for WTE conversion plants and also maintain the database for capacity, energy production, and subsequent distribution of energy from the plant. This can be regularly updated and monitored by the authenticated users from anywhere in the world using Web GIS."
}
@article{CHANG2019100979,
title = "A user-centric smart product-service system development approach: A case study on medication management for the elderly",
journal = "Advanced Engineering Informatics",
volume = "42",
pages = "100979",
year = "2019",
issn = "1474-0346",
doi = "https://doi.org/10.1016/j.aei.2019.100979",
url = "http://www.sciencedirect.com/science/article/pii/S147403461930552X",
author = "Danni Chang and Zhenyu Gu and Fan Li and Rong Jiang",
keywords = "User-centric design, Smart product-service system, Medication management, Multimodal user analysis, Providers integration network, BCE model",
abstract = "With the advancement in Internet implementations, computational intelligence and network technologies, smart product-service system (SPSS) has become an important research area. A lot of research effort has been devoted to construct the conceptual framework, identify the important elements and evaluate the effectiveness of SPSS. However, there is still no SPSS development approach from user-centric perspective. Therefore, this article aims to provide a novel understanding of user-centric SPSS (UC-SPSS), outline the conceptual framework of UC-SPSS and contribute a UC-SPSS development approach. Specifically, a multimodal user analysis module with S-E-T (society-economy-technology) analysis, user behavioral analysis and user segmentation is deployed. According to the user needs identified, a provider identification and integration network is established in the dimensions of material, data and value flows. Jointly considering the user needs and provider capability, the BCE (benefit-cost-expectation) model and Product Function Architecture are applied to assist in the realization of the smart, connected service. To illustrate, a UC-SPSS on medication management for the elderly was developed, and it has been evaluated from user experience and sustainable value aspects. The results showed that the developed medication service is interesting and helpful for the elderly to take their medication. However, the service is not simple enough, especially in data visualization. In terms of sustainable value, the developed service can achieve better performance in economic, material and energy costs, and can support the further regulation of medical industry. Based on the case illustration, the proposed approach appears effective to help with SPSS development."
}
@article{TORABIMOGHADAM201919,
title = "A new clustering and visualization method to evaluate urban heat energy planning scenarios",
journal = "Cities",
volume = "88",
pages = "19 - 36",
year = "2019",
issn = "0264-2751",
doi = "https://doi.org/10.1016/j.cities.2018.12.007",
url = "http://www.sciencedirect.com/science/article/pii/S0264275118307625",
author = "Sara {Torabi Moghadam} and Silvia Coccolo and Guglielmina Mutani and Patrizia Lombardi and Jean-Louis Scartezzini and Dasaraden Mauree",
keywords = "Built environment, Geographical information system, Statistical models, Deterministic models, Urban heat energy planning, Spatial decision support system",
abstract = "Spatial visualization is a very useful tool to help decision-makers in the urban planning process, i) to define future energy transition pathways, ii) to implement energy efficiency strategies and iii) to integrate renewable energy technologies in the context of sustainable cities. There is thus a need to develop new tools to understand the energy consumption patterns across cities. Statistical methods are often used to understand the driving parameters of energy consumption but rarely used to evaluate future urban refurbishment scenarios. Simulating whole cities using energy demand softwares can be very extensive in terms of computer resources and data collection. A new methodology, using city archetypes, is hence proposed to simulate the energy consumption of urban areas and to evaluate urban energy planning scenarios. The objective of this paper is to present a solid framework and innovative solution for the computation and visualization of energy saving at the city scale. The energy demand of cities, as well as the microclimatic conditions, are calculated by using a 3D model designed as function of the real city urban geometrical and physical characteristics. Data are extracted from a GIS database. We demonstrate how the number of buildings to be simulated can be drastically reduced thereby significantly decreasing the computational time and without compromising the accuracy of the results. This model is then used to evaluate the influence of two sets of refurbishment solutions. The energy consumption are then integrated back in the GIS to identify the areas in the city where refurbishment works are needed more rapidly. The city of Settimo Torinese (Italy) is used as a demonstrator for the proposed methodology, which can be applied to medium-sized cities worldwide with limited amount of information."
}
@article{VUARNOZ2018573,
title = "Temporal variations in the primary energy use and greenhouse gas emissions of electricity provided by the Swiss grid",
journal = "Energy",
volume = "161",
pages = "573 - 582",
year = "2018",
issn = "0360-5442",
doi = "https://doi.org/10.1016/j.energy.2018.07.087",
url = "http://www.sciencedirect.com/science/article/pii/S0360544218313847",
author = "Didier Vuarnoz and Thomas Jusselme",
keywords = "Swiss electricity mix, Hourly conversion factor, Greenhouse gas emissions, Cumulative energy demand, Non-renewable cumulative energy demand, Emission factor",
abstract = "It is a frequent practice nowadays to use mean annual conversion factors (CFs) when performing life-cycle assessment (LCA) of processes and products that use electricity supplied by the grid. In this paper, we conduct an hourly assessment of the greenhouse gas (GHG) emission factor, along with the conversion factors for the cumulative energy demand (CED) and its non-renewable part (CEDnr), of electricity supplied by the Swiss grid and its direct neighboring countries (France, Germany, and Austria; Italy being neglected). Based on an hourly inventory of energy flows during a one-year period (2015–2016), this attributional approach allows performance of various certification procedures of process or product manufacturing, and comparison of energy and carbon intensities of different national mixes. Hourly calculation allows evaluation of the order of magnitude of errors made when considering an annual mix. Visualization techniques are used to better understand the obtained data and to detect when strategies involving timing optimization of electricity use may be efficient. A case study is chosen to illustrate the relevance of hourly CFs when performing LCA associated to the exploitation of a given building. Moreover, mean annual CFs of interest are discriminated by electricity end-use sectors. This could be of great help for system designers willing to improve the assessment accuracy when hourly CFs are not readily available."
}
@article{MOHAMMADPOURFARD2017242,
title = "A statistical unsupervised method against false data injection attacks: A visualization-based approach",
journal = "Expert Systems with Applications",
volume = "84",
pages = "242 - 261",
year = "2017",
issn = "0957-4174",
doi = "https://doi.org/10.1016/j.eswa.2017.05.013",
url = "http://www.sciencedirect.com/science/article/pii/S0957417417303317",
author = "Mostafa Mohammadpourfard and Ashkan Sami and Ali Reza Seifi",
keywords = "Cyber-attacks, False data injection, Visualization, State estimation, Unsupervised learning, Topology changes, Distributed generation, Smart grid",
abstract = "To achieve intelligence in the future grid, a highly accurate state estimation is necessary as it is a prerequisite for many key functionalities in the successful operation of the power grid. Recent studies show that a new type of cyber-attack called False Data Injection (FDI) attack can bypass bad data detection mechanisms in the power system state estimation. Existing countermeasures might not be able to manage topology changes and integration of distributed generations because they are designed for a specific system configuration. To address this issue, an unsupervised method to distinguish between attack and normal patterns is proposed in this paper. This method can detect FDI attacks even after topology changes and integration of renewable energy sources. In this method, we assume that injecting false data into the power systems will lead to a deviation in the probability distribution of the state vector from the normal trend. The main phases of the proposed algorithm are: (1) Normalizing the dataset, (2) Adding several statistical measures as the new features to the dataset to quantify the probability distribution of the state vectors, (3) Employing principal component analysis to reduce the dimensionality of the dataset, (4) Visualizing the reduced data for humans and exploiting their creativity to detect attacks, and (5) Locating the attacks using Fuzzy C-means clustering algorithm. The proposed method is tested on both the IEEE 14-bus and IEEE 9-bus systems using real load data from the New York independent system operator with the following attack scenarios: (1) attacks without any topology change, (2) attacks after a contingency, and (3) attacks after integration of distributed generations. Experimental results show that our proposed method is superior to the state-of-the-art classification algorithms in dealing with changes. In addition, the reduced data which is helpful in distinguishing between attack and normal patterns can be fed into an expert system for further improvement of the security of the power grid."
}
@article{VARGASSALGADO2019e02474,
title = "Low-cost web-based Supervisory Control and Data Acquisition system for a microgrid testbed: A case study in design and implementation for academic and research applications",
journal = "Heliyon",
volume = "5",
number = "9",
pages = "e02474",
year = "2019",
issn = "2405-8440",
doi = "https://doi.org/10.1016/j.heliyon.2019.e02474",
url = "http://www.sciencedirect.com/science/article/pii/S2405844019361341",
author = "Carlos Vargas-Salgado and Jesus Aguila-Leon and Cristian Chiñas-Palacios and Elías Hurtado-Perez",
keywords = "Electrical engineering, Energy, Data analysis, Data analytics, Data visualization, Control system design, Power control system, Electrical system, Renewable energy resources, Raspberry, Hybrid renewable energy system, Web-based SCADA, Cloud computing, Arduino, Remote control and wireless monitoring",
abstract = "This paper presents the design and implementation of a low-cost Supervisory Control and Data Acquisition system based on a Web interface to be applied to a Hybrid Renewable Energy System (HRES) microgrid. This development will provide a reliable and low-cost control and data acquisition systems for the Renewable Energy Laboratory at Universitat Politècnica de València (LabDER-UPV) in Spain, oriented to the research on microgrid stability and energy generation. The developed low-cost SCADA operates on a microgrid that incorporates a photovoltaic array, a wind turbine, a biomass gasification plant and a battery bank as an energy storage system. Sensors and power meters for electrical parameters, such as voltage, current, frequency, power factor, power generation, and energy consumption, were processed digitally and integrated into Arduino-based devices. A master device on a Raspberry-PI board was set up to send all this information to a local database (DB), and a MySQL Web-DB linked to a Web SCADA interface, programmed in HTML5. The communications protocols include TCP/IP, I2C, SPI, and Serial communication; Arduino-based slave devices communicate with the master Raspberry-PI using NRF24L01 wireless radio frequency transceivers. Finally, a comparison between a standard SCADA against the developed Web-based SCADA system is carried out. The results of the operative tests and the cost comparison of the own-designed developed Web-SCADA system prove its reliability and low-cost, on average an 86% cheaper than a standard brandmark solution, for controlling, monitoring and data logging information, as well as for local and remote operation system when applied to the HRES microgrid testbed."
}
@article{MOOLEKAMP201550,
title = "Toyz: A framework for scientific analysis of large datasets and astronomical images",
journal = "Astronomy and Computing",
volume = "13",
pages = "50 - 57",
year = "2015",
issn = "2213-1337",
doi = "https://doi.org/10.1016/j.ascom.2015.10.001",
url = "http://www.sciencedirect.com/science/article/pii/S2213133715000943",
author = "F. Moolekamp and E. Mamajek",
keywords = "Big data, Visualization, Python, HTML5, Web application",
abstract = "As the size of images and data products derived from astronomical data continues to increase, new tools are needed to visualize and interact with that data in a meaningful way. Motivated by our own astronomical images taken with the Dark Energy Camera (DECam) we present Toyz, an open source Python package for viewing and analyzing images and data stored on a remote server or cluster. Users connect to the Toyz web application via a web browser, making it ​a convenient tool for students to visualize and interact with astronomical data without having to install any software on their local machines. In addition it provides researchers with an easy-to-use tool that allows them to browse the files on a server and quickly view very large images (>2 Gb) taken with DECam and other cameras with a large FOV and create their own visualization tools that can be added on as extensions to the default Toyz framework."
}
@article{RONZINO2015258,
title = "The Energy Efficiency Management at Urban Scale by Means of Integrated Modelling",
journal = "Energy Procedia",
volume = "83",
pages = "258 - 268",
year = "2015",
note = "Sustainability in Energy and Buildings: Proceedings of the 7th International Conference SEB-15",
issn = "1876-6102",
doi = "https://doi.org/10.1016/j.egypro.2015.12.180",
url = "http://www.sciencedirect.com/science/article/pii/S1876610215028453",
author = "Amos Ronzino and Anna Osello and Edoardo Patti and Lorenzo Bottaccioli and Chiara Danna and Andrea Lingua and Andrea Acquaviva and Enrico Macii and Michelangelo Grosso and Gianluca Messina and Gaetano Rasconà",
keywords = "BIM, District Information Model, interoperability, urban energy management, RES",
abstract = "Innovative technologies such as ICTs (Information and Communications Technologies) are recognized as being a key player against climate change and the use of sensors and actuators can efficiently control the whole energy chain in the Smart Thermal Grids at district level. On the other side, advances on 3D modelling, visualization and interaction technologies enable user profiling and represent part of the holistic approach which aims at integrating renewable energy solutions in the existing building stock. To unlock the potentiality of these technologies, the case study selected for this research focuses on interoperability between Building Information Models (BIM), GIS (Geographic Information System) models and Energy Analysis Models (EAM) for designing Renewable Energy Strategies (RES) among the demonstrator. The objectives aim at making a whole series of data concerning the energy efficiency and reduction at district level usable for various stakeholders, by creating a District Information Model (DIM). The described system also integrates BIM and district level 3D models with real-time data from sensors to analyse and correlate buildings utilization and provide real-time energy-related behaviours. An important role is played by the energy simulation through the EAM for matching measured and simulated data and to assess the energy performance of buildings starting from a BIM model or shared data. With this purpose interoperability tests are carried out between the BIM models and quasi-steady energy analysis tools in order to optimize the calculation of the energy demand according to the Italian technical specification UNI TS 11300. Information about the roofs slope and their orientation from the GIS model are used to predict the use of renewable energy – solar thermal and PV – within the selected buildings (both public and private) of the demonstrator in Turin, Italy. The expected results are a consistent reduction in both energy consumption and CO2 emissions by enabling a more efficient energy distribution policies, according to the real characteristics of district buildings as well as a more efficient utilization and maintenance of the energy distribution network, based on social behaviour and users attitudes and demand. In the future the project will allow open access with personal devices and A/R visualization of energy-related information to client applications for energy and cost-analysis, tariff planning and evaluation, failure identification and maintenance, energy information sharing in order to increase the user's awareness in the field of energy consumption."
}
@article{HOSEK2016222,
title = "Metadyn View: Fast web-based viewer of free energy surfaces calculated by metadynamics",
journal = "Computer Physics Communications",
volume = "198",
pages = "222 - 229",
year = "2016",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2015.08.037",
url = "http://www.sciencedirect.com/science/article/pii/S0010465515003367",
author = "Petr Hošek and Vojtěch Spiwok",
keywords = "Free energy, Molecular dynamics, Enhanced sampling, Web-based visualization",
abstract = "Metadynamics is a highly successful enhanced sampling technique for simulation of molecular processes and prediction of their free energy surfaces. An in-depth analysis of data obtained by this method is as important as the simulation itself. Although there are several tools to compute free energy surfaces from metadynamics data, they usually lack user friendliness and a build-in visualization part. Here we introduce Metadyn View as a fast and user friendly viewer of bias potential/free energy surfaces calculated by metadynamics in Plumed package. It is based on modern web technologies including HTML5, JavaScript and Cascade Style Sheets (CSS). It can be used by visiting the web site and uploading a HILLS file. It calculates the bias potential/free energy surface on the client-side, so it can run online or offline without necessity to install additional web engines. Moreover, it includes tools for measurement of free energies and free energy differences and data/image export.
Program summary
Program title: Metadyn View Catalogue identifier: AEYC_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEYC_v1_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: GPL v3.0 No. of lines in distributed program, including test data, etc.: 273269 No. of bytes in distributed program, including test data, etc.: 4632839 Distribution format: tar.gz Programming language: HTML5, JavaScript, CSS, WebGL. Computer: Any computer with a modern web browser compatible with HTML5, JavaScript and CSS. Operating system: Platform-independent. RAM: Depends on the number of Gaussian hills and dimensionality of the bias potential. Classification: 3, 7.7, 23. Nature of problem: Fast and interactive visualization of free energy surfaces of molecular systems calculated by metadynamics method. Solution method: Implementation of optimized Bias Sum algorithm and a set of tools for free energy surface analysis. Unusual features: The program, due to its web-based nature, can be run on a wide range of devices and without installation. Running time: Couple of seconds for a medium sized HILLS file (tens of thousands of lines)."
}
@article{CZARSKI2020373,
title = "A Mixed Reality application for studying the improvement of HVAC systems in learning factories",
journal = "Procedia Manufacturing",
volume = "45",
pages = "373 - 378",
year = "2020",
note = "Learning Factories across the value chain – from innovation to service – The 10th Conference on Learning Factories 2020",
issn = "2351-9789",
doi = "https://doi.org/10.1016/j.promfg.2020.04.039",
url = "http://www.sciencedirect.com/science/article/pii/S2351978920310775",
author = "Marvin Czarski and Yen Ting Ng and Marcus Vogt and Max Juraschek and Bastian Thiede and Puay Siew Tan and Sebastian Thiede and Christoph Herrmann",
keywords = "HVAC, Learning Factory, Die Lernfabrik, Manufacturing Control Tower, HoloLens, BPS, CFD",
abstract = "Heating, ventilation and air conditioning (HVAC) systems in factories provide controlled conditions for workers and production equipment. At the same time, these systems are responsible for a significant share of industrial energy consumption. Commonly, HVAC systems are treated separately from production systems. However, numerous interactions and cross-influences occur affecting the overall energy efficiency and air quality. With analyzing and understanding these indoor air conditions the goal is to enable future engineers and experts to design and set them up in a way that improves human comfort, while reducing energy consumption. To achieve this, a cyber-physical system approach in a learning factory is presented. Based on data provided by the learning factory infrastructure, a building performance simulation with an integrated computational fluid dynamics simulation is composed. With the implementation in the learning factory, different ventilation and operation scenarios can be examined in learning scenarios and trainings to convey competencies about cyber-physical production systems in general and influences on the connection to HVAC systems. A mixed reality application provides three-dimensional visualization of the cyber-model and computed results for the learners."
}
@article{BISHOP2015120,
title = "Location based information to support understanding of landscape futures",
journal = "Landscape and Urban Planning",
volume = "142",
pages = "120 - 131",
year = "2015",
note = "Special Issue: Critical Approaches to Landscape Visualization",
issn = "0169-2046",
doi = "https://doi.org/10.1016/j.landurbplan.2014.06.001",
url = "http://www.sciencedirect.com/science/article/pii/S0169204614001431",
author = "Ian D. Bishop",
keywords = "Landscape change, Location based information, Climate change, Smartphones, Augmented reality",
abstract = "We are entering a period of rapid change in such landscape shaping forces as climate, the demand for food and water, fire, the push toward renewable energy, and the transition to GM-based agriculture. As a result the landscape, which has traditionally been a point of stability for many people, is likely to change significantly. Can the technologies we have to hand, and their increasing ubiquity, assist people to better understand the forces leading to landscape change, comprehend the science of landscape, take a more informed view on proposals, and adapt to some inevitable outcomes? This paper considers the potential, in this context, of smartphones and other devices that are equipped with positioning and orientation devices, significant computational power and high-speed communications. We all travel through the landscape; sometimes we fly over. During these movements the possibility exists to entertain, and educate, people with information about the landscape through, or over, which they are traveling, to give them insights into the changes that have occurred historically and may occur in the future, the processes at work and the choices that exist. There are several ways to do this including audio commentary and augmented reality displays using the device camera coupled with visualized alternative conditions. A prototype iPhone, called ‘What's Here?’ that includes augmented audio and visual features for landscape interpretation is described, and is illustrated with data related to climate change, renewable energy infrastructure, land use change and sea level rise. Associated data, technology and user issues are discussed."
}
@article{MORI201544,
title = "Three-dimensional structures and lithium-ion conduction pathways of (Li2S)x(GeS2)100−x superionic glasses",
journal = "Solid State Ionics",
volume = "280",
pages = "44 - 50",
year = "2015",
issn = "0167-2738",
doi = "https://doi.org/10.1016/j.ssi.2015.08.010",
url = "http://www.sciencedirect.com/science/article/pii/S0167273815003070",
author = "Kazuhiro Mori and Kozo Furuta and Yohei Onodera and Kenji Iwase and Toshiharu Fukunaga",
keywords = "Lithium-ion conducting glass, Local structure, Neutron diffraction, X-ray diffraction, Reverse Monte Carlo modeling, Bond valence sum",
abstract = "The conduction pathways of Li ions in (7Li2S)x(GeS2)100−x glasses (x=40, 50, and 60) were predicted and visualized by combining reverse Monte Carlo (RMC) modeling and the bond valence sum (BVS) approach, using synchrotron X-ray and time-of-flight neutron diffraction data. The conduction pathways of the Li ions could be classified into four grades, according to the magnitude of |ΔV(Li)|: (I) |ΔV(Li)|<0.04 (i.e., relatively stable regions for the Li ions), (II) 0.04≤|ΔV(Li)|<0.07, (III) 0.07≤|ΔV(Li)|<0.10, and (IV) 0.10≤|ΔV(Li)|<0.13; here, |ΔV(Li)| is the mismatch of the BVS for Li ions, V(Li). These were used to obtain a clear understanding of the movement of the Li ions in the (7Li2S)x(GeS2)100−x glasses. It was also found that there is a definite relationship between the topology of the conduction pathways of the Li ions and the activation energy, Ea, of the electrical conduction."
}
@article{JANETZKO201427,
title = "Anomaly detection for visual analytics of power consumption data",
journal = "Computers & Graphics",
volume = "38",
pages = "27 - 37",
year = "2014",
issn = "0097-8493",
doi = "https://doi.org/10.1016/j.cag.2013.10.006",
url = "http://www.sciencedirect.com/science/article/pii/S0097849313001477",
author = "Halldór Janetzko and Florian Stoffel and Sebastian Mittelstädt and Daniel A. Keim",
keywords = "Importance-driven, Pixel-based visualization, Anomaly detection, Visual analytics",
abstract = "Commercial buildings are significant consumers of electrical power. Also, energy expenses are an increasing cost factor. Many companies therefore want to save money and reduce their power usage. Building administrators have to first understand the power consumption behavior, before they can devise strategies to save energy. Second, sudden unexpected changes in power consumption may hint at device failures of critical technical infrastructure. The goal of our research is to enable the analyst to understand the power consumption behavior and to be aware of unexpected power consumption values. In this paper, we introduce a novel unsupervised anomaly detection algorithm and visualize the resulting anomaly scores to guide the analyst to important time points. Different possibilities for visualizing the power usage time series are presented, combined with a discussion of the design choices to encode the anomaly values. Our methods are applied to real-world time series of power consumption, logged in a hierarchical sensor network."
}
@article{TYRALIS2017700,
title = "Data and code for the exploratory data analysis of the electrical energy demand in the time domain in Greece",
journal = "Data in Brief",
volume = "13",
pages = "700 - 702",
year = "2017",
issn = "2352-3409",
doi = "https://doi.org/10.1016/j.dib.2017.06.033",
url = "http://www.sciencedirect.com/science/article/pii/S2352340917302779",
author = "Hristos Tyralis and Georgios Karakatsanis and Katerina Tzouka and Nikos Mamassis",
abstract = "We present data and code for visualizing the electrical energy data and weather-, climate-related and socioeconomic variables in the time domain in Greece. The electrical energy data include hourly demand, weekly-ahead forecasted values of the demand provided by the Greek Independent Power Transmission Operator and pricing values in Greece. We also present the daily temperature in Athens and the Gross Domestic Product of Greece. The code combines the data to a single report, which includes all visualizations with combinations of all variables in multiple time scales. The data and code were used in Tyralis et al. (2017) [1]."
}
@article{LENZ2017365,
title = "Energy Efficiency in Machine Tool Operation by Online Energy Monitoring Capturing and Analysis",
journal = "Procedia CIRP",
volume = "61",
pages = "365 - 369",
year = "2017",
note = "The 24th CIRP Conference on Life Cycle Engineering",
issn = "2212-8271",
doi = "https://doi.org/10.1016/j.procir.2016.11.202",
url = "http://www.sciencedirect.com/science/article/pii/S2212827116313622",
author = "Juergen Lenz and Jan Kotschenreuther and Engelbert Westkaemper",
keywords = ", , ",
abstract = "The first step to lower energy impact of products is to quantify the current impact. The mayor energy consumer in machine tools is not the spindle, it‘s the auxiliary units, such as pneumatic, hydraulic system, actuators, chip disposal and others. The quantification is achieved by precise measurements from signals by the PLC with high sampling rates. The analysis of this data is done by Engineering Apps (eApps). The analysis of the energy per component reveals the potential for energy reduction. The reduction can be achieved by switching to a more efficiency component. In this research examples of component substitution measurements have been performed for more efficient electrical motors, cutting fluid pumps, compressors vs. controlled compressors and start-stop hydraulic power packs. The stand-by energy analysis is similar to the component energy analysis and reveals the potential for setting some components to sleep mode during stand-by. Examples of analysed sleep mode components are chip disposal, cutting fluid pump, pneumatic and hydraulic system. As a result the energy usage can be displayed and visualized in various ways such as energy per part, energy per time or energy per shift. The energy usage visualization can be split into usage per component. These results can also be displayed online on mobile devices."
}
@article{ZAPATASIERRA2019552,
title = "Wind missing data arrangement using wavelet based techniques for getting maximum likelihood",
journal = "Energy Conversion and Management",
volume = "185",
pages = "552 - 561",
year = "2019",
issn = "0196-8904",
doi = "https://doi.org/10.1016/j.enconman.2019.01.109",
url = "http://www.sciencedirect.com/science/article/pii/S0196890419301773",
author = "Antonio J. Zapata-Sierra and Alejandro Cama-Pinto and Francisco G. Montoya and Alfredo Alcayde and Francisco Manzano-Agugliaro",
keywords = "Wind data, Wavelet transform, FFT, Missing data, Renewable energy, Data filling",
abstract = "Long time series of wind data can have data gaps that may lead to errors in the subsequent analyses of the time series. This study proposes using the wavelet transform as a system to verify that a data completion technique is correct and that the data series behaves correctly, enabling the user to infer the expected results. Wind speed data from three weather stations located in southern Europe were used to test the proposed method. The series consist of data measured every 10 min for 11 years. Various techniques are used to complete the data of one of the series; the wavelet transform is used as the control method, and its scalogram is used to visualize it. If the representation in the scalogram has zero magnitude, it shows the absence of data, so that if the data are properly filled in, then they have similar magnitudes to the rest of the series. The proposed method has shown that in case of data series inconsistencies, the wavelet transform can identify the lack of accuracy of the natural periodicity of these data. This result can be visually checked using the WT’s scalogram. Additionally, the scalograms provide valuable information on the variables studied, e.g. periods of higher wind speed. In summary, the wavelet transform has proven to be an excellent analysis tool that reveals the seasonal pattern of wind speed in periodograms at various scales."
}
@article{ZIOLKOWSKA201631,
title = "Geological and hydrological visualization models for Digital Earth representation",
journal = "Computers & Geosciences",
volume = "94",
pages = "31 - 39",
year = "2016",
issn = "0098-3004",
doi = "https://doi.org/10.1016/j.cageo.2016.06.003",
url = "http://www.sciencedirect.com/science/article/pii/S0098300416301509",
author = "Jadwiga R. Ziolkowska and Reuben Reyes",
keywords = "Multi-dimensional visualization, Geospatial analysis, Virtual globes, Subsurface depth layered cueing, Ternary Visual Shape Logic (TVSL)",
abstract = "This paper presents techniques and interactive models for multi-dimensional analyses and geospatial visualization in virtual globes based on three application examples: (1) earthquakes around the world, (2) groundwater well levels in Texas, and (3) geothermal subsurface heat indexes in Texas. While studies are known that represent multi-dimensional geospatial data points, we develop and suggest multi-dimensional models for virtual globes using KML and KMZ (compressed KML files) with a complete and static time series data set. The benefit of this approach for the user is the ability to view and analyze time-based correlations interactively over the entire time span in one instance, which is not possible with animated (dynamic) models. The methods embedded in our models include: (a) depth layered cueing within subsurface Earth visualization for a better orientation when maneuvering below the ground, (b) a technique with Ternary Visual Shape Logic (TVSL) as a quick indicator of change over time, and (c) different visual representations of multiple dimensions for the addressed case study examples. The models can be applied to a variety of problems in different disciplines, especially to support decision-making processes."
}
@article{SCHMIDT201743,
title = "Spectrum image analysis tool – A flexible MATLAB solution to analyze EEL and CL spectrum images",
journal = "Micron",
volume = "93",
pages = "43 - 51",
year = "2017",
issn = "0968-4328",
doi = "https://doi.org/10.1016/j.micron.2016.11.004",
url = "http://www.sciencedirect.com/science/article/pii/S0968432816302761",
author = "Franz-Philipp Schmidt and Ferdinand Hofer and Joachim R. Krenn",
keywords = "Spectrum imaging, Data processing, Electron energy loss spectroscopy, Cathodoluminescence, Plasmonics",
abstract = "Spectrum imaging techniques, gaining simultaneously structural (image) and spectroscopic data, require appropriate and careful processing to extract information of the dataset. In this article we introduce a MATLAB based software that uses three dimensional data (EEL/CL spectrum image in dm3 format (Gatan Inc.'s DigitalMicrograph®)) as input. A graphical user interface enables a fast and easy mapping of spectral dependent images and position dependent spectra. First, data processing such as background subtraction, deconvolution and denoising, second, multiple display options including an EEL/CL moviemaker and, third, the applicability on a large amount of data sets with a small work load makes this program an interesting tool to visualize otherwise hidden details."
}
@article{WANHSU201994,
title = "The New Version of Government Research Bulletin (GRB) Website Applying Visualization and Text-mining Tools",
journal = "Procedia Computer Science",
volume = "146",
pages = "94 - 101",
year = "2019",
note = "14th International Conference on Current Research Information Systems, CRIS2018, FAIRness of Research Information",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2019.01.084",
url = "http://www.sciencedirect.com/science/article/pii/S1877050919300894",
author = "Tsai Wan-Hsu and Chen Jia-Yu",
keywords = "research information platform, data visualization, text mining, word cloud, cooperation network, similarity calculation, real-time calculation, interactive charts",
abstract = "Government Research Bulletin (GRB) (www.grb.gov.tw) is the official platform to provide up-to-date information regarding the government funded research projects in Taiwan. Since GRB was launched in 1997, it has been operated as a data warehouse to collect and store research information from research institutes. To increase the discoverability and accessibility of the research information in GRB, the website services of GRB have been improved recently by applying visualization tools and text-mining tools. The new GRB provides multi-dimensional search engine for users to search for research projects, researchers and institutes. Users can explore the search results through not only the related record list, but also the word cloud graphs derived from text-mining results of those related contents. Word cloud graphs can help users to form concepts in mind about what researches have been done recently. The new GRB also applies an API provided by Lucene to calculate the similarity between every two projects, and to provide the list of similar projects ranking by their similarity to the main project interested by users. Users can also easily find a group of researchers working on similar topics and their cooperation network graphs by typing in a researcher’s name or just a keyword in the researcher search engine. Furthermore, to help users locate the institutes which are conducting projects on a specific topic, geographic visualization tool is applied to display the location of those institutions. Besides the improvement of search engine, a data-oriented charting tool called Highcharts is also applied to display the real-time calculated results of the data collected by GRB. Users can interact with the charts online to explore the number of projects and the amount of research budgets that government has already input every year since 1993. To further serve users interested in some specific hot topics (such as biotech medicine, green energy, and artificial intelligent), the new GRB also provides analysis reports and value-added analyses using GRB data to reveal the research trends about those hot topics. Now GRB are still evolving for providing better research information services."
}
@article{MORI2013113,
title = "Visualization of conduction pathways in lithium superionic conductors: Li2S-P2S5 glasses and Li7P3S11 glass–ceramic",
journal = "Chemical Physics Letters",
volume = "584",
pages = "113 - 118",
year = "2013",
issn = "0009-2614",
doi = "https://doi.org/10.1016/j.cplett.2013.08.016",
url = "http://www.sciencedirect.com/science/article/pii/S0009261413010245",
author = "Kazuhiro Mori and Tomoharu Ichida and Kenji Iwase and Toshiya Otomo and Shinji Kohara and Hajime Arai and Yoshiharu Uchimoto and Zempachi Ogumi and Yohei Onodera and Toshiharu Fukunaga",
abstract = "For (Li2S)x(P2S5)100–x glasses and Li7P3S11 glass–ceramic, which are well-known lithium superionic conductors, the conduction pathways of lithium ions were predicted and visualized by combining reverse Monte Carlo (RMC) modeling and the bond valence sum (BVS) method using synchrotron X-ray and time-of-flight neutron diffraction data. The conduction pathways of the lithium ions could be classified into two types: lithium ‘stable’ and ‘metastable’ regions. In addition, it was found that a significant relationship exists between the topology of the conduction pathways of the lithium ions and the activation energy of the electrical conduction."
}
@article{PRIYA2020109541,
title = "Study of intermetallics for corrosion and creep resistant microstructure in Mg-RE and Mg-Al-RE alloys through a data-centric high-throughput DFT framework",
journal = "Computational Materials Science",
volume = "175",
pages = "109541",
year = "2020",
issn = "0927-0256",
doi = "https://doi.org/10.1016/j.commatsci.2020.109541",
url = "http://www.sciencedirect.com/science/article/pii/S092702562030032X",
author = "Pikee Priya and Xiaoli Yan and Santanu Chaudhuri",
keywords = "High-throughput, First-principles, Alloy database, Surface energy, Work function",
abstract = "The design of corrosion-resistant alloys requires a thorough understanding of the chemistry, microstructure, and its evolution in atmospheric conditions. While the alloy processing conditions and chemistry determine the microstructure according to the phase diagram, the microstructure determines the behavior when exposed to corrosive environments. An automated data-centric high-throughput framework using density functional theory (DFT) based first-principles approach was developed to rank intermetallic chemistry and texture for their vulnerability in corrosive environments. As a test case, a user-interactive database for Mg-X-Y composition was created and made publicly available. The high-throughput framework was subsequently used to screen Mg-RE and Mg-Al-RE (RE = La, Ce, Nd) alloy precipitates from the database for corrosion resistance. The surface energies for different symmetrically distinct crystallographic orientations and work functions for the stable phases were calculated using an automated and intelligent workflow without requiring manual intervention. The predictions were compared against previous experimental and theoretical results, calculated properties and corrosion rates. The data was analyzed to provide guidance and insights on microstructure design with a focus on corrosion and creep resistant alloys. While the heat of formation seems to be a reliable material descriptor for the thermal stability of the phases for better creep resistance, the minimum surface energy is a reliable descriptor for the most stable crystallographic surfaces critical for corrosion resistance. These correlations established using a consistent first-principles based theory, which are computationally inexpensive, can simplify the exploration of possible elemental makeup and microstructural design and guide the design of corrosion and creep resistant alloys."
}
@article{DULLINGER2015616,
title = "A modular thermal simulation tool for computing energy consumption of HVAC units in rail vehicles",
journal = "Applied Thermal Engineering",
volume = "78",
pages = "616 - 629",
year = "2015",
issn = "1359-4311",
doi = "https://doi.org/10.1016/j.applthermaleng.2014.11.065",
url = "http://www.sciencedirect.com/science/article/pii/S1359431114010990",
author = "Christian Dullinger and Walter Struckl and Martin Kozek",
keywords = "Rail vehicle, Data-based vehicle model, HVAC, Dynamic thermal simulation tool, Energy consumption",
abstract = "A method and its implementation for computing the energy consumption of the heating, ventilation and air conditioning (HVAC) unit of a light rail vehicle is presented. In order to simulate the energy consumption of the HVAC a modular structured simulation model is generated which provides access to the essential HVAC parameters. The simulation combines the HVAC system with a dynamic thermal vehicle model and includes operational and weather inputs. The vehicle model is obtained by a data-based system identification utilizing data from climatic wind tunnel experiments. Due to a gray-box modeling approach its parameters can be physically interpreted. Because of the small sampling time (Ts = 10 s) effects of transient switching operations can be observed. The holistic thermal simulation model is integrated in a user-friendly tool with graphical user interface (GUI). The tool enables an efficient processing of simulation data sets, visualization of time-domain results and computation of the annual energy consumption of the HVAC system. A tram line in Vienna was used to obtain real measurements for validation of the simulation model. The results of both short-time and long-time simulations yield realistic statements."
}
@article{ABDELALIM2017258,
title = "Data visualization and analysis of energy flow on a multi-zone building scale",
journal = "Automation in Construction",
volume = "84",
pages = "258 - 273",
year = "2017",
issn = "0926-5805",
doi = "https://doi.org/10.1016/j.autcon.2017.09.012",
url = "http://www.sciencedirect.com/science/article/pii/S0926580516302813",
author = "Aly Abdelalim and William O'Brien and Zixiao Shi",
keywords = "Energy flows in buildings, Model calibration, BPS (building performance simulation), BIM (building information modeling), Sankey diagrams",
abstract = "Modern commercial buildings' resource consumption is metered at various levels of spatial and temporal resolution to track and reduce energy use and greenhouse gas emissions. However, not all data that could be used to detect faults or identify efficiency improvements are available due to the cost of meters and inaccessibility of the data they produce. In the field of building operation, building performance simulation (BPS) can help in quantifying unmeasured energy flows, for instance solar gains, heat loss from infiltration, etc. Furthermore, integrating building information modeling (BIM) in building operation and maintenance can decrease operation risk and costs, as well as maintain facility management quality. However, in practice there is a lack of efficient utilization of this application by building operators. The aim of this paper is to provide an integrated framework to estimate and visualize energy flows and the associated cost. The framework consists of 1) developing a BIM model, 2) converting the BIM model to a BPS model, 3) calibrating the model, and 4) visualizing energy and cost flows using Sankey diagrams. The study demonstrates this framework on a real-world case study, and hence provides a comprehensive energy use assessment on the building level to facilitate the decision-making by building operators. Finally, the results of a survey that was deployed to a sample user group to assess the usability and effectiveness of the proposed Sankey diagrams are provided."
}
@article{SERHANI201779,
title = "New algorithms for processing time-series big EEG data within mobile health monitoring systems",
journal = "Computer Methods and Programs in Biomedicine",
volume = "149",
pages = "79 - 94",
year = "2017",
issn = "0169-2607",
doi = "https://doi.org/10.1016/j.cmpb.2017.07.007",
url = "http://www.sciencedirect.com/science/article/pii/S0169260717304832",
author = "Mohamed Adel Serhani and Mohamed El Menshawy and Abdelghani Benharref and Saad Harous and Alramzana Nujum Navaz",
keywords = "Mobile monitoring, Epileptic seizure, Big data, EEG, Mapreduce",
abstract = "Background and objectives
Recent advances in miniature biomedical sensors, mobile smartphones, wireless communications, and distributed computing technologies provide promising techniques for developing mobile health systems. Such systems are capable of monitoring epileptic seizures reliably, which are classified as chronic diseases. Three challenging issues raised in this context with regard to the transformation, compression, storage, and visualization of big data, which results from a continuous recording of epileptic seizures using mobile devices.
Methods
In this paper, we address the above challenges by developing three new algorithms to process and analyze big electroencephalography data in a rigorous and efficient manner. The first algorithm is responsible for transforming the standard European Data Format (EDF) into the standard JavaScript Object Notation (JSON) and compressing the transformed JSON data to decrease the size and time through the transfer process and to increase the network transfer rate. The second algorithm focuses on collecting and storing the compressed files generated by the transformation and compression algorithm. The collection process is performed with respect to the on-the-fly technique after decompressing files. The third algorithm provides relevant real-time interaction with signal data by prospective users. It particularly features the following capabilities: visualization of single or multiple signal channels on a smartphone device and query data segments.
Results
We tested and evaluated the effectiveness of our approach through a software architecture model implementing a mobile health system to monitor epileptic seizures. The experimental findings from 45 experiments are promising and efficiently satisfy the approach's objectives in a price of linearity. Moreover, the size of compressed JSON files and transfer times are reduced by 10% and 20%, respectively, while the average total time is remarkably reduced by 67% through all performed experiments.
Conclusions
Our approach successfully develops efficient algorithms in terms of processing time, memory usage, and energy consumption while maintaining a high scalability of the proposed solution. Our approach efficiently supports data partitioning and parallelism relying on the MapReduce platform, which can help in monitoring and automatic detection of epileptic seizures."
}
@article{MONTEIRO2018244,
title = "An urban building database (UBD) supporting a smart city information system",
journal = "Energy and Buildings",
volume = "158",
pages = "244 - 260",
year = "2018",
issn = "0378-7788",
doi = "https://doi.org/10.1016/j.enbuild.2017.10.009",
url = "http://www.sciencedirect.com/science/article/pii/S0378778817319485",
author = "Claudia Sousa Monteiro and Carlos Costa and André Pina and Maribel Y. Santos and Paulo Ferrão",
keywords = "Urban database, Building stock data, Energy data, Urban energy consumption, Smart city",
abstract = "Urban energy modelling requires a large amount of detailed data to perform systematic dynamic simulations of a large number of buildings, where the adoption of energy efficiency strategies is an important concern for sustainable urban planning. National statistical datasets collect important aggregated data regarding building construction, energy consumption and occupants, and cities are making a significant effort to update spatial referenced data of their territory. However, these data is generally not detailed enough, being available at different scales and in different formats. The integrated use of these data is critical to validate different methods to predict and model energy consumption in cities, as well in addressing its energy saving potential. Furthermore, scenario analysis for retrofit or new design is only possible at building scale, highlighting the potential of a bottom-up database. This paper presents the process of collecting, mapping, cleansing and integrating urban data resulting in an UBD to support an information system for Smart Cities. The goal is to reduce the gap between the available urban data and the specific data required to run a complete urban building energy simulation. Key characteristics of an UBD are explored and applied to a case study in Lisbon, Portugal. As a result, a Buildings Dashboard is developed, materializing the UBD user interface. This dashboard allows the interactive visualization and data exploration of the building stock at multiple scales. Future work includes the development of an Urban Scenario category, bringing new insights on urban energy simulation and scenario evaluation to this platform."
}
@article{KALNBALKITE2017297,
title = "Urban Planning Needs. Clustering of Energy End Users",
journal = "Energy Procedia",
volume = "113",
pages = "297 - 303",
year = "2017",
note = "International Scientific Conference “Environmental and Climate Technologies”, CONECT 2016, 12-14 October 2016, Riga, Latvia",
issn = "1876-6102",
doi = "https://doi.org/10.1016/j.egypro.2017.04.069",
url = "http://www.sciencedirect.com/science/article/pii/S1876610217321926",
author = "Antra Kalnbalkite and Dace Lauka and Dagnija Blumberga",
keywords = "urban energy planning, ArcGIS, thermal energy consumption",
abstract = "The main goal for energy efficiency is to reduce energy consumption in all ways – in production, transmission and consumption. The European Union has the main role in energy reduction and has set many key instruments to reduce energy consumption. The methodology which is used in this paper specifies energy loads using appropriate indicators in the next years. In this paper three types of indicators are used – total heat energy consumption, MWh/year; specific energy performance of buildings consumption, kWh/m2 year; heat supply tariff, EUR/MWh. Once the indicators are set, the next step is to show results. In this case the ArcGIS program which is very suitable for visualizing data between different regions is used."
}
@article{YANG201875,
title = "Informing regional water-energy-food nexus with system analysis and interactive visualization – A case study in the Great Ruaha River of Tanzania",
journal = "Agricultural Water Management",
volume = "196",
pages = "75 - 86",
year = "2018",
issn = "0378-3774",
doi = "https://doi.org/10.1016/j.agwat.2017.10.022",
url = "http://www.sciencedirect.com/science/article/pii/S0378377417303487",
author = "Y.C. Ethan Yang and Sungwook Wi",
keywords = "FEW nexus, Water competition, Coupled human-nature system, Uncertainty",
abstract = "In sub-Saharan Africa, water resources are scarce and subject to competing uses – especially for agricultural production, energy generation, and ecosystem services. These water intensive activities in the Usangu plains and the Ruaha National Park in southern Tanzania, present a typical case for such water competition at the water-energy-food nexus. To decipher the coupled human-nature interactions in the Great Ruaha River basin and effectively communicate the results to non-technical practitioners, the water-energy-food nexus competition in the system is simulated using an advanced water system modeling approach and findings are visualized via interactive web-based tools (Data-Driven Document, D3) that foster fuller understanding of the findings for both practitioners and stakeholders. Our results indicate that a combination of infrastructural and procedural measures, each acceptable from a social and economic perspective, and understanding that zero flows cannot be totally eliminated during dry years in the Ruaha National Park, are likely to be the best way forward. This study also reveals that the combination of improvements in irrigation efficiency, cutbacks on proposed expansion of irrigated lands, and a low head weir at the wetland outlet, significantly reduces the number of zero flow days (i.e., increasing ecosystem function), resulting in positive effects on agricultural sector from limited (if any) reduction in rice crop yields. These upstream measures are all relatively cost efficient and can combine to free-up resources for other economic activity downstream (i.e. more stable hydropower production)."
}
@article{HUPPMANN2019143,
title = "The MESSAGEix Integrated Assessment Model and the ix modeling platform (ixmp): An open framework for integrated and cross-cutting analysis of energy, climate, the environment, and sustainable development",
journal = "Environmental Modelling & Software",
volume = "112",
pages = "143 - 156",
year = "2019",
issn = "1364-8152",
doi = "https://doi.org/10.1016/j.envsoft.2018.11.012",
url = "http://www.sciencedirect.com/science/article/pii/S1364815218302330",
author = "Daniel Huppmann and Matthew Gidden and Oliver Fricko and Peter Kolp and Clara Orthofer and Michael Pimmer and Nikolay Kushin and Adriano Vinca and Alessio Mastrucci and Keywan Riahi and Volker Krey",
keywords = "Open-source, Energy system optimization, Integrated assessment, Strategic planning tool, Scenario management, Data warehouse",
abstract = "The MESSAGE Integrated Assessment Model (IAM) developed by IIASA has been a central tool of energy-environment-economy systems analysis in the global scientific and policy arena. It played a major role in the Assessment Reports of the Intergovernmental Panel on Climate Change (IPCC); it provided marker scenarios of the Representative Concentration Pathways (RCPs) and the Shared Socio-Economic Pathways (SSPs); and it underpinned the analysis of the Global Energy Assessment (GEA). Alas, to provide relevant analysis for current and future challenges, numerical models of human and earth systems need to support higher spatial and temporal resolution, facilitate integration of data sources and methodologies across disciplines, and become open and transparent regarding the underlying data, methods, and the scientific workflow. In this manuscript, we present the building blocks of a new framework for an integrated assessment modeling platform; the “ecosystem” comprises: i) an open-source GAMS implementation of the MESSAGE energy++ system model integrated with the MACRO economic model; ii) a Java/database back-end for version-controlled data management, iii) interfaces for the scientific programming languages Python & R for efficient input data and results processing workflows; and iv) a web-browser-based user interface for model/scenario management and intuitive “drag-and-drop” visualization of results. The framework aims to facilitate the highest level of openness for scientific analysis, bridging the need for transparency with efficient data processing and powerful numerical solvers. The platform is geared towards easy integration of data sources and models across disciplines, spatial scales and temporal disaggregation levels. All tools apply best-practice in collaborative software development, and comprehensive documentation of all building blocks and scripts is generated directly from the GAMS equations and the Java/Python/R source code."
}
@article{BONILLA2018155,
title = "Practical and low-cost monitoring tool for building energy management systems using virtual instrumentation",
journal = "Sustainable Cities and Society",
volume = "39",
pages = "155 - 162",
year = "2018",
issn = "2210-6707",
doi = "https://doi.org/10.1016/j.scs.2018.02.009",
url = "http://www.sciencedirect.com/science/article/pii/S2210670717308788",
author = "Diego Bonilla and Margarita Gil Samaniego and Rogelio Ramos and Héctor Campbell",
keywords = "BEMS, Virtual instrument, LabVIEW, Electric energy, Electricity costs, Carbon footprint, Building performance",
abstract = "Building Energy Management Systems (BEMS) have been acquiring greater importance in the world; nowadays are a novel trend, especially for building performance, electricity cost and carbon footprint. Actually, tools for BEMS present some technical and economical disadvantages for building managers; to overcome these limitations, a practical and cost-effective tool is presented. To achieve this, we propose a flexible, user-friendly and low-cost remote monitoring system called Virtual Energy Management System (VMS). This tool is based on virtual instrumentation and the graphical user interface is developed on the LabVIEW 2013® virtual programming platform. VMS displays and records on a computer the information of electrical parameters, electricity costs and carbon footprint; moreover, the user is allowed to visualize the monitoring display and access to records remotely through the internet. The VMS software was designed to be used by employees and building managers, so specialized personnel is not required. The validation was carried out in a building actually in use by the Autonomous University of the State of Baja California (UABC) located at Mexicali, Mexico. Results show that the total investment cost of a monitoring tool for BEMS can be reduced by 40% for small and medium buildings."
}
@article{PARK20182664,
title = "Comprehensive analysis of the relationship between thermal comfort and building control research - A data-driven literature review",
journal = "Renewable and Sustainable Energy Reviews",
volume = "82",
pages = "2664 - 2679",
year = "2018",
issn = "1364-0321",
doi = "https://doi.org/10.1016/j.rser.2017.09.102",
url = "http://www.sciencedirect.com/science/article/pii/S1364032117313655",
author = "June Young Park and Zoltan Nagy",
keywords = "Data-driven literature survey, Building energy, Building control, Thermal comfort, Occupant behavior",
abstract = "Buildings are responsible for about 30–40% of global energy demand. At the same time, we humans spend almost our entire life, up to 80–90% of the time, inside of buildings. Reducing energy demand through optimal operation is the subject of building control research, while human satisfaction in buildings is studied in the thermal comfort community. Thus, balancing the two is necessary for a sustainable and comfortable building stock. We review both research fields and their relationship using a data-driven approach. Based on specific search terms, all relevant abstracts from the Web Of Science database are downloaded and analyzed using the text mining software VOSviewer. We visualize the scientific landscapes of historic and recent trends, and analyze the citation network to investigate the interaction between thermal comfort and building control research. We find that building control focuses predominantly on energy savings rather than incorporating results from thermal comfort, especially when it comes to occupant satisfaction. We identify potential research directions in terms of bridging the two fields."
}
@article{ABDELALIM2015334,
title = "Visualization of energy and water consumption and GHG emissions: A case study of a Canadian University Campus",
journal = "Energy and Buildings",
volume = "109",
pages = "334 - 352",
year = "2015",
issn = "0378-7788",
doi = "https://doi.org/10.1016/j.enbuild.2015.09.058",
url = "http://www.sciencedirect.com/science/article/pii/S037877881530298X",
author = "Aly Abdelalim and William O’Brien and Zixiao Shi",
keywords = "Energy use intensity, Greenhouse gas (GHG) emissions, Urban metabolism (UM), Material flow analysis (MFA), Visualization of materials and energy flows, Sankey diagram, Spatial–temporal analysis",
abstract = "Campuses, communities, and other building clusters are major users of energy and water and thus can have a significant environmental impact. Frequently, their buildings’ resource consumption is metered to various levels of resolution to attempt to track and reduce it. However, the metering and data logging systems are often inconvenient and difficult to access due to use of multiple systems and technologies of varying vintages. This paper proposes several methods to analyze and visualize building-level water, natural gas, and electricity consumption and the upstream environmental impacts: Sankey diagrams and bar charts that normalize metered values by floor area and occupancy. The objective is to improve accessibility of these data to all stakeholders, including building operators, planners, occupants, and utilities. The methods are then applied to a 45-building Canadian university campus and an array of graphical representations of the data is provided. The resulting analysis and visualization reveals significant variation in consumption between buildings regardless of building vintage and function. Furthermore, it is concluded that identifying resource consumption reducing strategies, once inefficient buildings have been identified, would require higher data resolution – both spatial and temporal."
}
@article{KOMATSU2019118,
title = "A combination of SOM-based operating time estimation and simplified disaggregation for SME buildings using hourly energy consumption data",
journal = "Energy and Buildings",
volume = "201",
pages = "118 - 133",
year = "2019",
issn = "0378-7788",
doi = "https://doi.org/10.1016/j.enbuild.2019.07.036",
url = "http://www.sciencedirect.com/science/article/pii/S037877881930742X",
author = "Hidenori Komatsu and Osamu Kimura",
keywords = "Electricity conservation, Information provision, Small- and medium-sized enterprises, Smart meter data",
abstract = "Despite the growing penetration of smart meters and energy management systems in recent years, few tools have been proposed for energy demand data analysis and visualization to promote energy conservation in small- and medium-sized enterprises (SMEs). In this paper, we propose a new method consisting of operating time estimation and simplified disaggregation based only on the SME's hourly electricity demand data and actual outdoor temperature data. The method first estimates the operating time using a self-organizing map, which categorizes the hourly data as operating, half-operating, or non-operating time and eliminates manual parameter adjustment that requires user trial and error. Then, using the estimated operating time, our simplified disaggregation method estimates hourly air conditioning (AC) demand considering the variation in each office's temperature at which air conditioners are switched on. The method was evaluated using a public business energy management system dataset from Japan, and the average errors of yearly AC demand estimation were better than 10%. The outputs of the proposed method may provide information that is more relevant, reliable, and specific to each customer and efficiently promote energy conservation measures in SMEs."
}
@article{TYRALIS2017902,
title = "Exploratory data analysis of the electrical energy demand in the time domain in Greece",
journal = "Energy",
volume = "134",
pages = "902 - 918",
year = "2017",
issn = "0360-5442",
doi = "https://doi.org/10.1016/j.energy.2017.06.074",
url = "http://www.sciencedirect.com/science/article/pii/S036054421731068X",
author = "Hristos Tyralis and Georgios Karakatsanis and Katerina Tzouka and Nikos Mamassis",
keywords = "Electrical energy demand, Energy forecasting, Exploratory data analysis, Greece, Gross domestic product, Temperature",
abstract = "The electrical energy demand (EED) in Greece for the time period 2002–2016 is investigated. The aim of the study is to introduce a framework for the exploratory data analysis (EDA) of the EED in the time domain. To this end, the EED at the hourly, daily, seasonal and annual time scale along with the mean daily temperature and the Gross Domestic Product (GDP) of Greece are visualized. The forecast of the EED provided by the Greek Independent Power Transmission Operator (IPTO) is also visualized and is compared with the actual EED. Furthermore, the EED pricing system is visualized. The results of the study in general confirm and summarize the conclusions of previous relevant studies in Greece, each one treating a single topic and covering shorter and earlier time periods. Furthermore, some unexpected patterns are observed, which if not considered carefully could result to dubious models. Therefore, it is shown that the EDA of the EED in the time domain coupled with weather-, climate-related and socio-economic variables is essential for the building of a model for the short-, medium- and long-term EED forecasting, something not highlighted in the literature."
}
@article{MOUSA201768,
title = "Simulations and quantitative data analytic interpretations of indoor-outdoor temperatures in a high thermal mass structure",
journal = "Journal of Building Engineering",
volume = "12",
pages = "68 - 76",
year = "2017",
issn = "2352-7102",
doi = "https://doi.org/10.1016/j.jobe.2017.05.007",
url = "http://www.sciencedirect.com/science/article/pii/S2352710217301559",
author = "Wael A. Yousef Mousa and Werner Lang and Waleed A. Yousef",
keywords = "Thermal mass, Indoor temperature, Energy efficiency, Thermal simulations, data analysis, Pattern recognition",
abstract = "The present paper investigates the impact of thermal mass on indoor temperature and reduction of cooling loads in summer. The major contribution of this paper is providing an objective assessment and a quantitative data analytic interpretation from the pattern recognition literature for the reported findings. The experimental study adopted one of the traditional stone structures of medieval Cairo. The house was monitored during summer days for Indoor and outdoor temperatures. Further data for local climate were obtained and given to TRNSYS 17 in which simulations were generated and validated against measured data. The absolute deviance error between simulated and measured indoor temperatures was 0.3°C for a couple of monitored spaces. Data visualization and regression analysis of indoor temperature on two values of outdoor temperature show a relative stability of the indoor temperature, a direct result of the heat storage capacity of the stone walls. A quantitative interpretation of the regression equation tells that the indoor temperature increases by merely 1°C if the outdoor temperature increased by 11.5°C. Upon ambient-temperature-responsive natural ventilation, the maximum indoor temperatures were reduced, in average of 5.5°C and 4.2°C below ambient for two different spaces. A comparative analysis took place then between the base case and a modified model were the walls’ material was substituted with hollow red bricks. For the two rooms, the energy demand for cooling was found to be as less as 72% and 56% in the base case than the brick-walls model."
}
@article{MILLER2017360,
title = "Mining electrical meter data to predict principal building use, performance class, and operations strategy for hundreds of non-residential buildings",
journal = "Energy and Buildings",
volume = "156",
pages = "360 - 373",
year = "2017",
issn = "0378-7788",
doi = "https://doi.org/10.1016/j.enbuild.2017.09.056",
url = "http://www.sciencedirect.com/science/article/pii/S037877881732488X",
author = "Clayton Miller and Forrest Meggers",
keywords = "Data mining, Building performance, Performance classification, Energy efficiency, Smart meters",
abstract = "This study focuses on the inference of characteristic data from a data set of 507 non-residential buildings. A two-step framework is presented that extracts statistical, model-based, and pattern-based behavior. The goal of the framework is to reduce the expert intervention needed to utilize measured raw data in order to infer information such as building use type, performance class, and operational behavior. The first step is temporal feature extraction, which utilizes a library of data mining techniques to filter various phenomenon from the raw data. This step transforms quantitative raw data into qualitative categories that are presented in heat map visualizations for interpretation. In the second step, a random forest classification model is tested for accuracy in predicting primary space use, magnitude of energy consumption, and type of operational strategy using the generated features. The results show that predictions with these methods are 45.6% more accurate for primary building use type, 24.3% more accurate for performance class, and 63.6% more accurate for building operations type as compared to baselines."
}
@article{YANG2020101057,
title = "Quasiconformal rectilinear map",
journal = "Graphical Models",
volume = "107",
pages = "101057",
year = "2020",
issn = "1524-0703",
doi = "https://doi.org/10.1016/j.gmod.2019.101057",
url = "http://www.sciencedirect.com/science/article/pii/S1524070319300475",
author = "Yi-Jun Yang and Wei Zeng",
keywords = "Visualization, Rectilinear, Surface mapping, Polyomino, Quasiconformal",
abstract = "This paper presents a novel method to compute the quasiconformal rectilinear map for a 2D polygonal subdivision, which keeps the original topology and best preserves the shape of the original input in the elasticity sense by the curve-driven quasiconformal mapping. We first automatically compute the straightening styles of the subdivision curves, which are then utilized to generate the optimal rectilinear map with the minimal harmonic energy. The quasiconformal rectilinear map is aesthetically pleasing for human perception, and hierarchically and progressively flexible for the design of spatial data structures. Based on the quasiconformal rectilinear map, an approach is given to compute the polyomino map. We evaluate the proposed structural maps in the data visualization application."
}
@article{FRANCISCO2019113804,
title = "Understanding citizen perspectives on open urban energy data through the development and testing of a community energy feedback system",
journal = "Applied Energy",
volume = "256",
pages = "113804",
year = "2019",
issn = "0306-2619",
doi = "https://doi.org/10.1016/j.apenergy.2019.113804",
url = "http://www.sciencedirect.com/science/article/pii/S0306261919314916",
author = "Abigail Francisco and John E. Taylor",
keywords = "Citizen engagement, Community energy, Cyber-physical systems, Energy feedback, Open data",
abstract = "With the rise of advanced and affordable sensors offering continuous monitoring of city infrastructure, cities are increasingly seeking to become more ‘smart’ and are adopting data-driven approaches to help meet sustainability goals. In the area of building energy efficiency, closely coupled with this effort is the prevalence of building energy benchmarking policies, which require public disclosure of vast new quantities of building-level energy data at urban scales (i.e., open urban energy data). While existing research efforts have focused on the potential of this data to transform energy efficiency markets and investments in the real estate sector, little research has been dedicated to assessing this information’s value to the general public. Given that achieving energy reductions in the built environment will require not only energy efficiency investments, but also greater awareness, engagement, and action from ordinary citizens, we study the potential of open urban energy data in providing citizen benefits. Energy-cyber-physical systems offer a pertinent framework to link data from the virtual world to citizens’ physical reality in order to improve their understanding and decision making. Adopting an energy-cyber-physical system perspective, we aim to connect open urban energy data to citizens through the development and evaluation of a novel community-scale energy feedback system. This mobile cyber-physical system transforms building-level electricity consumption and production data across Georgia Tech’s campus into a mobile application consisting of three features: spatial feedback, energy supply feedback, and energy consumption feedback. Augmented-reality visualization elements are integrated into the system, providing Georgia Tech community members a direct link between their experienced physical environment and data stored in the virtual world. Applying a user-centered design approach, prospective users evaluate the system via thinking aloud sessions and user surveys to assess understandings and perceptions of open urban energy data for the Georgia Tech campus. The results contribute to literature seeking to create energy feedback systems at the community-scale and expand research investigating citizen reactions to and opinions of open urban energy data. This research is an integral step to further engagement and participation from the public to help achieve a sustainable and citizen-valued energy future."
}
@article{PEPIN2017450,
title = "Visual analytics for exploring topic long-term evolution and detecting weak signals in company targeted tweets",
journal = "Computers & Industrial Engineering",
volume = "112",
pages = "450 - 458",
year = "2017",
issn = "0360-8352",
doi = "https://doi.org/10.1016/j.cie.2017.01.025",
url = "http://www.sciencedirect.com/science/article/pii/S0360835217300475",
author = "Lambert Pépin and Pascale Kuntz and Julien Blanchard and Fabrice Guillet and Philippe Suignard",
keywords = "Social media, Visual analytics, Topic mining, Weak signals, Twitter",
abstract = "Business decision support tools, including social media data analysis, are required to help managers better understand trends and customer opinions. This paper presents a visual analytics-based approach to assist an expert user in tracking topics relative to his/her company from Twitter. Developed for visualizing topic long-term evolution and detecting weak signals, our process is composed of three complementary steps: (i) a time-dependent topic extraction based on a Latent Dirichlet Allocation, (ii) a topic relationship detection based on a dissimilarity which evaluates the topic proximities between consecutive time slots, and (iii) a topic evolution visualization inspired by a Sankey diagram popular in industrial environments to show dynamic relationships in a system. To test our approach, we have used a real-life dataset from the French energy company EDF from which we have analyzed the evolution of a corpus of more than 70000 tweets related to this company published over one year, and detected different types of evolving patterns hidden by the data volume and commonly masked by fully automatic mining algorithms."
}
@incollection{KIRWAN2020163,
title = "Chapter 8 - Smart city functions",
editor = "Christopher Kirwan and Fu Zhiyong",
booktitle = "Smart Cities and Artificial Intelligence",
publisher = "Elsevier",
pages = "163 - 192",
year = "2020",
isbn = "978-0-12-817024-3",
doi = "https://doi.org/10.1016/B978-0-12-817024-3.00008-8",
url = "http://www.sciencedirect.com/science/article/pii/B9780128170243000088",
author = "Christopher Kirwan and Fu Zhiyong",
keywords = "Artificial Intelligence, Machine Learning, Superintelligence, Applied Convergence, Sensors, Cloud, Edge Computing, Computing, Big Data, Connectivity, Narrow AI, General AI, Strong AI, Biomimicry, Singularity",
abstract = "The Internet of Things (IoT), Artificial Intelligence (AI) and 5G are establishing the building blocks of the post human smart city. These high impact industrial and commercial applications are contextualized within the evolutionary development and hierarchy of smart city functions. By exploring their inter-relationship and co-dependencies of the systems functions and subsystems at structural, operational and strategic levels, we can begin to visualize the impact these technologies will have on the design, operations and DNA of smart cities. Building on a combination of the hardware infrastructure that enables smart cities and diverse forms of AI and their capabilities and applications, smart city functions — Smart Mobility, Smart Environment, Smart People, Smart Governance, Smart Economy and Smart Living — are connected and optimized as individual and collective functions within an integrated city operating system. This new alignment allows cities to function much better, reducing noise within system functionality while creating efficiencies. Edge computing is an example of system optimization through the reduction of energy consumption, more efficient data processing and overcoming latency and bandwidth issues. The facilitated convergence process, enabled by AI, contributes to the desired outcome. The true smart city is not just about technology, but convergence on a new holistic, biomimetic system architecture level that optimizes the health and wellbeing of the users, the city and the operating system itself."
}
@article{CHEN2020110182,
title = "Simulation and case study on residential stochastic energy use behaviors based on human dynamics",
journal = "Energy and Buildings",
volume = "223",
pages = "110182",
year = "2020",
issn = "0378-7788",
doi = "https://doi.org/10.1016/j.enbuild.2020.110182",
url = "http://www.sciencedirect.com/science/article/pii/S0378778819336102",
author = "Shuqin Chen and Jiajing Wu and Yangyang Pan and Jian Ge and Zhuoju Huang",
keywords = "Stochastic energy use behavior, Human dynamics, Event mechanism, Simulation, Residential building",
abstract = "Simulating the stochastic energy use behaviors of occupants plays an important role in the accurate prediction and operational optimization of building energy consumption. There are many types of energy use behaviors in residential buildings with strong randomicity and complex driving forces. A set of reasonable stochastic energy use behaviors simulation method with improved operability and prediction accuracy is required for the stochastic energy use behaviors. Therefore, a description method for residential energy use behaviors is proposed based on the theory of human dynamics. The residential energy use behaviors are divided into persistent behaviors and transient ones according to the method. The characteristics and regularity of these behaviors are described from the inherent decision-making mechanisms, which lays a theoretical foundation for modelling residential stochastic energy use behaviors in residential buildings. Consequently, a simulation model and algorithm for stochastic energy use behaviors are established based on “subject and event mechanism”. The model focuses on the entire generation process from occupants’ movement to energy use behaviors. The occupants’ movement sequences and persistent event sorting units are generated according to the BARABASI priority queue model and the highest priority decision. Also, based on the various control logic, the occurrence of transient behaviors is predicted. Finally, a household is selected for the annual measurement of stochastic energy use behaviors. Based on the above simulation method, MATLAB and NETLOGO are utilized with algorithm programming to realize the dynamic simulation of stochastic energy use behaviors in this household. The accuracy of the model is verified after comparing with the measured data. The research results suggest that the simulation method can accurately predict residential stochastic energy use behaviors and realize the simulation visualization."
}
@article{DOMINGUEZ2013152,
title = "Power monitoring system for university buildings: Architecture and advanced analysis tools",
journal = "Energy and Buildings",
volume = "59",
pages = "152 - 160",
year = "2013",
issn = "0378-7788",
doi = "https://doi.org/10.1016/j.enbuild.2012.12.020",
url = "http://www.sciencedirect.com/science/article/pii/S037877881200669X",
author = "Manuel Domínguez and Juan J. Fuertes and Serafín Alonso and Miguel A. Prada and Antonio Morán and Pablo Barrientos",
keywords = "Power consumption, Energy efficiency, Web monitoring systems, Three-layer structure, Electrical models, Self-Organizing Map (SOM), Isometric Feature Mapping (ISOMAP), k-Means",
abstract = "Nowadays, our dependence on electricity is strong because power consumption has increased considerably in the last years. For that reason, an efficient use of electricity is necessary, especially in public buildings. In order to manage the power consumption, it is vital to measure and monitor the electrical systems. Monitoring can provide advanced visualization and data analysis tools which can help us to achieve energy savings and peak power optimization. In this work, we present a power monitoring system developed for the campus buildings at the University of León (ULE) in Spain. This system is based on a three-layer structure. In the server layer, data are acquired from meters installed in the campus buildings. In the middle layer, data are stored and processed. In the client layer, monitoring interfaces, accessible remotely through the Internet, provide both traditional and advanced monitoring tools, based on statistical and data mining techniques. These techniques exploit data in order to find electrical patterns, detect faults and deviations, predict future power consumption, optimize peak power, etc. The data acquired by the monitoring system during 2010 are analyzed. The results from the visualization and data analysis tools, implemented in the monitoring system, are presented. The application of the proposed tools led to economic savings of around 15% and deeper knowledge about the electrical system."
}
@article{CAPITANELLI201464,
title = "A Smart Home Information Management Model for Device Interoperability Simulation",
journal = "Procedia CIRP",
volume = "21",
pages = "64 - 69",
year = "2014",
note = "24th CIRP Design Conference",
issn = "2212-8271",
doi = "https://doi.org/10.1016/j.procir.2014.03.150",
url = "http://www.sciencedirect.com/science/article/pii/S2212827114006921",
author = "Andrea Capitanelli and Alessandra Papetti and Margherita Peruzzini and Michele Germani",
keywords = "Smart product engineering, Intelligent design, Customized and personalized product development",
abstract = "Numerous smart home systems have been created in the recent years, but they still lack of high interoperability and research has been focused on single smart technologies instead of the system interoperability as a whole. Furthermore, available systems are usually strongly technology-oriented and they neglect the user's satisfaction and the benefits’ analysis. In addition to this, modern systems impose the intelligent management of a huger amount of data, which needs to be properly coordinated to achieve higher performances and offer new energy-control services. This paper defines an information management model to improve device interoperability in smart homes. It allows selecting and classifying the devices, visualizing their data model, aggregating the necessary data according to the desired service functions, and finally defining a set of rules to coordinate device operations according to user preferences and external events. A case study focused on washing machines is presented to demonstrate the methodology implementation; it allows designing and developing an energy-control service for the selected device and optimizing its functions according to the users’ needs and preferences as well as the constraints of the use scenario. Finally, the benefits achieved with such a new service are evaluated in terms of energy consumption, costs reduction and user satisfaction in a simulated home environment that represents practical scenarios of use."
}
@article{ARDITO2020100512,
title = "PowTrAn: An R Package for power trace analysis",
journal = "SoftwareX",
volume = "12",
pages = "100512",
year = "2020",
issn = "2352-7110",
doi = "https://doi.org/10.1016/j.softx.2020.100512",
url = "http://www.sciencedirect.com/science/article/pii/S235271102030025X",
author = "Luca Ardito and Marco Torchiano and Riccardo Coppola and Giulio Antoniol",
keywords = "Energy consumption, Power trace analysis, R language",
abstract = "Energy efficiency is an increasingly important non-functional property of software, especially when it runs on mobile or IoT devices. An engineering approach demands a reliable measurement of energy consumption of software while performing computational tasks. In this paper, we describe PowTrAn, an R package supporting the analysis of the power traces of a device executing software tasks. The tool analyzes traces with embedded markers, a non-invasive technique that enables gauging software efficiency based on the energy consumed by the whole device. The package effectively handles large power traces, detects work units, and computes correct energy measures, even in noisy conditions, such as those caused by multiple processes working simultaneously. PowTrAn was validated on applications in realistic conditions and multiple hardware configurations. PowTrAn also provides data visualization that helps the user to assess the measurement consistency, and it also helps to highlight possible energy outliers."
}
@article{SANCHEZHIDALGO2018351,
title = "A survey on visual data representation for smart grids control and monitoring",
journal = "Sustainable Energy, Grids and Networks",
volume = "16",
pages = "351 - 369",
year = "2018",
issn = "2352-4677",
doi = "https://doi.org/10.1016/j.segan.2018.09.007",
url = "http://www.sciencedirect.com/science/article/pii/S2352467718301802",
author = "Maria-Angeles Sanchez-Hidalgo and Maria-Dolores Cano",
keywords = "Control, Data representation, Monitoring, Power grids, Smart grids, QoE, Visualization",
abstract = "The legacy grid is evolving towards an intelligent-rooted grid system, whose architectural components should be able to improve and adapt their performance according to learning experiences. In the long term, decision making and actions execution may become completely machine-autonomous tasks in smart grids due to the progress of artificial intelligence and machine learning techniques. However, in the short and medium terms, control and monitoring activities will still require human–computer interaction. Due to the inherent characteristics of the smart grids, all gathered and processed data are heterogeneous by nature, being necessary to create novel ICT-based tools and platforms able to offer a broader multi-dimensional control perspective for the energy industry. In this context, visualization plays a key role in facilitating the challenging tasks of monitoring, analyzing, and responding to the events in the smart grid. In this paper, we explore the scientific- and industry-related literature to present a comprehensible survey on visualization methods for smart grids control and monitoring."
}
@incollection{ZUFFEREY2018107,
title = "Chapter 6 - Unsupervised Learning Methods for Power System Data Analysis",
editor = "Reza Arghandeh and Yuxun Zhou",
booktitle = "Big Data Application in Power Systems",
publisher = "Elsevier",
pages = "107 - 124",
year = "2018",
isbn = "978-0-12-811968-6",
doi = "https://doi.org/10.1016/B978-0-12-811968-6.00006-1",
url = "http://www.sciencedirect.com/science/article/pii/B9780128119686000061",
author = "Thierry Zufferey and Andreas Ulbig and Stephan Koch and Gabriela Hug",
keywords = "Data visualization, Distribution grid transparency, K-Means clustering, Smart meter data",
abstract = "This chapter focuses on the use of the K-Means clustering algorithm for an enhanced visibility of the electrical distribution system which can be provided by advanced metering infrastructure and supported by big data technologies and parallel cloud computing environments such as Spark and H2O. Based on smart meter data of more than 30,000 loads in the City of Basel, Switzerland, and thanks to an appropriate cluster analysis, it is shown that useful knowledge of the grid state can be gained without any further information concerning the type of consumer and their habits. Once energy data is judiciously prepared, the features extraction is an important step. A graphical user interface is presented which illustrates the potentially great flexibility in the choice of features according to the needs of distribution system operators (DSOs). For example, the distribution of the various types of customers across the power system is of interest to DSOs. This chapter presents thus some pertinent examples of clustering outcomes that are visualized on the map of Basel, which notably enables to easily identify heating and cooling demand or gain insight into the energy consumption throughout the day for different neighborhoods."
}
@article{LIN201664,
title = "Ecosystem discovery: Measuring clean energy innovation ecosystems through knowledge discovery and mapping techniques",
journal = "The Electricity Journal",
volume = "29",
number = "8",
pages = "64 - 75",
year = "2016",
issn = "1040-6190",
doi = "https://doi.org/10.1016/j.tej.2016.09.012",
url = "http://www.sciencedirect.com/science/article/pii/S1040619016301518",
author = "Jessica Lin and Supriya Chinthavali and Chelsey Dunivan Stahl and Christopher Stahl and Sangkeun Lee and Mallikarjun Shankar",
keywords = "Clean energy innovation ecosystems, Natural language processing, Mapping",
abstract = "While the term ‘innovation ecosystem’ is often utilized, the concept is rarely quantified. Oak Ridge National Lab conducted a ground-breaking application of natural language processing, link analysis and other computational techniques to transform text and numerical data into metrics on clean energy innovation activity and geography for the U.S. Department of Energy. The project demonstrates that a machine-assisted methodology gives the user a replicable method to rapidly identify, quantify and characterize clean energy innovation ecosystems. EPSA advanced a novel definition for clean energy innovation ecosystem as the overlap of five Ecosystem Components: 1) nascent clean energy indicators, 2) investors, 3) enabling environment, 4) networking assets and 5) large companies. The tool was created with the flexibility to allow the user to choose the weights of each of the five ecosystem components and the subcomponents. This flexibility allows the user to visualize different subsets of data as well as the composite IE rank. In an independent parallel effort, a DOE analyst in EPSA developed a short list of 22 top US clean energy innovation ecosystems; the Ecosystem Discovery tool was able to identify over 90% of the analyst-reported ecosystems. Full validation and calibration remain outstanding tasks. The tool and the underlying datasets have the potential to address a number of important policy questions. The initial broad list of U.S. clean energy innovation ecosystems, with geographic area, technology focus, and list and types of involved organizations can help describe regional technology activities and capabilities. The implementation of knowledge discovery techniques also revealed both the potential and limitations of an automatic machine extraction methodology to gather ecosystem component data. The project demonstrates that a machine-assisted methodology gives the user a replicable method to rapidly identify, quantify, and characterize clean energy innovation ecosystems."
}
@article{GARCIA201895,
title = "Interactive visualization for NILM in large buildings using non-negative matrix factorization",
journal = "Energy and Buildings",
volume = "176",
pages = "95 - 108",
year = "2018",
issn = "0378-7788",
doi = "https://doi.org/10.1016/j.enbuild.2018.06.058",
url = "http://www.sciencedirect.com/science/article/pii/S0378778818304146",
author = "Diego García and Ignacio Díaz and Daniel Pérez and Abel A. Cuadrado and Manuel Domínguez and Antonio Morán",
keywords = "Energy efficiency, Visual analytics, Building energy consumption, NMF, Energy disaggregation, Interactive displays",
abstract = "Non-intrusive load monitoring (NILM) techniques have recently attracted much interest, since they allow to obtain latent patterns from power demand data in buildings, revealing useful information to the expert user. Unsupervised methods are specially attractive, since they do not require labeled datasets. Particularly, non-negative matrix factorization (NMF) methods decompose a single power demand measurement over a certain time period into a set of components or “parts” that are sparse, non-negative and sum up the original measured quantity. Such components reveal hidden temporal patterns which may be difficult to interpret in complex systems such as large buildings. We suggest to integrate the knowledge of the user into the analysis in order to recognize the real events inside the electric network behind the learnt patterns. In this paper, we integrate the available domain knowledge of the user by means of a visual analytics web application in which an expert user can interact in a fluid way with the NMF outcome through visual approaches such as barcharts, heatmaps or calendars. Our approach is tested with real electric power demand data from a hospital complex, showing how the interpretation of the decomposition is improved by means of interactive data cube visualizations, in which the user can insightfully relate the NMF components to characteristic demand patterns of the hospital such as those derived from human activity, as well as to inefficient behaviors of the largest systems in the hospital."
}
@incollection{TAKACS2018367,
title = "Chapter 7 - Operation, Monitoring, and Surveillance of Electrical Submersible Pumping Systems",
editor = "Gabor Takacs",
booktitle = "Electrical Submersible Pumps Manual (Second Edition)",
publisher = "Gulf Professional Publishing",
edition = "Second Edition",
pages = "367 - 404",
year = "2018",
isbn = "978-0-12-814570-8",
doi = "https://doi.org/10.1016/B978-0-12-814570-8.00007-6",
url = "http://www.sciencedirect.com/science/article/pii/B9780128145708000076",
author = "Gabor Takacs",
keywords = "Data Visualization, DHM (downhole measurement), Digital oilfield, Monitoring, Operating, Pulling, Running, SCADA, Surveillance, Wellhead choke",
abstract = "This chapter covers the detailed description of operating, monitoring, and surveillance methods of electrical submersible pumping (ESP) installations. Detailed guidelines for transporting, running, pulling, and starting up of ESP equipment are presented. Production control is often affected by wellhead chokes that permit the limitation of liquid rates to the desired levels; this solution, however, may cause high wastes of electric power. The use of variable speed drive units to reduce the waste of energy is illustrated by a sample calculation. The evolution of ESP monitoring is presented and the up-to-date permanent downhole measurement systems are introduced. Modern surveillance methods (including SCADA) are presented along with the requirements of the digital oilfield. Data visualization procedures are illustrated by presenting several field cases."
}
@article{PEREZ201411257,
title = "Power-Consumption Analysis through Web-Based Visual Data Exploration",
journal = "IFAC Proceedings Volumes",
volume = "47",
number = "3",
pages = "11257 - 11262",
year = "2014",
note = "19th IFAC World Congress",
issn = "1474-6670",
doi = "https://doi.org/10.3182/20140824-6-ZA-1003.01267",
url = "http://www.sciencedirect.com/science/article/pii/S1474667016434051",
author = "Daniel Pérez and Ignacio Díaz and Abel A. Cuadrado and Francisco J. García-Fernández and Alberto B. Diez and Manuel Domínguez",
keywords = "smart grids, monitoring and performance assessment, visual pattern recognition, supervision, energy expenditure, electric power systems, decision support systems",
abstract = "In recent years, the increasing capabilities of current technologies have made the acquisition and storage of large datasets an easy task. However, most times their unmanageable size, along with their complexity makes it a challenge to handle and analyze them. The emerging field of visual analytics relies on visualization principles and interactive interfaces to provide ways for amplifying human cognition, so that the information processing is improved and a better understanding of these datasets is achieved. In addition, current technologies to develop visual interfaces allow to develop powerful interaction mechanisms that enable the user to be an active part in the process. In this paper, data visualization foundations and web-based methods are considered for user-driven supervision tasks in decision-support systems. A real case consisting in the analysis of electric power demand in university buildings is presented via a web application developed using the recent data visualization library called D3. Time-series visualization and similarity maps are represented along with interactive techniques, allowing a dynamic data exploration."
}
@article{LYNES2018781,
title = "Cold-Activated Lipid Dynamics in Adipose Tissue Highlights a Role for Cardiolipin in Thermogenic Metabolism",
journal = "Cell Reports",
volume = "24",
number = "3",
pages = "781 - 790",
year = "2018",
issn = "2211-1247",
doi = "https://doi.org/10.1016/j.celrep.2018.06.073",
url = "http://www.sciencedirect.com/science/article/pii/S2211124718310003",
author = "Matthew D. Lynes and Farnaz Shamsi and Elahu Gosney Sustarsic and Luiz O. Leiria and Chih-Hao Wang and Sheng-Chiang Su and Tian Lian Huang and Fei Gao and Niven R. Narain and Emily Y. Chen and Aaron M. Cypess and Tim J. Schulz and Zachary Gerhart-Hines and Michael A. Kiebish and Yu-Hua Tseng",
keywords = "brown adipose tissue, white adipose tissue, lipidomics, thermogenesis, cardiolipin",
abstract = "Summary
Thermogenic fat expends energy during cold for temperature homeostasis, and its activity regulates nutrient metabolism and insulin sensitivity. We measured cold-activated lipid landscapes in circulation and in adipose tissue by MS/MSALL shotgun lipidomics. We created an interactive online viewer to visualize the changes of specific lipid species in response to cold. In adipose tissue, among the approximately 1,600 lipid species profiled, we identified the biosynthetic pathway of the mitochondrial phospholipid cardiolipin as coordinately activated in brown and beige fat by cold in wild-type and transgenic mice with enhanced browning of white fat. Together, these data provide a comprehensive lipid bio-signature of thermogenic fat activation in circulation and tissue and suggest pathways regulated by cold exposure."
}
@article{CHIEN2017436,
title = "Implementation of Cloud BIM-based Platform Towards High-performance Building Services",
journal = "Procedia Environmental Sciences",
volume = "38",
pages = "436 - 444",
year = "2017",
note = "Sustainable synergies from Buildings to the Urban Scale",
issn = "1878-0296",
doi = "https://doi.org/10.1016/j.proenv.2017.03.129",
url = "http://www.sciencedirect.com/science/article/pii/S1878029617301330",
author = "Szu-cheng Chien and Tzu-chun Chuang and Huei-Sheng Yu and Yi Han and Boon Hee Soong and King Jet Tseng",
keywords = "BIM, Building Management System, Facility Management, Built Environment, System Integration",
abstract = "In this research effort, we aim to develop an advanced integrated platform between FM, BIM, BMS, and front-end sensor data visualization. We first assessed of the status quo, conceptual design of the platform components and configuration, and identified the LoD. Subsequently, the BMS and environmental sensing information were incorporated into the BIM-based platform. Such information categories include the building geometry, indoor air quality, energy generation and consumption. This integrated BIM-based platform is exemplified in one existing low-rise zero energy office building in Tropics as a demonstration and pilot study. In this research effort, the platform to link different types of building information system and the visualization of front-end data in the phase of building operation and maintenance was developed. Also, the implementation process of this BIM-based platform together with a user-friendly dashboard for ease of O&M was demonstrated. The research results are expected to serve as the solid basis towards larger scale and comprehensive platform development."
}
@article{GROEHOKAMP2018114,
title = "Improved visualization of hypodense liver lesions in virtual monoenergetic images from spectral detector CT: Proof of concept in a 3D-printed phantom and evaluation in 74 patients",
journal = "European Journal of Radiology",
volume = "109",
pages = "114 - 123",
year = "2018",
issn = "0720-048X",
doi = "https://doi.org/10.1016/j.ejrad.2018.11.001",
url = "http://www.sciencedirect.com/science/article/pii/S0720048X18303966",
author = "N. {Große Hokamp} and V.C. Obmann and R. Kessner and K.R. Laukamp and T. Persigehl and S. Haneder and N. Ramaiya",
keywords = "Dual energy computed tomography, Spectral detector computed tomography, Liver lesions, Virtual monoenergetic images, 3D-printing",
abstract = "Objectives
The well-known boost of iodine associated-attenuation in low-keV virtual monoenergetic images (VMI_low) is frequently used to improve visualization of lesions and structures taking up contrast media. This study aimed to evaluate this concept in reverse. Hence to investigate if increased attenuation within the liver allows for improved visualization of little or not-enhancing lesions.
Methods
A 3D-printed phantom mimicking the shape of a human liver exhibiting a lesion in its center was designed and printed. Both, parenchyma- and lesion-mimic were filled with different solutions exhibiting 80/100/120HU and 0/15/40/60HU, respectively. Further, a total of 74 contrast-enhanced studies performed on a spectral detector CT scanner (SDCT) were included in this retrospective study. Patients had MRI or follow-up proven cysts and/or hypodense metastases. VMI of 40–200 keV as well as conventional images (CI) were reconstructed. ROI were placed in lesion and parenchyma(-mimics) on CI and transferred to VMI. Signal- and contrast-to-noise ratio were calculated (S-/CNR). Further, two radiologists independently evaluated image quality. Data was statistically assessed using ANOVA or Wilcoxon-test.
Results
In phantoms, S/CNR was significantly higher in VMI_low. The cyst-mimic in highly attenuating parenchyma-mimic on CI yielded a CNR of 6.4 ± 0.8; using VMI_40 keV, mildly hypodense lesion-mimic in poorly attenuating parenchyma-mimic exhibited a similar CNR (5.8 ± 0.9; p ≤ 0.05). The same tendency was observed in patients (cyst in CI/metastasis in VMI_40 keV: 4.4 ± 1.2/3.9 ± 1.8; p ≤ 0.05). Qualitative analysis indicated a benefit of VMI_40 keV (p ≤ 0.05).
Conclusions
VMI_low from SDCT allow for an improved visualization of hypodense focal liver lesions exploiting the concept of contrast blooming in reverse."
}
@article{TYRALIS2016262,
title = "Spatial Analysis of Electrical Energy Demand Patterns in Greece: Application of a GIS-based Methodological Framework",
journal = "Energy Procedia",
volume = "97",
pages = "262 - 269",
year = "2016",
note = "European Geosciences Union General Assembly 2016, EGU Division Energy, Resources & the Environment (ERE)",
issn = "1876-6102",
doi = "https://doi.org/10.1016/j.egypro.2016.10.071",
url = "http://www.sciencedirect.com/science/article/pii/S187661021631030X",
author = "Hristos Tyralis and Nikos Mamassis and Yorgos N. Photis",
keywords = "Keywords: cluster and outlier analysis, electrical energy demand, GIS, Greece, Gross Domestic Product, population, spatial analysis",
abstract = "We investigate various uses of the Electrical Energy Demand (EED) in Greece (agricultural, commercial, domestic, industrial use) and we examine their relationships with variables such as population and the Gross Domestic Product. The analysis is performed on data from the year 2012 and have spatial resolution down to the level of prefecture. We both visualize the results of the analysis and we perform spatial cluster and outlier analysis. The definition of the spatial patterns of the aforementioned variables in a GIS environment provides insight of the regional development model in Greece."
}
@article{TYRALIS2017340,
title = "Spatial analysis of the electrical energy demand in Greece",
journal = "Energy Policy",
volume = "102",
pages = "340 - 352",
year = "2017",
issn = "0301-4215",
doi = "https://doi.org/10.1016/j.enpol.2016.12.033",
url = "http://www.sciencedirect.com/science/article/pii/S0301421516306978",
author = "Hristos Tyralis and Nikos Mamassis and Yorgos N. Photis",
keywords = "Cluster and outlier analysis, Electrical Energy Demand, Gross Domestic Product, Grouping analysis, Hot Spot analysis, Spatial analysis",
abstract = "The Electrical Energy Demand (EED) of the agricultural, commercial and industrial sector in Greece, as well as its use for domestic activities, public and municipal authorities and street lighting are analysed spatially using Geographical Information System and spatial statistical methods. The analysis is performed on data which span from 2008 to 2012 and have annual temporal resolution and spatial resolution down to the NUTS (Nomenclature of Territorial Units for Statistics) level 3. The aim is to identify spatial patterns of the EED and its transformations such as the ratios of the EED to socioeconomic variables, i.e. the population, the total area, the population density and the Gross Domestic Product (GDP). Based on the analysis, Greece is divided in five regions, each one with a different development model, i.e. Attica and Thessaloniki which are two heavily populated major poles, Thessaly and Central Greece which form a connected geographical region with important agricultural and industrial sector, the islands and some coastal areas which are characterized by an important commercial sector and the rest Greek areas. The spatial patterns can provide additional information for policy decision about the electrical energy management and better representation of the regional socioeconomic conditions."
}
@article{COOL2010393,
title = "Gibbs: Phase equilibria and symbolic computation of thermodynamic properties",
journal = "Calphad",
volume = "34",
number = "4",
pages = "393 - 404",
year = "2010",
issn = "0364-5916",
doi = "https://doi.org/10.1016/j.calphad.2010.07.005",
url = "http://www.sciencedirect.com/science/article/pii/S0364591610000507",
author = "Thomas Cool and Alexander Bartol and Matthew Kasenga and Kunal Modi and R. Edwin García",
keywords = "Symbolic thermodynamics, Phase diagrams, CALPHAD, Interdiffusion, Phase field",
abstract = "A general purpose open source, Python-based framework, Gibbs, is presented to perform multiphysical equilibrium calculations of material properties. The developed architecture allows to prototype symbolic and numerical representations of materials by starting from analytic models, tabulated experimental data, or Thermo-Calc data files. These constructions are based on the addition of arbitrary energy contributions that range from the traditional thermochemical to mechanical and surface tension. Gibbs seamlessly interfaces with FiPy to prototype interdiffusion and microstructural evolution (phase field) models. Through its flexible Graphical User Interface, Gibbs allows rapid deployment of computational thermodynamic applications with intuitive user interfaces, and through the developed viewers, direct visualization and analysis of data can be readily performed for those physical properties that are relevant for the problem at hand. Example applications to chemically homogeneous ferroelectrics and two component (binary) solids are presented."
}
@article{GREENBERG201344,
title = "Sustain: An experimental test bed for building energy simulation",
journal = "Energy and Buildings",
volume = "58",
pages = "44 - 57",
year = "2013",
issn = "0378-7788",
doi = "https://doi.org/10.1016/j.enbuild.2012.11.026",
url = "http://www.sciencedirect.com/science/article/pii/S0378778812006299",
author = "Donald Greenberg and Kevin Pratt and Brandon Hencey and Nathaniel Jones and Lars Schumann and Justin Dobbs and Zhao Dong and David Bosworth and Bruce Walter",
keywords = "Software applications, Building energy modeling, Building energy simulation, Whole building energy analysis",
abstract = "Current building energy simulation technology requires extensive labor, time and expertise to create building energy models, substantial computational time for accurate simulations, and generates data in formats that make results difficult to interpret. These deficiencies can be ameliorated using modern graphical user interfaces and algorithms which take advantage of modern computer architectures and display capabilities. This paper describes a novel test bed environment which offers an interactive graphical interface, provides access to simulation modules that run at accelerated computational speeds, and presents new graphic visualization methods for the interpretation of simulation results. Its modular structure makes it suitable for use in early stage building design, for use as a research platform for the investigation of new simulation methods, and for use as a tool for teaching concepts of sustainable design. Improvements in the accuracy and execution speed of many of the simulation modules are based on the modification of advanced computer graphics rendering algorithms. Significant performance improvements are illustrated in several computationally expensive energy simulation modules."
}
@article{HOSSAIN2020800,
title = "Towards energy-aware cloud-oriented cyber-physical therapy system",
journal = "Future Generation Computer Systems",
volume = "105",
pages = "800 - 813",
year = "2020",
issn = "0167-739X",
doi = "https://doi.org/10.1016/j.future.2017.08.045",
url = "http://www.sciencedirect.com/science/article/pii/S0167739X1731885X",
author = "M. Shamim Hossain and Md. Abdur Rahman and Ghulam Muhammad",
keywords = "Cyber-physical therapy system, Big data, Health sensors, Motion tracking devices, Therapeutic kinematic data, Energy efficiency",
abstract = "The Cyber-Physical System (CPS) is an emerging computing technology that involves in sensing, computing, controlling and communication between physical components (e.g., smart sensors, devices, systems and human beings) and cyber components (e.g., cloud and big data centers). The sensing, controlling and interaction have significant promises towards the realization of the current and future therapy system, where cloud resources and data centers are expected to process complex therapeutic heterogeneous big data. Although, the CPS has a great potential for such a sensing and controlling of therapy, however, energy efficiency is crucial for such a therapy system for its sustainability, especially for elderly people who cannot physically optimize energy consumptions. To this end, this article proposes an energy-aware cyber-physical therapy system (T-CPS), which incorporates smart things and devices in both the physical and cyber world for therapy sensing. To provide energy-efficient affordable therapeutic services, the T-CPS framework uses multi-modal sensing for the provision of therapy sensing, therapy playback, annotation, visualization, and energy efficiency. The framework was evaluated by real subjects along with several therapists. Test results show the usefulness of the proposed T-CPS framework."
}
@article{CHEONG2019230,
title = "Platform for wireless pressure sensing with built-in battery and instant visualization",
journal = "Nano Energy",
volume = "62",
pages = "230 - 238",
year = "2019",
issn = "2211-2855",
doi = "https://doi.org/10.1016/j.nanoen.2019.05.047",
url = "http://www.sciencedirect.com/science/article/pii/S2211285519304537",
author = "Woon Hyung Cheong and Byungkook Oh and Se-Hee Kim and Jiuk Jang and Sangyoon Ji and Seunghee Lee and Jinwoo Cheon and Seunghyup Yoo and Sang-Young Lee and Jang-Ung Park",
keywords = "Wireless communication, Battery, Display, Packaging, Wearable electronics, Pressure sensor",
abstract = "Wireless communication through linkage with a smartphone and other portable devices in the sensor area are essential for increasing the efficiency of utilization by storing sensing-value data. Thus, the demand for wireless technology is increasing due to the advantages it provides for the various applications that use these data. However, there is still considerable ambiguity concerning the low portability of such technology due to the increased volume with component integration, the high consumption of power, and the necessity of having a separate battery. Herein, we present solutions for these problems with demonstrations that involve 1) the miniaturization of the device by altering the structure of the built-in battery, 2) the use of a pressure-activated switch for the low-power driving technology, and 3) the implementation of a wireless communication platform by integrating a Bluetooth module with the devices. In addition, we demonstrate a human-interactive display that enables users to instantly observe the changes in the brightness of the organic light-emitting diodes (OLED) as the pressure changes. We show that the system can display the measured, real-time pressure values on the screens of mobile devices, which represents a significant advancement in the fields of energy science and biomedical science."
}
@article{DOCA2014131,
title = "Smart meters and energy savings in Italy: Determining the effectiveness of persuasive communication in dwellings",
journal = "Energy Research & Social Science",
volume = "3",
pages = "131 - 142",
year = "2014",
issn = "2214-6296",
doi = "https://doi.org/10.1016/j.erss.2014.07.015",
url = "http://www.sciencedirect.com/science/article/pii/S2214629614000930",
author = "Simona D’Oca and Stefano P. Corgnati and Tiziana Buso",
keywords = "Home energy saving, Persuasive communication, Smart metering, Electricity consumption",
abstract = "To secure a sustainable energy development in the residential sector, attitudes and human behavior need to be modified toward more efficient and conscious energy usage. The goal of this research is to assess evaluations and to test the effectiveness in reducing domestic electricity consumption. The aim of the smart monitoring system we evaluate is to provide households with a user-friendly tool that improves awareness of energy behavior in homes, enabling better management via the visualization of consumption and persuasive tailored information on domestic electricity use. In our study, the system was tested on 31 Italian families selected among volunteers all over Italy, participating to the first trial phase from October 2012 to November 2013. A combination of persuasive communication strategies such as graphical real-time and historical feedback based on real data and comparison tools to encourage competitiveness against “similar” households were provided to users through a domestic user-friendly interface. In addition, personalized energy saving prompts were sent via web-newsletters to trial users. The study concludes that energy related persuasive communication is effective in reducing electricity consumption in dwellings on average −18% and up to −57%."
}
@article{GUDLAUGSSON2020168,
title = "Classification of stakeholders of sustainable energy development in Iceland: Utilizing a power-interest matrix and fuzzy logic theory",
journal = "Energy for Sustainable Development",
volume = "57",
pages = "168 - 188",
year = "2020",
issn = "0973-0826",
doi = "https://doi.org/10.1016/j.esd.2020.06.006",
url = "http://www.sciencedirect.com/science/article/pii/S0973082620302593",
author = "Bjarnhéðinn Guðlaugsson and Reza Fazeli and Ingunn Gunnarsdóttir and Brynhildur Davidsdottir and Gunnar Stefansson",
keywords = "Stakeholder analysis, Stakeholder power-interest matrix, Sustainable energy development, Stakeholder classification, Fuzzy logic",
abstract = "A transition towards a sustainable energy system is a challenging process that involves multiple stakeholders with differing viewpoints. Several transition pathways are possible, but the decision regarding which pathway to follow should be based on a justifiable and transparent framework that accounts for the concerns of all relevant stakeholder groups. This study proposes a participatory stakeholder analysis process that includes a two-dimensional power-interest matrix and fuzzy logic to define and classify relevant stakeholder groups. The proposed framework was implemented to recognize the relevant stakeholder groups of the Icelandic energy system and assess their salience. The defined stakeholder groups were then evaluated based on two attributes—power and interest—using data collected from a web-based questionnaire. Considering the wide range of the attributes' values for the many stakeholder groups, fuzzy logic was applied to plot a 3D decision surface, which provided a more accurate assessment of stakeholders' salience. The generated power-interest map visualizes the dynamic influential level of stakeholder groups when it comes to decision-making concerning sustainable energy development. The fuzzy decision surface shows a more accurate image of the difference in the degree of power and interest and saliences of the stakeholder group. Results reveal that Decision-makers, Industrial Users, Professional Interest groups, and Energy Producers have the highest salience and are the most influential stakeholder groups concerning decision-making. Less influential stakeholder group such as Landowners exhibit a high degree of variation in influential level across the sustainable energy themes. The findings suggest that Icelandic decision-makers responsible for energy development face a complex challenge when it comes to designing and implementing a policy that is approved across all stakeholder groups. This study offers new insights to policymakers on the most influential stakeholder groups and the variation of influential level across stakeholders groups in the Icelandic energy system. Understanding the power and interest of different stakeholders can improve the decision- and policy-making process and promote a successful transition to a sustainable energy system."
}
@article{MULLER201756,
title = "Fluid–structure interaction mechanisms leading to dangerous power swings in Francis turbines at full load",
journal = "Journal of Fluids and Structures",
volume = "69",
pages = "56 - 71",
year = "2017",
issn = "0889-9746",
doi = "https://doi.org/10.1016/j.jfluidstructs.2016.11.018",
url = "http://www.sciencedirect.com/science/article/pii/S0889974616303401",
author = "A. Müller and A. Favrel and C. Landry and F. Avellan",
keywords = "Pressure surge, Self-oscillation, Francis turbines, Full load, Cavitation, Swirling flows, Laser Doppler Velocimetry (LDV)",
abstract = "Hydropower plants play an important regulatory role in the large scale integration of volatile renewable energy sources into the existing power grid. This duty however requires a continuous extension of their operating range, provoking the emergence of complex flow patterns featuring cavitation inside the turbine runner and the draft tube. When the power output is maximized at full load, self-excited pressure oscillations in the hydraulic system may occur, which translate into significant electrical power swings and thus pose a serious threat to the grid stability as well as to the operational safety of the machine. Today's understanding of the underlying fluid–structure interaction mechanisms is incomplete, yet crucial to the development of reliable numerical flow models for stability analysis, and for the design of potential countermeasures. This study therefore reveals how the unsteady flow inside the machine forces periodic mechanical loads onto the runner shaft. For this purpose, the two-phase flow field at the runner exit is investigated by Laser Doppler Velocimetry and high-speed visualizations, which are then compared to the simultaneously measured wall pressure oscillations in the draft tube cone and the mechanical torque on the runner shaft. The results are presented in the form of a comprehensive, mean phase averaged evolution of the relevant hydro-mechanical data over one period of the instability. They show that the flow in the runner, and thus the resulting torque applied to the shaft, is critically altered by a cyclic growth, shedding and complete collapse of cavitation on the suction side of the runner blades. This is accompanied by a significant flow swirl variation in the draft tube cone, governing the characteristic breathing motion of the cavitation vortex rope."
}
@article{SULLIVAN201570,
title = "Combining geographic information systems and ethnography to better understand and plan ocean space use",
journal = "Applied Geography",
volume = "59",
pages = "70 - 77",
year = "2015",
issn = "0143-6228",
doi = "https://doi.org/10.1016/j.apgeog.2014.11.027",
url = "http://www.sciencedirect.com/science/article/pii/S0143622814002823",
author = "Colleen M. Sullivan and Flaxen D.L. Conway and Caroline Pomeroy and Madeleine Hall-Arber and Dawn J. Wright",
keywords = "Ethnography, GIS, Marine spatial planning, Ocean space use, P-GIS, Participatory mapping",
abstract = "Agencies in the US with oversight for marine renewable energy development idealistically have sought space where this new use might proceed unhindered by other uses. Despite experiential evidence of spatial overlap among existing ocean uses, a lack of documentation made the identification of potential space-use conflicts, communication between existing and potential ocean users, and the design of mitigation exceedingly challenging. We conducted a study along the US Atlantic and Pacific coasts to gather and document available spatial information on existing use through a compilation and organization of geographic information system (GIS) data. Stakeholder group meetings were used to vet the collected spatial data, and ethnographic interviews were conducted to gather knowledge and cultural perspectives. Results show extensive coverage and overlap of existing ocean space uses and provide a visualization of the social and cultural landscape of the ocean that managers can use to determine which stakeholders to engage. Marine resource managers are encouraged to recognize that marine space use is dynamic and multi-dimensional and as such research thereof requires a balance between the efficiency of GIS and the stories captured and told by ethnographic research. There are important linkages within and across fisheries and other uses, communities and interests, and across the land–sea interface. Therefore, it is important to use techniques demonstrated in this research that (1) integrate ethnographic and geospatial data collection and analysis; (2) engage stakeholders throughout the process; and (3) recognize the unique qualities of each geographic location and user group to support sound decision-making."
}
@article{LEA2018921,
title = "Data visualization for assessing the biofuel commercialization potential within the business intelligence framework",
journal = "Journal of Cleaner Production",
volume = "188",
pages = "921 - 941",
year = "2018",
issn = "0959-6526",
doi = "https://doi.org/10.1016/j.jclepro.2018.02.288",
url = "http://www.sciencedirect.com/science/article/pii/S0959652618306267",
author = "Bih-Ru Lea and Wen-Bin Yu and Hokey Min",
keywords = "Sustainability, Data visualization, Alternative fuel, Business intelligence",
abstract = "With the ever-increasing energy demand, triggered by the continued population growth and accelerated industrial revolution, renewable energy has emerged as the world's fastest-growing energy source. Renewable energy's popularity has grown because it is environmentally friendly and abundant in natural environments. Despite its enormous potential as a viable alternative to traditional (fossil fuel-based) energy sources, renewable energy has rarely been commercialized and utilized. Its lack of commercialization has something to do with a lack of evidence proving its eco- and cost-efficiency. With this in mind, this paper aims to assess the eco- and cost-efficiency of renewable energy such as algae-based biofuels using data visualization. It also intends to help increase public awareness and facilitate the commercialization of renewable energy such as biofuels. Through experiments, this paper found that the success of biofuel commercialization hinged on temperature, light intensity, and algae strain. Another important finding is that the low carbon footprint resulting from biofuel consumption may not directly contribute to the immediate revenue growth of a biofuel producing company, but it can foster a long-term positive image that will help attract more customers in the future with increased brand recognition. Furthermore, this paper evaluates the effectiveness, the level of user involvement, and the usability of two data visualization tools built upon the dashboard and the balanced scorecard. Based on the case study, this paper demonstrates how effective and useful the tools are in communicating the firm's strategic goals toward sustainability and thus provides easier practical guidelines for renewable energy development decisions."
}
@article{DINH2019120,
title = "Automated visualization of concrete bridge deck condition from GPR data",
journal = "NDT & E International",
volume = "102",
pages = "120 - 128",
year = "2019",
issn = "0963-8695",
doi = "https://doi.org/10.1016/j.ndteint.2018.11.015",
url = "http://www.sciencedirect.com/science/article/pii/S0963869518303451",
author = "Kien Dinh and Nenad Gucunski and Tarek Zayed",
keywords = "Concrete, Corrosion, Ground-penetrating radar, Nondestructive evaluation, Condition assessment, Bridge inspection, Automation",
abstract = "Ground-penetrating radar (GPR) is one of the most commonly used technologies for condition assessment of concrete bridge decks. However, there have been no fully automated algorithms to visualize the data collected with this technique. In such context, the current paper presents a method for a full automation of GPR data visualization and analysis. Based on the background removal, depth correction, synthetic aperture focusing technique (SAFT), and interpolation algorithms, this automated method produces a plan view map of amplitude of GPR signals. In the obtained map, two types of information are observed at the same time. First, as the strongest reflectors of electromagnetic energy, rebars will appear as the most visible. Second, the areas of corrosive environment and, thus, likely corrosion, will be detected as having low amplitude rebar reflections. As a proof of concept, the proposed method was implemented for two bare concrete bridge decks and two concrete bridge decks with asphalt overlays. In all cases, the results obtained were excellent where the maps pinpointed the areas affected by corrosion. These areas were confirmed by other methods of evaluation, such as electrical resistivity (ER), half-cell potential (HCP), chloride analysis of core samples, or visual inspection. With the demonstrated performance, the proposed method is expected to be an excellent alternative to the available methods of GPR data evaluation and visualization. In the future, it should be improved to provide an indication of corrosion severity/probability at each deck location."
}
@article{MENG2018309,
title = "multiflexxlib: A Python package for data reduction and visualization for the cold-neutron multi energy wide angle analyzer MultiFLEXX",
journal = "SoftwareX",
volume = "7",
pages = "309 - 312",
year = "2018",
issn = "2352-7110",
doi = "https://doi.org/10.1016/j.softx.2018.09.006",
url = "http://www.sciencedirect.com/science/article/pii/S2352711018301055",
author = "Siqin Meng and Rasmus Toft-Petersen and Lijie Hao and Klaus Habicht",
keywords = "Inelastic neutron scattering, Three-axis spectroscopy, Visualization tool",
abstract = "A Python software package for data reduction and visualization of continuous angle multiple energy analysis (CAMEA) type detector backend MultiFLEXX is presented. The software concept focuses on unambiguous, automated aggregation of experimental data and preservation of raw data structure in graphical representation, enabling on-the-fly analysis of experimental data from MultiFLEXX with minimal amount of user input, reducing confusion and human error in studies involving multiple parameters. The software also provides a set of interfaces for versatile tweaking of graphing parameters, facilitating generation of production-quality graphs for use in publications. The software enhances the role of MultiFLEXX as a swift mapping option available at the cold-neutron triple-axis spectrometer FLEXX."
}
@article{ULLAH2018324,
title = "Simulator for modeling, analysis, and visualizations of thermal status in data centers",
journal = "Sustainable Computing: Informatics and Systems",
volume = "19",
pages = "324 - 340",
year = "2018",
issn = "2210-5379",
doi = "https://doi.org/10.1016/j.suscom.2017.12.005",
url = "http://www.sciencedirect.com/science/article/pii/S2210537917301907",
author = "Rahmat Ullah and Naveed Ahmad and Saif U.R. Malik and Saeed Akbar and Adeel Anjum",
keywords = "Data center, Simulator, Modeling, Visualization, Prediction",
abstract = "Data centers (DC) are richly instrumented systems consists of highly coupled elements that store and process a large amount of data. To perform large computation and storage, a DC is equipped with more than thousands of servers or even more. Due to a large number of these computational devices put in use at DC, produces a large amount of heat. Therefore the cost to maintain the thermal balance in a DC has increased significantly and has become almost equal to the cost of operating these systems. The main problem in heat management is ‘Hot spots creation’ which can cause hardware inefficiencies, and operational disruptions and in turn have a negative impact on overall functionality. To address these issues this paper aims at decreasing energy consumption of DC by allowing administrators, designers, and planners to model, visualize and analyze the thermal status of various configurations and solutions. A major difficulty in the thermal analysis of DC is the lack of simulation tool, where the impact of design (layout) and workload on thermal status can be tested. Therefore there is a need for a simulator that approaches the problem from an end-user perspective and takes into account all the factors that are critical to analyzing thermal balance in DC. A simulator is developed that takes DC computational devices as input and provide models that allow designers to analyze and visualize DC thermal conditions. The simulator allows the user to apply different job allocation strategies and can analyze thermal status for each; that help them choose the best strategy for job allocation. It enables DC administrator to organize servers and racks before their actual implementation. Moreover, servers can be relocated to analyze and maintain thermal balance. The user will also able to predict thermal condition after a specific time period. The simulator enables a DC user/administrator to maintain thermal balance in DC by detecting the root cause of hot spots under different workload; which will help them make better workload balancing decisions."
}
@article{MENARD2015113,
title = "Interoperable Exchange of Surface Solar Irradiance Observations: A Challenge",
journal = "Energy Procedia",
volume = "76",
pages = "113 - 120",
year = "2015",
note = "European Geosciences Union General Assembly 2015 - Division Energy, Resources and Environment, EGU 2015",
issn = "1876-6102",
doi = "https://doi.org/10.1016/j.egypro.2015.07.867",
url = "http://www.sciencedirect.com/science/article/pii/S1876610215016434",
author = "Lionel Ménard and Daniel Nüst and Khai- {Minh Ngo} and Philippe Blanc and Simon Jirka and Joan Masó and Thierry Ranchin and Lucien Wald",
keywords = "GEOSS, web, spatial data infrastructure, open data, sensor web enablement, solar energy, surface solar irradiance, SSI",
abstract = "We present how implementations of the Sensor Web Enablement framework of the Open Geospatial Consortium are integrated into an existing spatial data infrastructure. The result is registered as a community portal for professionals in solar energy in the GEOSS Common Infrastructure, demonstrating the benefits of interoperable exchange of in-situ time-series observations of surface solar irradiation. Easy access to, and sharing of data improves the information base for planning and monitoring of solar power resources. Providing users with visualization and download functionality for in-situ measurements is a key aspect for engaging the energy community to share, release and integrate in-situ measurements."
}
@article{KATTA2020104485,
title = "Development of disaggregated energy use and greenhouse gas emission footprints in Canada’s iron, gold, and potash mining sectors",
journal = "Resources, Conservation and Recycling",
volume = "152",
pages = "104485",
year = "2020",
issn = "0921-3449",
doi = "https://doi.org/10.1016/j.resconrec.2019.104485",
url = "http://www.sciencedirect.com/science/article/pii/S092134491930391X",
author = "Anil Kumar Katta and Matthew Davis and Amit Kumar",
keywords = "Mining, Energy intensity, Energy demand, Greenhouse gas emissions, Sankey diagram",
abstract = "This study develops the disaggregated energy use and greenhouse gas (GHG) emission footprint for Canada’s iron, gold, and potash mining sectors. Currently, only high-level aggregated data at the sectoral and regional levels exists in the literature. Through bottom-up energy demand tree development, we identified end-use processes for each mining operation in these sectors. The energy intensities for each end-user were calculated and used in a bottom-up energy-environmental model to determine the associated end-use process GHG emissions. The results were then used to develop Sankey diagrams that allow us to visualize the energy and GHG emissions flows from resource to end use by energy use sector, fuel type, and various jurisdictions in Canada. The overall energy and GHG emission intensities for iron, gold, and potash mining are 0.7, 149.8, 1.8 GJ/Mg and 33, 4922, 158 kg CO2 eq./Mg, respectively. Firing, ventilation, and product drying and steam generation end-use devices had the highest energy use share of 42%, 20%, and 47% in iron, gold, and potash mining sectors, respectively, in 2016. Firing in iron mining, ore transport in gold mining, and product drying and steam generation in potash mining were responsible for 66%, 22%, and 34% of the respective total sectoral GHG emissions. 56% of the GHG emissions were from Saskatchewan, followed by Quebec (18%), and Newfoundland and Labrador (14%). The results from this study provide benchmarks to develop energy savings and GHG mitigation strategies useful for decision making."
}
@article{VANGALEN2018325,
title = "The role of central dopamine and serotonin in human obesity: lessons learned from molecular neuroimaging studies",
journal = "Metabolism",
volume = "85",
pages = "325 - 339",
year = "2018",
issn = "0026-0495",
doi = "https://doi.org/10.1016/j.metabol.2017.09.007",
url = "http://www.sciencedirect.com/science/article/pii/S0026049517302512",
author = "Katy A. {van Galen} and Kasper W. {ter Horst} and Jan Booij and Susanne E. {la Fleur} and Mireille J. Serlie",
keywords = "Dopamine, Serotonin, Obesity, Neuroimaging, Humans",
abstract = "Obesity results from an imbalance between energy intake and expenditure, and many studies have aimed to determine why obese individuals continue to (over)consume food under conditions of caloric excess. The two major “neurotransmitter hypotheses” of obesity state that increased food intake is partially driven by decreased dopamine-mediated reward and decreased serotonin-mediated homeostatic feedback in response to food intake. Using molecular neuroimaging studies to visualize and quantify aspects of the central dopamine and serotonin systems in vivo, recent PET and SPECT studies have also implicated alterations in these systems in human obesity. The interpretation of these data, however, is more complex than it may appear. Here, we discuss important characteristics and limitations of current radiotracer methods and use this framework to comprehensively review the available human data on central dopamine and serotonin in obesity. On the basis of the available evidence, we conclude that obesity is associated with decreased central dopaminergic and serotonergic signaling and that future research, especially in long-term follow-up and interventional settings, is needed to advance our understanding of the neuronal pathophysiology of obesity in humans."
}

@article{WANG2013108,
title = "Smad7 foci are present in micronuclei induced by heavy particle radiation",
journal = "Mutation Research/Genetic Toxicology and Environmental Mutagenesis",
volume = "756",
number = "1",
pages = "108 - 114",
year = "2013",
note = "From DNA Damage to Chromosomal Aberrations",
issn = "1383-5718",
doi = "https://doi.org/10.1016/j.mrgentox.2013.04.011",
url = "http://www.sciencedirect.com/science/article/pii/S1383571813001137",
author = "Minli Wang and Janapriya Saha and Francis A. Cucinotta",
keywords = "High LET radiation, DNA repair, Micronuclei, Smad7, TGFβ/Smad pathway",
abstract = "DNA damage and reactive oxygen species (ROS) generated by ionizing radiation (IR) activate DNA damage response (DDR) and cytokine signaling pathways, including double strand break (DSB) repair and TGFβ/Smad signaling pathway. Proteins assembled at IR-induced DSB sites can be visualized as foci, including γH2AX, 53BP1, ATM and ATF2. Unrepaired DSBs are thought to be one origin of micronuclei (MN), an indicator of genotoxic stress and chromosomal instability. Studies have detected γH2AX in IR-induced MN, indicating the presence of DSB in MN. Previously we reported that TGFβ downstream proteins Smad7 and phospho-Smad2 (pSmad2) co-localized with DDR proteins following radiation. Here we studied the status of Smad7 and pSmad2 in MN post high linear energy transfer (LET) radiation in human normal and cancerous cells. We observed γH2AX foci in IR-induced MN, whereas 53BP1 and ATF2 were absent. Interestingly, Smad7 foci, but not pSmad2, were detectable in both spontaneous and IR-induced MN. We compared the effect of particle track structures on the yield of MN using 5.6MeV/u boron (B) and 600MeV/u iron (Fe) particles with similar LET (200 and 180keV/μm, respectively) in human fibroblasts. The frequency of MN induced by B was lower than that by Fe particles, albeit the proportion of Smad7-positive to Smad7-negative MN remained constant. An increased frequency of spontaneous MN, with slightly higher ratio of Smad7 or γH2AX positive, was found in human prostate cancer cells (PC3) compared to normal cells. 24h after 1Gy of Fe particles exposure, the yield of MN increased, and the majority (∼70%) carried γH2AX and Smad7. Phospho-ATM (Ser1981) foci were found in both spontaneous and IR-induced MN in PC3 cells, displaying a much lower frequency compared to γH2AX and Smad7. Our data suggest a unique role of Smad7 in IR-induced MN formation, which may associate with DNA repair, apoptosis and genomic instability."
}
@article{WEIJERMARS20114667,
title = "Can we close Earth's sustainability gap?",
journal = "Renewable and Sustainable Energy Reviews",
volume = "15",
number = "9",
pages = "4667 - 4672",
year = "2011",
issn = "1364-0321",
doi = "https://doi.org/10.1016/j.rser.2011.07.085",
url = "http://www.sciencedirect.com/science/article/pii/S1364032111003303",
author = "Ruud Weijermars",
keywords = "Ecological footprint, Sustainable energy solutions, Ecosystem services, Ecological dilemma matrix, Scenarios, Technophiles, Biophiles",
abstract = "The principal options for engineering Earth's ecological future can be concisely visualized in a conceptual dilemma matrix. Scaling of the matrix with real world data confirms the widening of Earth's sustainability gap, due to our increasing ecological footprint. The simplicity of the dilemma matrix articulated here may help to focus the debate at future Global Summits and World Future Energy Summits on the critical scenario options. Geoscientists and engineers at energy companies share a major responsibility with many societal actors in setting the right example, particularly in searching for sustainable energy solutions. One view is that technology can help solve all issues. Another view is that nature is needed for sustainable ecosystem services. A most pessimistic view is based on analogy of human behavior with that of ants – eusocial groups like ourselves. Ant wars for access to limited resources warn us for a future where scarcity of resources may force us to resort to brutal, competitive behavior – rather than civilized diplomacy. Options to avoid such an outcome are outlined in this study."
}
@article{PERLES2018566,
title = "An energy-efficient internet of things (IoT) architecture for preventive conservation of cultural heritage",
journal = "Future Generation Computer Systems",
volume = "81",
pages = "566 - 581",
year = "2018",
issn = "0167-739X",
doi = "https://doi.org/10.1016/j.future.2017.06.030",
url = "http://www.sciencedirect.com/science/article/pii/S0167739X17313663",
author = "Angel Perles and Eva Pérez-Marín and Ricardo Mercado and J. Damian Segrelles and Ignacio Blanquer and Manuel Zarzo and Fernando J. Garcia-Diego",
keywords = "Cultural heritage, Preventive conservation, Internet of things",
abstract = "Internet of Things (IoT) technologies can facilitate the preventive conservation of cultural heritage (CH) by enabling the management of data collected from electronic sensors. This work presents an IoT architecture for this purpose. Firstly, we discuss the requirements from the artwork standpoint, data acquisition, cloud processing and data visualization to the end user. The results presented in this work focuses on the most critical aspect of the architecture, which are the sensor nodes. We designed a solution based on LoRa and Sigfox technologies to produce the minimum impact in the artwork, achieving a lifespan of more than 10 years. The solution will be capable of scaling the processing and storage resources, deployed either in a public or on-premise cloud, embedding complex predictive models. This combination of technologies can cope with different types of cultural heritage environments."
}
@article{LIZARBE2013448,
title = "Imaging hypothalamic activity using diffusion weighted magnetic resonance imaging in the mouse and human brain",
journal = "NeuroImage",
volume = "64",
pages = "448 - 457",
year = "2013",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2012.09.033",
url = "http://www.sciencedirect.com/science/article/pii/S1053811912009433",
author = "Blanca Lizarbe and Ania Benítez and Manuel Sánchez-Montañés and Luis F. Lago-Fernández and María L. Garcia-Martin and Pilar López-Larrubia and Sebastián Cerdán",
keywords = "Appetite regulation, Functional imaging, Cerebral activation, Diffusion weighted MRI, Image analysis, Biexponential diffusion",
abstract = "Hypothalamic appetite regulation is a vital homeostatic process underlying global energy balance in animals and humans, its disturbances resulting in feeding disorders with high morbidity and mortality. The objective evaluation of appetite remains difficult, very often restricted to indirect measurements of food intake and body weight. We report here, the direct, non‐invasive visualization of hypothalamic activation by fasting using diffusion weighted magnetic resonance imaging, in the mouse brain as well as in a preliminary study in the human brain. The brain of fed or fasted mice or humans were imaged at 7 or 1.5Tesla, respectively, by diffusion weighted magnetic resonance imaging using a complete range of b values (10<b<2000s.mm−2). The diffusion weighted image data sets were registered and analyzed pixel by pixel using a biexponential model of diffusion, or a model-free Linear Discriminant Analysis approach. Biexponential fittings revealed statistically significant increases in the slow diffusion parameters of the model, consistent with a neurocellular swelling response in the fasted hypothalamus. Increased resolution approaches allowed the detection of increases in the diffusion parameters within the Arcuate Nucleus, Ventromedial Nucleus and Dorsomedial Nucleus. Independently, Linear Discriminant Analysis was able to classify successfully the diffusion data sets from mice and humans between fed and fasted states. Present results are consistent with increased glutamatergic neurotransmission during orexigenic firing, a process resulting in increased ionic accumulation and concomitant osmotic neurocellular swelling. This swelling response is spatially extendable through surrounding astrocytic networks until it becomes MRI detectable. Present findings open new avenues for the direct, non‐invasive, evaluation of appetite disorders and other hypothalamic pathologies helping potentially in the development of the corresponding therapies."
}
@article{HURRY20161,
title = "Atmospheric monitoring and detection of fugitive emissions for Enhanced Oil Recovery",
journal = "International Journal of Greenhouse Gas Control",
volume = "45",
pages = "1 - 8",
year = "2016",
issn = "1750-5836",
doi = "https://doi.org/10.1016/j.ijggc.2015.11.031",
url = "http://www.sciencedirect.com/science/article/pii/S1750583615301468",
author = "Jacquelyn Hurry and David Risk and Martin Lavoie and Bjørn-Gustaf Brooks and Claire L. Phillips and Mathias Göckede",
keywords = "Carbon Capture Utilization and Storage, Enhanced Oil Recovery, Multi-gas, Atmospheric, Monitoring",
abstract = "In Weyburn, Saskatchewan, carbon dioxide (CO2) is injected into the Weyburn oilfield for Enhanced Oil Recovery (EOR). Cenovus Energy Inc. operates more than 1000 active wells, processing plants, and hundreds of kilometres of pipeline infrastructure over a >100km2 area. While vehicle-based atmospheric detection of gas leakage would be convenient for a distributed operation such as Weyburn, implementing atmospheric detection schemes, particularly those that target CO2, are challenging in that natural ecosystems and other human activities both emit CO2 and will contribute to regular false positives. Here we present field test results of a multi-gas atmospheric detection technique that uses observed trace gas ratios (CO2, CH4, and H2S) to discriminate plumes of gas originating from different sources. This work is part of a larger project focused on multi-scale fugitive emissions detection and plume discrimination. During 2013 and 2014, we undertook vehicle-based mobile surveys of CO2, CH4, H2S, and δ 13CH4, in the Weyburn oilfield, using customized Cavity Ring Down Spectroscopy (CRDS) instruments that also alternated as stationary receptors. Mobile surveys provided georeferenced observations of atmospheric gas concentrations every 20–30m, along a route driven at roughly 70kmh−1. Data were uploaded to remote servers and processed using visualization tools that allowed us to constrain the location and timing of potential emission events. Results from one day of mobile surveying, September 24, 2013, are presented here to illustrate how industrial activities, combustion engine and flare stack source emissions can be discriminated on the basis of excess mixing gas ratios, at distances from a few hundreds metres, to kilometres, in the Weyburn oilfield."
}
@article{SGARBI2012181,
title = "Pre-processing of data coming from a laser-EMAT system for non-destructive testing of steel slabs",
journal = "ISA Transactions",
volume = "51",
number = "1",
pages = "181 - 188",
year = "2012",
issn = "0019-0578",
doi = "https://doi.org/10.1016/j.isatra.2011.07.004",
url = "http://www.sciencedirect.com/science/article/pii/S0019057811000772",
author = "Mirko Sgarbi and Valentina Colla and Sivia Cateni and Stuart Higson",
keywords = "Non destructive testing systems, Surface defect detection, Signal processing",
abstract = "Non destructive test systems are increasingly applied in the industrial context for their strong potentialities in improving and standardizing quality control. Especially in the intermediate manufacturing stages, early detection of defects on semi-finished products allow their direction towards later production processes according to their quality, with consequent considerable savings in time, energy, materials and work. However, the raw data coming from non destructive test systems are not always immediately suitable for sophisticated defect detection algorithms, due to noise and disturbances which are unavoidable, especially in harsh operating conditions, such as the ones which are typical of the steelmaking cycle. The paper describes some pre-processing operations which are required in order to exploit the data coming from a non destructive test system. Such a system is based on the joint exploitation of Laser and Electro-Magnetic Acoustic Transducer technologies and is applied to the detection of surface and sub-surface cracks in cold and hot steel slabs."
}
@article{LUPELLI2015835,
title = "Provenance metadata gathering and cataloguing of EFIT++ code execution",
journal = "Fusion Engineering and Design",
volume = "96-97",
pages = "835 - 839",
year = "2015",
note = "Proceedings of the 28th Symposium On Fusion Technology (SOFT-28)",
issn = "0920-3796",
doi = "https://doi.org/10.1016/j.fusengdes.2015.04.016",
url = "http://www.sciencedirect.com/science/article/pii/S0920379615002458",
author = "I. Lupelli and D.G. Muir and L. Appel and R. Akers and M. Carr and P. Abreu",
keywords = "Provenance, Metadata, Data Management",
abstract = "Journal publications, as the final product of research activity, are the result of an extensive complex modeling and data analysis effort. It is of paramount importance, therefore, to capture the origins and derivation of the published data in order to achieve high levels of scientific reproducibility, transparency, internal and external data reuse and dissemination. The consequence of the modern research paradigm is that high performance computing and data management systems, together with metadata cataloguing, have become crucial elements within the nuclear fusion scientific data lifecycle. This paper describes an approach to the task of automatically gathering and cataloguing provenance metadata, currently under development and testing at Culham Center for Fusion Energy. The approach is being applied to a machine-agnostic code that calculates the axisymmetric equilibrium force balance in tokamaks, EFIT++, as a proof of principle test. The proposed approach avoids any code instrumentation or modification. It is based on the observation and monitoring of input preparation, workflow and code execution, system calls, log file data collection and interaction with the version control system. Pre-processing, post-processing, and data export and storage are monitored during the code runtime. Input data signals are captured using a data distribution platform called IDAM. The final objective of the catalogue is to create a complete description of the modeling activity, including user comments, and the relationship between data output, the main experimental database and the execution environment. For an intershot or post-pulse analysis (∼1000 time slices, 65×65 grid, mpi execution n=8 cores) of a typical MAST pulse, the overhead in the code runtime caused by the Provenance Metadata Gathering System is less than 10%, the metadata/data size ratio is about ∼20%, which we consider to be reasonable according to the present literature. A visualization interface based on Gephi for catalogue interrogation, will be presented."
}
@article{TRILLOMONTERO2014144,
title = "Development of a software application to evaluate the performance and energy losses of grid-connected photovoltaic systems",
journal = "Energy Conversion and Management",
volume = "81",
pages = "144 - 159",
year = "2014",
issn = "0196-8904",
doi = "https://doi.org/10.1016/j.enconman.2014.02.026",
url = "http://www.sciencedirect.com/science/article/pii/S0196890414001459",
author = "D. Trillo-Montero and I. Santiago and J.J. Luna-Rodriguez and R. Real-Calvo",
keywords = "Grid connected PV installations, Software development, Monitoring data, Analysis of PV data",
abstract = "The aim of this paper was to design and develop a software application that enables users to perform an automated analysis of data from the monitoring of grid-connected photovoltaic (PV) systems. This application integrates data from all devices already in operation such as environmental sensors, inverters and meters, which record information on typical PV installations. This required the development of a Relational Database Management System (RDBMS), consisting of a series of linked databases, enabling all PV system information to be stored; and a software, called S·lar, which enables all information from the monitoring to be automatically migrated to the database as well as determining some standard magnitudes related to performances and losses of PV installation components at different time scales. A visualization tool, which is both graphical and numerical, makes access to all of the information be a simple task. Moreover, the application enables relationships between parameters and/or magnitudes to be easily established. Furthermore, it can perform a preliminary analysis of the influence of PV installations on the distribution grids where the produced electricity is injected. The operation of such a software application was implemented by performing the analysis of two grid-connected PV installations located in Andalusia, Spain, via data monitoring therein. The monitoring took place from January 2011 to May 2012."
}
@article{KALT2015100,
title = "Biomass streams in Austria: Drawing a complete picture of biogenic material flows within the national economy",
journal = "Resources, Conservation and Recycling",
volume = "95",
pages = "100 - 111",
year = "2015",
issn = "0921-3449",
doi = "https://doi.org/10.1016/j.resconrec.2014.12.006",
url = "http://www.sciencedirect.com/science/article/pii/S0921344914002687",
author = "Gerald Kalt",
keywords = "Biomass, Material flows, MFA, Austria, Bioenergy, Sankey diagram",
abstract = "In order to achieve the targets defined in the European Union's “Low Carbon Roadmap”, the “Energy Roadmap 2050” and the “Bioeconomy Strategy”, an enhanced use of biomass is required; not only for energy but also for material uses. In this context and to facilitate targeted resource and energy policy measures, profound knowledge of the status quo of biomass utilization is of crucial importance. The core objective of this paper is to provide complete flow diagrams of the biomass streams within the Austrian economic system from a meso-scale perspective, taking into account all types of uses. Contrary to material flow accounts (MFA), internal streams (e.g. due to biomass processing and transformation, recycling and reuse of residues and by-products, stock changes of end-consumer products) are explicitly taken into consideration and quantified. This approach reveals gaps and inconsistencies in statistical data and facilitates conclusions about quantities not recorded in statistics. Furthermore, the structure of biomass use is visualized and the extent of biogenic material reuse and recycling is revealed. The results show that biomass imports to Austria surpassed exports by about 15% in 2011 (based on dry mass). The distribution of biomass among the different uses depends on whether direct consumption or final uses are considered. In the latter case, which is considered more appropriate, inland biomass consumption was distributed as follows: 7% human food, 18% raw material, 38% energy and 37% animal feed. Exports are primarily composed of wood products. Contrary to common assumption, energy recovery is still usually the ultimate step of cascadic biomass use rather than primary purpose, or based on by-products. Judging from wood quantities being processed and consumed and foreign trade data, domestic wood supply according to felling reports (and stated as “domestic extraction used” in official MFA data) is clearly underrated. Conversely, domestic feed production according to MFA data is inconsistent with official animal feed statistics and appears to be overestimated by at least 30%."
}
@article{ROSSBACH20191,
title = "10 years of pioneering X-ray science at the Free-Electron Laser FLASH at DESY",
journal = "Physics Reports",
volume = "808",
pages = "1 - 74",
year = "2019",
note = "10 years of pioneering X-ray science at the Free-Electron Laser FLASH at DESY",
issn = "0370-1573",
doi = "https://doi.org/10.1016/j.physrep.2019.02.002",
url = "http://www.sciencedirect.com/science/article/pii/S0370157319300663",
author = "Jörg Rossbach and Jochen R. Schneider and Wilfried Wurth",
abstract = "Free-electron lasers produce extremely brief, coherent, and bright laser-like photon pulses that allow to image matter at atomic resolution and at timescales faster than the characteristic atomic motions. In pulses of about 50 femtoseconds duration they provide as many photons as one gets in 1 s from modern storage ring synchrotron radiation facilities. FLASH, the Free-Electron Laser at DESY in Hamburg was the first FEL in the XUV/soft X-ray spectral range, started operation as a user facility in summer 2005, and was for almost 5 years the only short wavelength FEL facility worldwide. Hence, most of the technological developments as well as the scientific experiments performed by the user community were new and unique as outlined below. FLASH was driving FEL science and technology and paved the way for many new ideas. Because of using a linear accelerator in superconducting RF technology FLASH combines the extreme peak brightness characteristic for FELs with very high average brightness. It also was the prototype for the European XFEL located in the Hamburg metropolitan area, which started user operation in summer 2017. The present review provides an overview of the progress made with accelerator science and technology at FLASH for the production of stable beams of well characterized electron pulses, reduction of the pulse jitter to the femtosecond level, generation of ultra-short photon pulses, adequate synchronization of the machine parameters with the experiment, and demonstrating advanced FEL schemes using variable gap undulators. Much of this was done in the very exciting early days of FEL science when it was even not clear if the FEL concept could be realized for X-rays. The development and the operation of the FLASH user facility is described, as well as the techniques developed to make use of the new type of X-ray beams including photon beam diagnostics and damage studies of the optical elements. The review emphasizes breakthrough experiments which demonstrated that many of the ideas collected in the world-wide discussion of the scientific case of free-electron lasers could indeed be realized and they often produced unexpected results. The first experiment on Coulomb explosion of Xe clusters performed in 2002 was a clear demonstration of the feasibility of experiments with free-electron laser beams and opened a lively discussion in the atom, molecular and optical physics community (AMO). Time resolved single-shot single-particle imaging, summarized in the slogan “Take movies instead of pictures”, was one of the most popular science drivers for the construction of free-electron X-ray lasers. As a first step in this direction experiments using a highly focused beam of FLASH demonstrated that pictures of 2 dimensional objects could be reconstructed from single-shot single-particle diffraction patterns. Explosion dynamics of nano-size particles hit by an intense FEL pulse were studied. This method, called “diffraction before destruction”, is now very successfully applied with hard X-rays and, to a large extent, solves the radiation damage problem in structural biology. A long term goal is to determine the 3 dimensional structure of a large molecule from a single-shot diffraction pattern. Along these lines the 3D architecture of free Ag nanoparticles could be determined from one diffraction pattern only using soft X-rays from FLASH. To understand light–matter interactions in this new parameter space a number of pioneering AMO experiments have been performed including non-linear interactions in atoms, molecules and clusters. Multiphoton photoionization processes in the presence of intense optical fields have been studied, as well as photo-absorption of XUV photon energies on molecular ions important for astrophysics. The nature of formation and breaking of molecular bonds was investigated in VUV pump–VUV probe experiments using a reaction microscope and a specific delay line. As an example the process of ultrafast isomerization of acetylene molecules C2H2 triggered by single photon excitation has been studied. The structural changes during the isomerization process were visualized and an isomerization time of 52 +/- 15 fs was found. Clusters of variable size, which can be produced routinely, allow distinguish between inter- and intra-atomic effects and are considered model systems for the investigation of light–matter interactions in multi-atom objects. As an example such experimental studies provided instructive data for benchmarking theoretical models describing cluster ionization in intense short-wavelength laser pulses. The combination of single-shot single-particle imaging for determination of the cluster size with spectroscopy was crucial for success of these experiments. The investigations could later be extended to very large Xe clusters providing new insights into the nanoplasma formation and explosion dynamics of such large systems From early on, studies of high energy density plasmas and warm dense matter have been one of the most prominent research fields in building the scientific case for X-ray free-electron lasers. A good understanding of this complex regime between cold solids and hot dilute plasmas is important for high pressure studies, applied materials studies, inertial fusion, and planetary interiors. With the first observation of saturable absorption of an L-shell transition in Aluminum and pioneering studies of warm dense hydrogen FLASH kicked off research of matter in extreme conditions with free-electron lasers. In condensed matter experiments the emphasis is not so much on the peak power of the FEL beam and extreme focusing, but on beam properties like polarization and pulse duration. The sample has to stay intact in the beam over hours and the number of photons per pulse impinging on the sample has to be limited to avoid space charge effects. After demonstrating the possibility to record single-shot resonant magnetic scattering images with FELs the first time-resolved demagnetization study using a pump–probe approach with an IR-pump pulse and an XUV probe pulse to record a resonant magnetic scattering pattern as a function of pump–probe delay was also performed at FLASH. Free-electron lasers offer the possibility to extend the well-established X-ray spectroscopic techniques for the investigation of the static electronic structure of matter to probing the evolution of the electronic structure in the time domain after controlled excitation. At FLASH first time resolved core level photoemission (TR-XPS) experiments have been performed which are element specific and provide information on the dynamics of the local charge state around a specific center. Using 198 eV photons in a surface study at Ir single crystals it was possible to separate surface and bulk contributions in the Ir 4f levels with sufficient instrumental resolution. Time and angular resolved photoelectron spectroscopy (TR-ARPES) is a very powerful tool to study non-equilibrium electron dynamics of condensed matter systems, since it offers the possibility to follow the dynamics of the full band structure of a material. In another pioneering experiment the photo-induced dynamics of the Mott insulator 1T-TaS2 was studied at FLASH by investigating the dynamics of the Ta 4f photoemission. The formation of a commensurate charge density wave (CCDW) leads to a splitting of the Ta 4f level which decreases first on a sub-picosecond time scale due to electronic melting of the CCDW and afterwards on a picosecond lifetime due to electron–phonon coupling. This leads to transfer of energy from the electronic system to the lattice and a partial melting of the periodic lattice distortions accompanying the periodic charge arrangement in the CCDW phase. In materials science X-ray absorption and emission spectroscopy are among the most powerful spectroscopies to study the electronic structure of matter. The wavelength of the radiation is scanned over certain element specific resonances which at FLASH 1 can only be done by scanning the electron energy. This is time consuming and makes the experiments difficult. Nevertheless, the first time-resolved X-ray emission spectroscopy (XES) experiment was done at FLASH 1 in order to study non-thermal melting of a silicon sample. From a comparison of the observed valence electronic structure at different times after the photoexcitation it became clear that in the melting process in the first few ps a non-equilibrium low density liquid state is reached. The existence of such a metastable low density liquid state had been postulated for many systems that show tetragonal bonding in the crystalline phase like water for example, but spectroscopically the time-resolved silicon XES data taken at FLASH verified its existence for the first time. FLASH 2 has tunable undulators and it was demonstrated that scanning of the wavelength is very easy there."
}
@article{JANSSEN2020154167,
title = "Twelve weeks of exenatide treatment increases [18F]fluorodeoxyglucose uptake by brown adipose tissue without affecting oxidative resting energy expenditure in nondiabetic males",
journal = "Metabolism",
volume = "106",
pages = "154167",
year = "2020",
issn = "0026-0495",
doi = "https://doi.org/10.1016/j.metabol.2020.154167",
url = "http://www.sciencedirect.com/science/article/pii/S0026049520300317",
author = "Laura G.M. Janssen and Kimberly J. Nahon and Katrien F.M. Bracké and Dennis {van den Broek} and Renée Smit and Aashley S.D. {Sardjoe Mishre} and Lisa L. Koorneef and Borja Martinez-Tellez and Jedrzej Burakiewicz and Hermien E. Kan and Floris H.P. {van Velden} and Lenka M. {Pereira Arias-Bouda} and Lioe-Fee {de Geus-Oei} and Jimmy F.P. Berbée and Ingrid M. Jazet and Mariëtte R. Boon and Patrick C.N. Rensen",
keywords = "Brown adipose tissue, [F]FDG-PET/CT, Glucagon-like peptide-1 receptor agonism, Lipid metabolism, MRI, Weight loss",
abstract = "Aims/hypothesis
Brown adipose tissue (BAT) improves energy metabolism by combusting glucose and lipids into heat. Agonism of the glucagon-like peptide-1 receptor (GLP-1R) within the central nervous system activates BAT in mice. Moreover, in patients with type 2 diabetes, GLP-1R agonism lowers body weight and improves glucose and lipid levels, possibly involving BAT activation. Interestingly, people from South Asian descent are prone to develop cardiometabolic disease. We studied the effect of GLP-1R agonism on BAT in humans, specifically in South Asians and Europids without obesity or type 2 diabetes.
Methods
Twelve Dutch South Asian and 12 age- and BMI-matched Europid nondiabetic men received 12 weeks extended-release exenatide (Bydureon) in this single-arm prospective study. Before and after treatment, BAT was visualized by a cold-induced [18F]FDG-PET/CT scan and a thermoneutral MRI scan, and resting energy expenditure (REE), substrate oxidation, body composition and fasting plasma glucose and serum lipids were determined. Appetite was rated using a visual analogue scale.
Results
Since the effect of exenatide on metabolic parameters did not evidently differ between ethnicities, data of all participants were pooled. Exenatide decreased body weight (−1.5 ± 0.4 kg, p < 0.01), without affecting REE or substrate oxidation, and transiently decreased appetite ratings during the first weeks. Exenatide also lowered triglycerides (−15%, p < 0.05) and total cholesterol (−5%, p < 0.05), and tended to lower glucose levels. Notably, exenatide increased BAT metabolic volume (+28%, p < 0.05) and mean standardized uptake value (+11%, p < 0.05) ([18F]FDG-PET/CT), without affecting supraclavicular adipose tissue fat fraction (MRI).
Conclusions/interpretation
We show for the first time that GLP-1R agonism increases [18F]FDG uptake by BAT in South Asian and Europid men without obesity or type 2 diabetes.
Trial registry
Clinicaltrials.gov NCT03002675."
}
@article{ALMANSA2016102,
title = "PENGEOM—A general-purpose geometry package for Monte Carlo simulation of radiation transport in material systems defined by quadric surfaces",
journal = "Computer Physics Communications",
volume = "199",
pages = "102 - 113",
year = "2016",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2015.09.019",
url = "http://www.sciencedirect.com/science/article/pii/S0010465515003707",
author = "Julio Almansa and Francesc Salvat-Pujol and Gloria Díaz-Londoño and Artur Carnicer and Antonio M. Lallena and Francesc Salvat",
keywords = "Constructive quadric geometry, Monte Carlo particle transport, Ray tracing, Geometry visualization",
abstract = "The Fortran subroutine package pengeom provides a complete set of tools to handle quadric geometries in Monte Carlo simulations of radiation transport. The material structure where radiation propagates is assumed to consist of homogeneous bodies limited by quadric surfaces. The pengeom subroutines (a subset of the penelope code) track particles through the material structure, independently of the details of the physics models adopted to describe the interactions. Although these subroutines are designed for detailed simulations of photon and electron transport, where all individual interactions are simulated sequentially, they can also be used in mixed (class II) schemes for simulating the transport of high-energy charged particles, where the effect of soft interactions is described by the random-hinge method. The definition of the geometry and the details of the tracking algorithm are tailored to optimize simulation speed. The use of fuzzy quadric surfaces minimizes the impact of round-off errors. The provided software includes a Java graphical user interface for editing and debugging the geometry definition file and for visualizing the material structure. Images of the structure are generated by using the tracking subroutines and, hence, they describe the geometry actually passed to the simulation code.
Program summary
Program title: Pengeom Catalogue identifier: AEYH_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEYH_v1_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 89390 No. of bytes in distributed program, including test data, etc.: 5062646 Distribution format: tar.gz Programming language: Fortran, Java. Computer: PC with Java Runtime Environment installed. Operating system: Windows, Linux. RAM: 210 MiB Classification: 21.1, 14. Nature of problem: The Fortran subroutines perform all geometry operations in Monte Carlo simulations of radiation transport with arbitrary interaction models. They track particles through material systems consisting of homogeneous bodies limited by quadric surfaces. Particles are moved in steps (free flights) of a given length, which is dictated by the simulation program, and are halted when they cross an interface between media of different compositions or when they enter selected bodies. Solution method: The pengeom subroutines are tailored to optimize simulation speed and accuracy. Fast tracking is accomplished by the use of quadric surfaces, which facilitate the calculation of ray intersections, and of modules (connected volumes limited by quadric surfaces) organized in a hierarchical structure. Optimal accuracy is obtained by considering fuzzy surfaces, with the aid of a simple algorithm that keeps control of multiple intersections of a ray and a surface. The Java GUI PenGeomJar provides a geometry toolbox; it allows building and debugging the geometry definition file, as well as visualizing the resulting geometry in two and three dimensions. Restrictions: By default pengeom can handle systems with up to 5000 bodies and 10,000 surfaces. These numbers can be increased by editing the Fortran source file. Unusual features: All geometrical operations are performed internally. The connection between the steering main program and the tracking routines is through a Fortran module, which contains the state variables of the transported particle, and the input-output arguments of the subroutine step. Rendering of two- and three-dimensional images is performed by using the pengeom subroutines, so that displayed images correspond to the definitions passed to the simulation program. Additional comments: Java editor and viewer (PenGeomJar), geometry examples, translator to POV-Ray™ format, detailed manual. The Fortran subroutine package pengeom is part of the penelope code system [1]. Running time: The running time much depends on the complexity of the material system. The most complicated example provided, phantom, an anthropomorphic phantom, has 264 surfaces and 169 bodies and modules, with different levels of grouping; the largest module contains 51 daughters. The rendering of a 3D image of phantom with 1680x1050 pixels takes about 25 s (i.e., about 1.5⋅10−5 seconds per ray) on an Intel Core I7-3520M CPU, with Windows 7 and subroutines compiled with gfortran. References:[1]F. Salvat, PENELOPE-2014: A Code System for Monte Carlo Simulation of Electron and Photon Transport, OECD/NEA Data Bank, Issy-les-Moulineaux, France, 2015. Available from http://www.nea.fr/lists/penelope.html."
}
@article{PERALTA2019105925,
title = "Automatic restoration of large-scale distribution networks with distributed generators, voltage control devices and heating loads",
journal = "Electric Power Systems Research",
volume = "176",
pages = "105925",
year = "2019",
issn = "0378-7796",
doi = "https://doi.org/10.1016/j.epsr.2019.105925",
url = "http://www.sciencedirect.com/science/article/pii/S0378779619302445",
author = "Renzo Amilcar Vargas Peralta and Jonatas Boás Leite and José Roberto Sanches Mantovani",
keywords = "Network restoration, Power distribution, Cold load pickup, Distributed generation, Voltage control, Metaheuristic",
abstract = "Energy supply interruption by permanent faults causes economic and social problems, dissatisfaction of customers and penalization to electrical power distribution companies. In the procedure to restore distribution networks, power interruption damages must be minimized by energizing the largest possible number of customer loads in the shortest time interval. The restoration problem of distribution networks can be mathematically formulated as a mixed integer nonlinear programming problem that is non-convex of type NP-complete. The existence of heating loads causes the cold load pickup condition in the distribution network that requires the step-by-step procedure increasing the complexity of the restoration problem. In this work, therefore, it is proposed a methodology to solve the restoration problem of large-scale distribution networks with heating loads where the mathematical model is subjected to intentional islanding of distributed generation and voltage control through optimized adjustments on capacitor banks and voltage regulators. The outcomes are achieved under a testing distribution system with 53 nodes and a real system with 7052 nodes."
}
@article{AMIN2017415,
title = "Fluorescence-based CdTe nanosensor for sensitive detection of cytochrome C",
journal = "Biosensors and Bioelectronics",
volume = "98",
pages = "415 - 420",
year = "2017",
issn = "0956-5663",
doi = "https://doi.org/10.1016/j.bios.2017.07.020",
url = "http://www.sciencedirect.com/science/article/pii/S0956566317304682",
author = "Rehab M. Amin and Souad A. Elfeky and Thomas Verwanger and Barbara Krammer",
keywords = "Quantum dots, Cytochrome c, Nanosensor, Dermal fibroblasts, Cadmium telluride",
abstract = "Cytochrome c (Cyt c) is commonly used as intrinsic biomarker for several characteristics of the cell such as respiration, energy level and apoptosis. In the present study a simple colorimetric sensor should be developed and tested for the real-time detection of Cyt c in living cells. We synthesized cadmium telluride quantum dots (CdTe QDs) capped with thioglycolic acid (TGA) as a fluorometric Cyt c nanosensor. The synthesized TGA/CdTe QDs nanosensor was characterized by Fourier transform infrared spectroscopy, transmission electron microscopy, and absorption as well as fluorescence spectrophotometry. We investigated the developed TGA/CdTe QDs sensor with regard to its applicability in the fluorometric detection of Cyt c. Results showed that the TGA/CdTe QDs could be used as a sensitive fluorescence probe for the quantification of different concentrations of Cyt c ranging from 0.5 − 2.5μM. Increased binding of QDs to Cyt c results in decreasing fluorescence. The fluorescence of the QDs is inversely correlated to the Cyt c concentration. Based on these data, a standard curve up to 2.5μM Cyt c was established. Moreover, the developed nanosensor was applied in different concentrations on primary human dermal fibroblasts. Results showed that TGA/CdTe QDs were taken up by cells and could be visualized by fluorescence microscopy. Quantification of Cyt c within living cells via QDs is, however, influenced by various factors such as cell damage, QD aggregation or the level of reactive oxygen species, which have to be taken into account."
}
@article{DONG2011427,
title = "FRET analysis of protein tyrosine kinase c-Src activation mediated via aryl hydrocarbon receptor",
journal = "Biochimica et Biophysica Acta (BBA) - General Subjects",
volume = "1810",
number = "4",
pages = "427 - 431",
year = "2011",
issn = "0304-4165",
doi = "https://doi.org/10.1016/j.bbagen.2010.11.007",
url = "http://www.sciencedirect.com/science/article/pii/S0304416510002576",
author = "Bin Dong and Wei Cheng and Wen Li and Jie Zheng and Dalei Wu and Fumio Matsumura and Christoph Franz Adam Vogel",
keywords = "AhR, COX-2, c-Src, EGF, FRET, TCDD",
abstract = "Background
Activation of the protein tyrosine kinase c-Src (c-Src kinase) induced by the exposure to the environmental pollutant 2,3,7,8-tetrachlorodibenzo-p-dioxin (TCDD) has been shown in various cell types. Most previous works used Western blot analysis to detect the phosphorylation on the Tyr416 residue, which activates c-Src kinase.
Methods
Here we compared the results of c-Src tyrosine phosphorylation via aryl hydrocarbon receptor (AhR)-dependent mechanisms from Western blot analysis with fluorescent resonance energy transfer (FRET) assay detecting c-Src activation after treatment with TCDD to activate AhR in two different human cell types.
Results
Western blot analyses show time-dependent phosphorylation of c-Src by TCDD in HepG2 and MCF-10A cells. Data from FRET assay visualized and quantified the activation of c-Src kinase induced by TCDD in living cells of both cell types. The FRET efficiency decreased by 20%, 5min after TCDD treatment and continued decreasing until the end of the experiment, 25min after TCDD treatment. PP2, a c-Src specific inhibitor, suppressed both TCDD- and epidermal growth factor- (EGF) induced c-Src activation. In contrast, the AhR antagonist 3′-methoxy-4′nitroflavone (MNF) blocked only TCDD- but not EGF-induced activation of c-Src.
Conclusions
The current study shows that the early activation of c-Src via EGF and AhR signaling pathways can be visualized in living cells using the FRET assay which is in line with Western blot analysis.
General Significance
The FRET assay provides a useful tool to visualize and quantify c-Src kinase activation via AhR in living cells."
}
@article{SHIN2010488,
title = "Voxel-based analysis of Alzheimer's disease PET imaging using a triplet of radiotracers: PIB, FDDNP, and FDG",
journal = "NeuroImage",
volume = "52",
number = "2",
pages = "488 - 496",
year = "2010",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2010.04.013",
url = "http://www.sciencedirect.com/science/article/pii/S1053811910004076",
author = "Jonghan Shin and Sang-Yoon Lee and Seog Ju Kim and So-Hee Kim and Seong-Jin Cho and Young-Bo Kim",
keywords = "Alzheimer, Amyloid, Tau, Metabolism, Neuropathology, Positron emission tomography",
abstract = "Beta amyloid plaques, neurofibrillary tangles, and impaired glucose metabolism are among the most prevalent pathological characteristics of Alzheimer's disease (AD). However, separate visualization of these three AD-related pathologies in living humans has not been conducted. Here, we show that positron emission tomography (PET) imaging using the three radiotracers (11)C-Pittsburgh compound B (PIB), 2-(1-{6-[(2-(18)F-fluoroethyl)(methyl)amino]-2-naphthyl}ethylidene) malononitrile (FDDNP), and 2-[18F]fluoro-2-deoxy-d-glucose (FDG), in the same subjects, with and without AD, can provide valuable information on the pathological patterns of the distribution of tracers for amyloid plaque, neurofibrillary tangle, and glucose hypometabolism in AD. Voxel-based analysis of PIB-PET in patients with AD compared with normal control subjects showed that patients with AD have highly significant PIB retention in brain regions known to have high amyloid plaque deposition (e.g., frontal, parietal, temporal, and posterior cingulate/precuneus cortices). In contrast, voxel-based analysis of FDDNP-PET showed significantly high FDDNP binding in some brain regions known to have high tangle accumulation in patients with AD compared with age-matched normal subjects (e.g., entorhinal cortex, inferior temporal gyrus, and secondary visual cortex). In addition, because FDDNP binds both plaques and tangles but PIB binds plaques specifically, we examined subtracted PET data (FDDNP minus PIB) acquired from the same patients with AD using an SPM analysis. We found that the hippocampal formation was the most significant brain region in the voxel mapping of FDDNP minus PIB in the same patients with AD. Voxel-based analysis of FDG-PET in the same subjects revealed that brain regions with glucose hypometabolism in patients with AD overlap with regions of high PIB binding. In conclusion, PET imaging using these three radiotracers in the same subjects may contribute toward developing and testing disease-modifying drugs targeting amyloid pathology, tau pathology, and/or energy metabolism."
}
@article{MAGNO201638,
title = "InfiniTime: Multi-sensor wearable bracelet with human body harvesting",
journal = "Sustainable Computing: Informatics and Systems",
volume = "11",
pages = "38 - 49",
year = "2016",
note = "SI: IGCC 2014",
issn = "2210-5379",
doi = "https://doi.org/10.1016/j.suscom.2016.05.003",
url = "http://www.sciencedirect.com/science/article/pii/S2210537916300816",
author = "Michele Magno and Davide Brunelli and Lukas Sigrist and Renzo Andri and Lukas Cavigelli and Andres Gomez and Luca Benini",
keywords = "Wearable devices, Autonomous systems, Zero-power, Indoor energy harvesting, Human harvesting, Power management",
abstract = "Wearable technology is gaining popularity, with people wearing everything “smart” from clothing to glasses and watches. Present-day wearables are typically battery-powered, and their limited lifetime has become the critical issue. Most devices need recharging every few days or even hours, falling short of the expectations for a truly satisfactory user experience. This paper presents the design, implementation and in-field evaluation of InfiniTime, a novel sensor-rich smart bracelet powered by energy harvesting. It is designed to achieve self-sustainability using solar cells with only modest indoor light levels and thermoelectric generators (TEG’s) with small temperature gradients from the body heat. The wearable device is equipped with an ultra-low power camera and a microphone, in addition to accelerometer and temperature sensors commonly used in commercial devices. Experimental characterization of the fully operational prototype demonstrates a wide range of energy optimization techniques used to achieve self-sustainability with harvested energy only. Our experiments in real-world scenarios show an average of up to 550μW for photovoltaic in indoor and 98μW for TEG with only 3° temperature gradient and up to 250μW for 5° gradient. Simulations using energy intake measurements from solar and TEG modules confirm that InfiniTime achieves self-sustainability with indoor lighting levels and body heat for several realistic applications featuring data acquisition from the on-board camera and multiple sensors, as well as visualization and wireless connectivity. The highly optimized low-power architecture of the presented prototype features image acquisitions at 1.15 frames per second, powered only from the energy harvesters."
}
@article{GRAVINA2017158,
title = "Cloud-based Activity-aaService cyber–physical framework for human activity monitoring in mobility",
journal = "Future Generation Computer Systems",
volume = "75",
pages = "158 - 171",
year = "2017",
issn = "0167-739X",
doi = "https://doi.org/10.1016/j.future.2016.09.006",
url = "http://www.sciencedirect.com/science/article/pii/S0167739X16303016",
author = "Raffaele Gravina and Congcong Ma and Pasquale Pace and Gianluca Aloi and Wilma Russo and Wenfeng Li and Giancarlo Fortino",
keywords = "Cloud computing, Activity monitoring, Wearable sensors, Programming framework, Software as a service",
abstract = "This paper proposes Activity as a Service (Activity-aaService), a full-fledged cyber–physical framework to support community, on-line and off-line human activity recognition and monitoring in mobility. Activity-aaService is able to address the current lack of Cloud-Assisted Body Area Networks platforms and applications supporting monitoring and analysis of human activity for single individuals and communities. Activity-aaService is built atop the BodyCloud platform so enabling efficient BSN-based sensor data collection and local processing (Body-side), high performance computing of collected sensor data and data storing on the Cloud (Cloud-side), workflow-based programming of data analysis (Analyst-side), and advanced visualization of results (Viewer-side). Specifically, it provides specific, powerful and flexible programming abstractions for the rapid prototyping of efficient human activity-oriented applications. The effectiveness of the proposed framework has been demonstrated through the development of several prototypes related to physical activity monitoring, step counting, physical energy estimation, automatic fall detection, and smart wheelchair support. Finally, performance evaluation of the proposed framework at the Body-side of the activity classification has been carried out by analyzing processing load, data transmission time, CPU usage, memory footprint, and battery consumption using four heterogeneous mobile devices representing low, medium and high performance mobile platforms."
}
@article{ZHANG2015173,
title = "Parallel AFMPB solver with automatic surface meshing for calculation of molecular solvation free energy",
journal = "Computer Physics Communications",
volume = "190",
pages = "173 - 181",
year = "2015",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2014.12.022",
url = "http://www.sciencedirect.com/science/article/pii/S0010465515000089",
author = "Bo Zhang and Bo Peng and Jingfang Huang and Nikos P. Pitsianis and Xiaobai Sun and Benzhuo Lu",
keywords = "Poisson–Boltzmann equation, Boundary integral equation, Automatic surface meshing, Solvation free energy, Fast multipole methods, Parallelization, Cilk Plus",
abstract = "We present PAFMPB, an updated and parallel version of the AFMPB software package for fast calculation of molecular solvation-free energy. The new version has the following new features: (1) The adaptive fast multipole method and the boundary element methods are parallelized; (2) A tool is embedded for automatic molecular VDW/SAS surface mesh generation, leaving the requirement for a mesh file at input optional; (3) The package provides fast calculation of the total solvation-free energy, including the PB electrostatic and nonpolar interaction contributions. PAFMPB is implemented in C and Fortran programming languages, with the Cilk Plus extension to harness the computing power of both multicore and vector processing. Computational experiments demonstrate the successful application of PAFMPB to the calculation of the PB potential on a dengue virus system with more than one million atoms and a mesh with approximately 20 million triangles.
Program summary
Program title: Parallel AFMPB Catalogue identifier: AEGB_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEGB_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: GNU General Public License, version 2 No. of lines in distributed program, including test data, etc.: 40558 No. of bytes in distributed program, including test data, etc.: 2349976 Distribution format: tar.gz Programming language: Mixed C and Fortran, Compiler: Intel or GNU with Cilk Plus enabled. Computer: Any, but the code is mainly designed for multicore architectures. Operating system: Linux. RAM: Depends on the size of the discretized biomolecular system. Classification: 3. Catalogue identifier of previous version: AEGB_v1_1 Journal reference of previous version: Comput. Phys. Comm. 184 (2013) 2618 External routines: Users are allowed to use external routines/libraries (e.g., MSMS [6] and TMSMesh [4]) to generate compatible surface mesh input data if they choose not to use the embedded automatic mesh generation tool in the package. Post-processing tools such as VCMM [5] and VMD [3] can also be used for visualization and analyzing results. The package uses two subprograms: (1) The iterative Krylov subspace solver, SPARSKIT, from Yousef Saad [2]; and (2) Cilk-based parallel fast multipole methods from FMMSuite [1]. Does the new version supersede the previous version? Yes Nature of problem: Numerical solution of the linearized Poisson–Boltzmann equation that describes electrostatic interactions of molecular systems in ionic solutions. Solution method: The linearized Poisson–Boltzmann equation is reformulated as a boundary integral equation and is subsequently discretized using the node-patch scheme. The resulting linear system is solved using Krylov subspace solvers iteratively. The reformulation of the equation provides an upper bound for the number of iterations. Within each iteration, the matrix–vector multiplication is accelerated using the adaptive plane-wave expansion based fast multipole methods. The majority of the codes are parallelized using the Cilk runtime. Reasons for new version: New functions are added and a few old functions like force calculations are removed. The algorithm is parallelized and most parts of the code are rewritten. Summary of revisions: The computation is parallelized and an automatic mesh generation method for BEM is added. Restrictions: The program has only been tested on machines running Linux operating system. Additional comments: The Cilk runtime used in the development and testing is from the Intel compiler Suite. The GNU Cilk Plus and Cilk Plus/LLVM branches have not been tested. Running time: The running time depends on the number of discretized elements (N) and their distribution. It also depends on the number of cores used in the computation. References: [1] http://www.fastmultipole.org/. [2] http://www-users.cs.umn.edu/~saad/software/. [3] http://www.ks.uiuc.edu/Research/vmd/. [4] http://www.continuummodel.org. [5] S. Bai, B. Lu, VCMM: A visual tool for continuum molecular modeling. J. Mol. Graph. Model. 50 (2014) 44–49. [6] Scanner, F. Michel, Olson, J. Arthur, Spehner, J. Claude, Reduced surface: An efficient way to compute molecular surfaces. Biopolymers 38 (1996) 305–320."
}
@article{WEN201812,
title = "Enhanced coronary calcium visualization and detection from dual energy chest x-rays with sliding organ registration",
journal = "Computerized Medical Imaging and Graphics",
volume = "64",
pages = "12 - 21",
year = "2018",
issn = "0895-6111",
doi = "https://doi.org/10.1016/j.compmedimag.2018.01.004",
url = "http://www.sciencedirect.com/science/article/pii/S0895611118300065",
author = "Di Wen and Katelyn Nye and Bo Zhou and Robert C. Gilkeson and Amit Gupta and Shiraz Ranim and Spencer Couturier and David L. Wilson",
keywords = "Coronary artery calcification, Dual energy chest radiography, Image processing, Sliding organ registration, ROC",
abstract = "We have developed a technique to image coronary calcium, an excellent biomarker for atherosclerotic disease, using low cost, low radiation dual energy (DE) chest radiography, with potential for widespread screening from an already ordered exam. Our dual energy coronary calcium (DECC) processing method included automatic heart silhouette segmentation, sliding organ registration and scatter removal to create a bone-image-like, coronary calcium image with significant reduction in motion artifacts and improved calcium conspicuity compared to standard, clinically available DE processing. Experiments with a physical dynamic cardiac phantom showed that DECC processing reduced 73% of misregistration error caused by cardiac motion over a wide range of heart rates and x-ray radiation exposures. Using the functional measurement test (FMT), we determined significant image quality improvement in clinical images with DECC processing (p < 0.0001), where DECC images were chosen best in 94% of human readings. Comparing DECC images to registered and projected CT calcium images, we found good correspondence between the size and location of calcification signals. In a very preliminary coronary calcium ROC study, we used CT Agatston calcium score >50 as the gold standard for an actual positive test result. AUC performance was significantly improved from 0.73 ± 0.14 with standard DE to 0.87 ± 0.10 with DECC (p = 0.0095) for this limited set of surgical patient data biased towards heavy calcifications. The proposed DECC processing shows good potential for coronary calcium detection in DE chest radiography, giving impetus for a larger clinical evaluation."
}
@article{LEE201416,
title = "Cost versus reliability sizing strategy for isolated photovoltaic micro-grids in the developing world",
journal = "Renewable Energy",
volume = "69",
pages = "16 - 24",
year = "2014",
issn = "0960-1481",
doi = "https://doi.org/10.1016/j.renene.2014.03.019",
url = "http://www.sciencedirect.com/science/article/pii/S0960148114001633",
author = "Mitchell Lee and Daniel Soto and Vijay Modi",
keywords = "Isolated micro-grids, Cost versus reliability, Micro-grid system sizing",
abstract = "For many isolated regions in the developing world micro-grids which combine photovoltaic electricity generation and battery storage may represent the most reliable and least expensive form of energy service. Due to climate induced solar resource variations, achieving high reliability levels necessitates excess generation and storage capacity which can significantly increase the end consumer cost of energy. Due to severe financial limitations, many consumers in the developing world may prefer cost versus reliability trade-offs, as long as their basic energy needs are met. Defining reliability as the percent of electricity demand a grid can deliver, we utilize a time series energy balance algorithm at hourly resolution to create cost versus reliability curves of micro-grid performance. We then propose a micro-grid sizing strategy which enables designers with knowledge of local energy needs to determine the acceptability of potential micro-grids. Our strategy relies on visualizing simulation data at increasing levels of temporal resolution to determine where energy shortfalls occur and if they interfere with high priority energy demand. A case study is presented which utilizes the proposed methods. Results suggest that the methodology has the potential to reduce the cost of service while maintaining acceptable consumer reliability."
}
@article{SCHLUTZ201215,
title = "Integrating advanced mobility into lunar surface exploration",
journal = "Acta Astronautica",
volume = "75",
pages = "15 - 24",
year = "2012",
issn = "0094-5765",
doi = "https://doi.org/10.1016/j.actaastro.2012.01.005",
url = "http://www.sciencedirect.com/science/article/pii/S0094576512000173",
author = "Juergen Schlutz and Ernst Messerschmid",
keywords = "Lunar surface environment, Mobility, Human exploration, Moon, Terrain analysis, Modeling, ISECG",
abstract = "With growing knowledge of the lunar surface environment from recent robotic missions, further assessment of human lunar infrastructures and operational aspects for surface exploration become possible. This is of particular interest for the integration of advanced mobility assets, where path planning, balanced energy provision and consumption as well as communication coverage grow in importance with the excursion distance. The existing modeling and simulation tools for the lunar surface environment have therefore been revisited and extended to incorporate aspects of mobile exploration. An extended analysis of the lunar topographic models from past and ongoing lunar orbital missions has resulted in the creation of a tool to calculate and visualize slope angles in selected lunar regions. This allows for the identification of traversable terrain with respect to the mobile system capabilities. In a next step, it is combined with the analysis of the solar illumination conditions throughout this terrain to inform system energy budgets in terms of electrical power availability and thermal control requirements. The combination of the traversability analysis together with a time distributed energy budget assessment then allows for a path planning and optimization for long range lunar surface mobility assets, including manned excursions as well as un-crewed relocation activities. The above mentioned tools are used for a conceptual analysis of the international lunar reference architecture, developed in the frame of the International Architecture Working Group (IAWG) of the International Space Exploration Coordination Group (ISECG). Its systems capabilities are evaluated together with the planned surface exploration range and paths in order to analyze feasibility of the architecture and to identify potential areas of optimization with respect to time-based and location-based integration of activities."
}
@article{PERDIKAKIS20151112,
title = "Optimize Energy Efficiency in the Supply Chain of FMCGs with the Use of Semantic Web Technologies",
journal = "Procedia Engineering",
volume = "132",
pages = "1112 - 1119",
year = "2015",
note = "MESIC Manufacturing Engineering Society International Conference 2015.",
issn = "1877-7058",
doi = "https://doi.org/10.1016/j.proeng.2015.12.603",
url = "http://www.sciencedirect.com/science/article/pii/S1877705815045154",
author = "A. Perdikakis and A. Shukla and D. Kiritsis",
keywords = "Energy efficiency, Supply chain, FMCGs, Ontology, Semantic Web Technologies, Domain definition",
abstract = "Supply Chain Management is a critical domain for Fast Moving Consumer Goods (FMCGs). This domain is known for its complexity. New standards and regulations regarding Energy Efficiency and Environmental Aspects in general, as well as customer demand, make the analysis, modeling and design of the Supply Chain more and more complicated. Partners involved in these processes are numerous and of diverse background. To help solving this problem, common understanding of the domain and exchange of information among partners involved in the Supply Chain is of high importance. An ontology capturing the knowledge of the domain was created. To achieve maximum efficiency of the domain operations in terms of cost, quality of service and environmental impact, concept definitions from multiple sources were gathered. An advanced software solution that leverages semantic web technologies, enables users to link data from multiple Excel spreadsheets and relational databases together in real-time for data collection, collaboration, and reporting. In this framework, a new way for collaboration throughout the supply chain with the use of an underlying ontology, semantic technologies and visualization technics is introduced. The proposed approach is applied in the context of the FP7 European project e-SAVE."
}
@article{BACHHAV201031,
title = "Effect of controlled laser microporation on drug transport kinetics into and across the skin",
journal = "Journal of Controlled Release",
volume = "146",
number = "1",
pages = "31 - 36",
year = "2010",
issn = "0168-3659",
doi = "https://doi.org/10.1016/j.jconrel.2010.05.025",
url = "http://www.sciencedirect.com/science/article/pii/S0168365910003937",
author = "Y.G. Bachhav and S. Summer and A. Heinrich and T. Bragagna and C. Böhler and Y.N. Kalia",
keywords = "P.L.E.A.S.E.®, Lidocaine, Transdermal delivery, Stratum corneum ablation, Micropore",
abstract = "The objectives of this study were to investigate a novel laser microporation technology ( P.L.E.A.S.E. Painless Laser Epidermal System) and to determine the effect of pore number and depth on the rate and extent of drug delivery across the skin. In addition, the micropores were visualized by confocal laser scanning microscopy and histological studies were used to determine the effect of laser fluence (energy applied per unit area) on pore depth. Porcine ear skin was used as the membrane for both the pore characterization and drug transport studies. Confocal images in the XY-plane revealed that the pores were typically 150–200μm in diameter. Histological sections confirmed that fluence could be used to effectively control pore depth — low energy application (4.53 and 13.59J/cm2) resulted in selective removal of the stratum corneum (20–30μm), intermediate energies (e.g., 22.65J/cm2) produced pores that penetrated the viable epidermis (60–100μm) and higher application energies created pores that reached the dermis (>150–200μm). The effects of pore number and pore depth on molecular transport were quantified by comparing lidocaine delivery kinetics across intact and porated skin samples. After 24h, cumulative skin permeation of lidocaine with 0 (control), 150, 300, 450 and 900 pores was 107±46, 774±110, 1400±344, 1653±437 and 1811±642µg/cm2, respectively; there was no statistically significant difference between 300, 450 and 900 pore data — probably due to the effect of drug depletion since >50% of the applied dose was delivered. Importantly, increasing fluence did not produce a statistically significant increase in lidocaine permeation; after 24h, cumulative lidocaine permeation was 1180±448, 1350±445, 1240±483 and 1653±436µg/cm2 at fluences of 22.65, 45.3, 90.6 and 135.9J/cm2, respectively. Thus, shallow pores were equally effective in delivering lidocaine. Increasing lidocaine concentration in the formulation from 10 to 25mg/ml produced a corresponding increase in permeation (at 24h, 1650±437 and 4005±1389µg/cm2, respectively). The validity of the porcine skin model was confirmed as transport across porcine and human skins was shown to be statistically equivalent (at 24h, 1811±642 and 2663±208µg/cm2, respectively). The clinical potential of the technology and its capacity to provide significantly faster delivery than conventional passive administration was demonstrated in short duration experiments involving application of a marketed lidocaine cream (LMX4®) to laser-porated skin; after only 5min of formulation application, lidocaine deposition was measured at 61.3±7.5µg/cm2. In conclusion, the results demonstrate the ability of P.L.E.A.S.E.® (i) to create well-defined conduits in the skin, (ii) to provide a controlled enhancement of transdermal transport and (iii) to enable improvement in both the rate and extent of drug delivery."
}
@article{CHIANG201413,
title = "Pattern analysis in daily physical activity data for personal health management",
journal = "Pervasive and Mobile Computing",
volume = "13",
pages = "13 - 25",
year = "2014",
issn = "1574-1192",
doi = "https://doi.org/10.1016/j.pmcj.2013.12.003",
url = "http://www.sciencedirect.com/science/article/pii/S1574119213001545",
author = "Jung-Hsien Chiang and Pei-Ching Yang and Hsuan Tu",
keywords = "Pattern recognition, Daily activity habits, Personal health management, Health care",
abstract = "Purpose: sedentary lifestyles have resulted in an increasing number of people who are at increased risk of various conditions and diseases, including overweight, obesity, and metabolic syndromes. Our objective was to systematically record the daily life journal on a platform to increase the self-awareness and improve the sedentary lifestyle and to assist clinicians in understanding and facilitating patients’ daily physical activity. Method: we developed a portable activity pattern recognition system designed to automatically recognize the daily activity habits of users, and provide visualized life logs on the wellness self-management platform for patients and clinicians. Based on the participants’ and the clinician’s comments, appropriate modifications were made. Results: persuading people to improve their activities during non-working hours can enhance the general physical activity. Since users’ smartphones automatically monitor their energy expenditure, healthcare professionals can use these data to assist their patients in addressing health problems stemming from the obesity or metabolic syndromes, thus empowering users to avert or delay the progression of diabetes, cardiovascular disease and other complications. Discussion and conclusions: the clinical pilot study showed the feasibility of applying this persuasive technology to improve the physical activity of overweight people. The limitation of the study is the need for Wi-Fi and 3G environments and a smartphone."
}
@incollection{GAO201857,
title = "Chapter 4 - Frameworks for Big Data Integration, Warehousing, and Analytics",
editor = "Reza Arghandeh and Yuxun Zhou",
booktitle = "Big Data Application in Power Systems",
publisher = "Elsevier",
pages = "57 - 73",
year = "2018",
isbn = "978-0-12-811968-6",
doi = "https://doi.org/10.1016/B978-0-12-811968-6.00004-8",
url = "http://www.sciencedirect.com/science/article/pii/B9780128119686000048",
author = "Feng Gao",
keywords = "MapReduce, Spark, Lambda architecture, BaaS, Microservice architecture, Complex event processing, Complementary operation of multiform energy system, Microgrid, Smart grid",
abstract = "Chapter Overview
Big data is a term for large and complex datasets that traditional processing approaches are not suitable to deal with them. Big data usually comes from the Internet, enterprise systems, Internet of Things, and other information systems. Data collection and preparation, storage management, data processing, data analysis, and knowledge presentation would generate new insights to support decision-making and business intelligent operation. Smart grid is a developing trend of electrical power energy industry. The core message is to implement the next generation of cyber-physical systems shaping future energy industry that is based on a deep merge of operational technology and internet information technology. The growth of smart grid is dependent on the availability of high performance computing (HPC) and analytics technology to process a massive amount of data set. Deployment of advanced technologies within smart grid and usage of state-of-the-art computing systems provide utility companies with innovative capabilities. These advances lead to unprecedented explosion of data volumes. As smart grid operations will leverage advanced metering infrastructure to drive more real time decision-making and operational activities, complex event processing and stream computing are needed for the modern smart grid. The chapter discusses one core technique that would support the growth of smart grid, big data with HPC, with a focus on the platform, data integration, warehousing, and analytics that are particularly adaptive to handle a variety of characteristics of energy industry data. Finally, the chapter summarizes and proposes a comprehensive, technical solution for smart grid platform with applications focusing on complementary operation of multiform energy system that supports all aspects within a data lifetime cycle, e.g., acquisition, storage, analytics, and visualization."
}
@article{PIRES201810,
title = "A dual energy micro-CT methodology for visualization and quantification of biofilm formation and dentin demineralization",
journal = "Archives of Oral Biology",
volume = "85",
pages = "10 - 15",
year = "2018",
issn = "0003-9969",
doi = "https://doi.org/10.1016/j.archoralbio.2017.09.034",
url = "http://www.sciencedirect.com/science/article/pii/S0003996917303163",
author = "Paula Maciel Pires and Thais Pires dos Santos and Andrea Fonseca-Gonçalves and Matheus Melo Pithon and Ricardo Tadeu Lopes and Aline de Almeida Neves",
keywords = "Dental biofilm, Microcosm model, Micro-CT, Dual-energy",
abstract = "Objective
The aim of this study was to induce artificial caries in human sound dentin by means of a microcosm model using human saliva as source of bacteria and to apply a novel dual-energy micro-CT technique to quantify biofilm formation and evaluate its demineralization potential.
Design
Eight sound third molars had the occlusal enamel removed by cutting with a diamond disk and five cylindrical cavities (±2mm diameter; ±1.5mm depth) were prepared over the dentin surface in each specimen (n=40 cavities). After sterilization, each specimen received the bacterial salivary inoculum obtained from individuals without any systemic diseases presenting dentin caries lesions and were incubated in BHI added of with 5% sucrose for 96h to allow biofilm formation. After that, two consecutive micro-CT scans were acquired from each specimen (40kv and 70kv). Reconstruction of the images was performed using standardized parameters. After alignment, registration, filtering and image calculations, a final stack of images containing the biofilm volume was obtained from each prepared cavity. Dentin demineralization degree was quantified by comparison with sound dentin areas. All data were analyzed using Shapiro-Wilk test and Spearman correlation using α=5%.
Results
Dual-energy micro-CT technique disclosed biofilm formation in all cavities. Biofilm volume inside each cavity varied from 0.30 to 1.57mm3. A positive correlation between cavity volume and volume of formed biofilm was obtained (0.77, p<0.01). The mineral decrease obtained in dentin was high (±90%) for all cavities and all demineralized areas showed mineral density values lower than a defined threshold for dentin caries (1.2g/cm3).
Conclusion
Dual-energy micro-CT technique was successful in the quantification of a microcosm human bacterial biofilm formation and to quantify its demineralization potential in vitro."
}
@article{SMITH20113037,
title = "Evaluated Nuclear Data Covariances: The Journey From ENDF/B-VII.0 to ENDF/B-VII.1",
journal = "Nuclear Data Sheets",
volume = "112",
number = "12",
pages = "3037 - 3053",
year = "2011",
note = "Special Issue on ENDF/B-VII.1 Library",
issn = "0090-3752",
doi = "https://doi.org/10.1016/j.nds.2011.11.004",
url = "http://www.sciencedirect.com/science/article/pii/S0090375211001153",
author = "Donald L. Smith",
abstract = "Recent interest from data users on applications that utilize the uncertainties of evaluated nuclear reaction data has stimulated the data evaluation community to focus on producing covariance data to a far greater extent than ever before. Although some uncertainty information has been available in the ENDF/B libraries since the 1970ʼs, this content has been fairly limited in scope, the quality quite variable, and the use of covariance data confined to only a few application areas. Today, covariance data are more widely and extensively utilized than ever before in neutron dosimetry, in advanced fission reactor design studies, in nuclear criticality safety assessments, in national security applications, and even in certain fusion energy applications. The main problem that now faces the ENDF/B evaluator community is that of providing covariances that are adequate both in quantity and quality to meet the requirements of contemporary nuclear data users in a timely manner. In broad terms, the approach pursued during the past several years has been to purge any legacy covariance information contained in ENDF/B-VI.8 that was judged to be subpar, to include in ENDF/B-VII.0 (released in 2006) only those covariance data deemed then to be of reasonable quality for contemporary applications, and to subsequently devote as much effort as the available time and resources allowed to producing additional covariance data of suitable scope and quality for inclusion in ENDF/B-VII.1. Considerable attention has also been devoted during the five years since the release of ENDF/B-VII.0 to examining and improving the methods used to produce covariance data from thermal energies up to the highest energies addressed in the ENDF/B library, to processing these data in a robust fashion so that they can be utilized readily in contemporary nuclear applications, and to developing convenient covariance data visualization capabilities. Other papers included in this issue discuss in considerable detail various aspects of the data producer communityʼs efforts to improve the evaluation methods and to add covariance content to the ENDF/B library. The present paper offers just a brief glimpse of these activities by drawing material from covariance papers presented at meetings, workshops and international conferences during the past five years. Highlighted are: advances in methods for producing and processing covariance data, recently developed covariance visualization capabilities, and the development and implementation of quality assurance (QA) requirements that should be satisfied for covariance data to be included in ENDF/B-VII.1."
}
@incollection{KAILA2010249,
title = "3 - Magnetic Resonance Imaging Diagnostics of Human Brain Disorders",
editor = "Madan Kaila and Rakhi Kaila",
booktitle = "Quantum Magnetic Resonance Imaging Diagnostics of Human Brain Disorders",
publisher = "Elsevier",
address = "London",
pages = "249 - 503",
year = "2010",
isbn = "978-0-12-384711-9",
doi = "https://doi.org/10.1016/B978-0-12-384711-9.00003-8",
url = "http://www.sciencedirect.com/science/article/pii/B9780123847119000038",
author = "Madan Kaila and Rakhi Kaila",
abstract = "Publisher Summary
The signal in magnetic resonance imaging MRI sequences depends on chemical composition of tissues; cellular organization and metabolism are of little relevance. Diffusion-weighted images (DWI) is designed to evaluate diffusion movements of the molecules on a spatial axis. These incoherent microscopic molecular movements cause loss of phase of the protons and reduce signal intensity. Diffusion may be evaluated on a different spatial axis at different time/space scales, depending on the direction, timing, and strength of the applied magnetic field gradients. Signal may simply be converted into an image or may be further elaborated to obtain a quantitative map of the ADC. A region of restricted diffusion on a given spatial axis will appear hyperintense on the DWI and hypointense on the ADC maps. Diffusion in the tissues depends on capillary flow and on the intracellular movements requiring active metabolism. In addition proton magnetic resonance spectroscopy (PMRS) provides useful information, complementing that gained through MRI, with regard to cell membrane proliferation, neuronal damage, energy metabolism, and necrotic transformation of brain or tumor tissues. One needs to be able to interpret both images and the spectroscopy data accurately to derive information about the location, edema, mass effect, calcification, cyst formation, visualization, and contrast enhancement, given the patient's age and clinical presentation."
}
@article{TURKEZ2017257,
title = "Toxicogenomic responses of human alveolar epithelial cells to tungsten boride nanoparticles",
journal = "Chemico-Biological Interactions",
volume = "273",
pages = "257 - 265",
year = "2017",
issn = "0009-2797",
doi = "https://doi.org/10.1016/j.cbi.2017.06.027",
url = "http://www.sciencedirect.com/science/article/pii/S0009279717301308",
author = "Hasan Türkez and Mehmet Enes Arslan and Erdal Sönmez and Abdulgani Tatar and Metin Açikyildiz and Fatime Geyikoğlu",
keywords = "Tungsten boride nanoparticles, Microarray analysis, Toxicogenomics, Human lung alveolar epithelial cells",
abstract = "During the recent years, microarray analysis of gene expression has become an inevitable tool for exploring toxicity of drugs and other chemicals on biological systems. Therefore, toxicogenomics is considered as a fruitful area for searching cellular pathways and mechanisms including cancer, immunological diseases, environmental responses, gene-gene interactions and chemical toxicity. In this work, we examined toxic effects of Tungsten Borides NPs on gene expression profiling of the human lung alveolar epithelial cells (HPAEpiC). In line with this purpose, a single crystal of tungsten boride (mixture of WB and W2B) nanoparticles was synthesized by means of zone melting method, and characterized via using X-ray crystallography (XRD), transmission electron microscope (TEM), scanning electron microscope (SEM) and energy-dispersive X-ray spectroscopy (EDX) techniques. Cell viability and cytotoxicity were determined by 3-(4,5-dimethyl-thiazol-2-yl) 2,5-diphenyltetrazolium bromide (MTT), neutral red (NR) and lactate dehydrogenase (LDH) release tests. The whole genome microarray expression analysis was performed to find out the effects of WB and W2B NPs mixture on gene expression of the HPAEpiC cell culture. 123 of 40,000 gene probes were assigned to characterize expression profile for WB/W2B NPs exposure. According to results; 70 genes were up-regulated and 53 genes were down-regulated (≥2 fold change). For further investigations, these genes were functionally classified by using DAVID (The Database for Annotation, Visualization and Integrated Discovery) with gene ontology (GO) analysis. In the light of the data gained from this study, it could be concluded that the mixture of WB/W2B NPs can affect cytokine/chemokine metabolism, angiogenesis and prevent migration/invasion by activating various genes."
}
@article{FRONTZEK2018464,
title = "The Wide Angle Neutron Diffractometer squared (WAND2) - Possibilities and future",
journal = "Physica B: Condensed Matter",
volume = "551",
pages = "464 - 467",
year = "2018",
note = "The 11th International Conference on Neutron Scattering (ICNS 2017)",
issn = "0921-4526",
doi = "https://doi.org/10.1016/j.physb.2017.12.027",
url = "http://www.sciencedirect.com/science/article/pii/S0921452617310128",
author = "M.D. Frontzek and K.M. Andrews and A.B. Jones and B.C. Chakoumakos and J.A. Fernandez-Baca",
keywords = "Neutron scattering, Instrumentation",
abstract = "The Wide Angle Neutron Diffractometer (WAND) at the High Flux Isotope Reactor (HFIR), Oak Ridge National Laboratory (ORNL) has been built and continues to be, a joint project between ORNL and the Japan Atomic Energy Agency (JAEA). Equipped with a 1-dimensional position sensitive detector (PSD), the instrument is a multi-purpose instrument for both powder and single crystal diffraction. WAND is currently in the process of a 2-phase upgrade to become a world-class, general purpose instrument. In phase 1, finished in the beginning of 2016, the whole instrument was essentially re-built from scratch, keeping only the front-end and the 1-D PSD. Phase 2 will replace the 1-D PSD with the state-of-the-art BNL120 2-D PSD which comes from the Los Alamos Neutron Science Center. Currently, the detector is integrated off-line into the data acquisition architecture at HFIR and SNS. The new instrument, WAND2, will have event mode capability, improved efficiency, and higher resolution and will be available for general users in the proposal call 2018A. This contribution presents results highlighting the improvements on WAND after phase 1. The upgraded instrument now accommodates the whole suite of available sample environment (50 mK–1500 K, magnetic fields (5 T), high pressures (4 GPa)). Also, the background could be reduced significantly by a factor of 2 through improved shielding, allowing the detection of weak signals. The phase 2 upgrade will require new electronics, data acquisition, and visualization and will result in an altogether new instrument: WAND2."
}
@article{ASAHARA2018209,
title = "Exposure dose measurement during diagnostic pediatric X-ray examination using an optically stimulated luminescence (OSL) dosimeter based on precise dose calibration taking into consideration variation of X-ray spectra",
journal = "Radiation Measurements",
volume = "119",
pages = "209 - 219",
year = "2018",
issn = "1350-4487",
doi = "https://doi.org/10.1016/j.radmeas.2018.10.007",
url = "http://www.sciencedirect.com/science/article/pii/S1350448718304621",
author = "Takashi Asahara and Hiroaki Hayashi and Sota Goto and Emi Tomita and Natsumi Kimoto and Yoshiki Mihara and Takumi Asakawa and Yuki Kanazawa and Akitoshi Katsumata and Kosaku Higashino and Kazuta Yamashita and Tohru Okazaki and Takuya Hashizume",
keywords = "OSL dosimeter, Medical dosimetry, Scattered X-rays, Penetrating X-rays, Actual dose measurement, Pediatric X-ray examination",
abstract = "When dosimetry is performed in actual clinical situations using diagnostic X-rays, the energy dependence of the dosimeter should be taken into consideration. Although the dosimeter is calibrated with standard X-rays, it is unclear whether this dosimeter can be used in various clinical situations in which radiation exposure is not only caused by direct X-rays but also scattered X-rays. In this paper, in order to evaluate the applicability of an OSL dosimeter, we propose a novel method to derive a proper calibration factor using X-ray spectrum and the efficiency of the dosimeter. The X-ray spectra were calculated by means of a Monte-Carlo simulation code taking into consideration the effects of beam hardening, scattered X-rays and backscattering X-rays. As a result of analysis of calibration factors, we found that the difference of calibration factors from the standard, which was determined with 80 kV X-rays, was within 30%. Here, we focused our attention on dose measurement of medical staff who held a patient during pediatric X-ray examination; in this examination, a calibration factor can be determined precisely. In order to determine the dosimetric points, we mimicked a clinical situation using a neonate phantom and human dummies, and surface dose distributions to the human dummies were visualized. Based on the phantom study, we determined that the dosimetric positions of both hands and neck of the assistant during the actual clinical measurement were most important. Then, actual dosimetry in a clinical situation was performed. The OSL dosimeters were attached to the neck and hands of assistants during a pediatric X-ray examination, and doses were evaluated individually for each examination (N = 163). By analyzing data trends, we found that the dose to the assistant's hands was relatively higher when a younger patient was examined. In contrast, the dose to the neck position was the same regardless of the patient's age. In conclusion, we performed accurate dosimetry taking into consideration differences of X-ray spectra being incident to the dosimeter, and using our method, we can analyze individual doses to assistants for each pediatric X-ray examination."
}
@article{BUTH2018171,
title = "Training concept for and with digitalization in learning factories: An energy efficiency training case",
journal = "Procedia Manufacturing",
volume = "23",
pages = "171 - 176",
year = "2018",
note = "“Advanced Engineering Education & Training for Manufacturing Innovation”8th CIRP Sponsored Conference on Learning Factories (CLF 2018)",
issn = "2351-9789",
doi = "https://doi.org/10.1016/j.promfg.2018.04.012",
url = "http://www.sciencedirect.com/science/article/pii/S2351978918304840",
author = "Lennart Büth and Stefan Blume and Gerrit Posselt and Christoph Herrmann",
keywords = "Training, Digitalization, Energy Efficiency, Industry 4.0",
abstract = "Industry 4.0 is regarded as the fourth industrial revolution, posing huge challenges to industry in terms of technology implementation as well as human resources development. Adequate education and training is required in order to prepare employees for changes in their working environment related to quickly advancing digitalization. In particular, theoretical knowledge and practical skills regarding data acquisition, processing, visualization and interpretation are needed to exploit the full potential of digitalization. Learning factories offer a suitable environment to combine theoretical learning and practical application and are therefore predestined to impart Industry 4.0 knowledge and skills. Beyond this background, in this paper a training concept for industrial employees in a learning factory environment is described. The concept focuses on the topics energy transparency and efficiency in manufacturing, which is strongly enhanced by digitalization due to the need to combine sensor data, machine data and production planning and execution. The training covers relevant elements on the way from manual to highly automated solutions. Hence, it fosters both, the understanding of the basic methods as well as their implementation in the context of Industry 4.0. An overview of energy efficiency and digitalization trainings in learning factories is stated. Following, the specific training concept and the environment is described thereafter the concept is generalized. As the concept has been already implemented in practice, first experiences are used to show the effectiveness of this approach."
}
@article{GROGER201212,
title = "CityGML – Interoperable semantic 3D city models",
journal = "ISPRS Journal of Photogrammetry and Remote Sensing",
volume = "71",
pages = "12 - 33",
year = "2012",
issn = "0924-2716",
doi = "https://doi.org/10.1016/j.isprsjprs.2012.04.004",
url = "http://www.sciencedirect.com/science/article/pii/S0924271612000779",
author = "Gerhard Gröger and Lutz Plümer",
keywords = "Interoperability, Standards, GIS, Urban, City, Three-dimensional",
abstract = "CityGML is the international standard of the Open Geospatial Consortium (OGC) for the representation and exchange of 3D city models. It defines the three-dimensional geometry, topology, semantics and appearance of the most relevant topographic objects in urban or regional contexts. These definitions are provided in different, well-defined Levels-of-Detail (multiresolution model). The focus of CityGML is on the semantical aspects of 3D city models, its structures, taxonomies and aggregations, allowing users to employ virtual 3D city models for advanced analysis and visualization tasks in a variety of application domains such as urban planning, indoor/outdoor pedestrian navigation, environmental simulations, cultural heritage, or facility management. This is in contrast to purely geometrical/graphical models such as KML, VRML, or X3D, which do not provide sufficient semantics. CityGML is based on the Geography Markup Language (GML), which provides a standardized geometry model. Due to this model and its well-defined semantics and structures, CityGML facilitates interoperable data exchange in the context of geo web services and spatial data infrastructures. Since its standardization in 2008, CityGML has become used on a worldwide scale: tools from notable companies in the geospatial field provide CityGML interfaces. Many applications and projects use this standard. CityGML is also having a strong impact on science: numerous approaches use CityGML, particularly its semantics, for disaster management, emergency responses, or energy-related applications as well as for visualizations, or they contribute to CityGML, improving its consistency and validity, or use CityGML, particularly its different Levels-of-Detail, as a source or target for generalizations. This paper gives an overview of CityGML, its underlying concepts, its Levels-of-Detail, how to extend it, its applications, its likely future development, and the role it plays in scientific research. Furthermore, its relationship to other standards from the fields of computer graphics and computer-aided architectural design and to the prospective INSPIRE model are discussed, as well as the impact CityGML has and is having on the software industry, on applications of 3D city models, and on science generally."
}
@article{HAMID2018933,
title = "Unsteady stagnation-point flow of Williamson fluid generated by stretching/shrinking sheet with Ohmic heating",
journal = "International Journal of Heat and Mass Transfer",
volume = "126",
pages = "933 - 940",
year = "2018",
issn = "0017-9310",
doi = "https://doi.org/10.1016/j.ijheatmasstransfer.2018.05.076",
url = "http://www.sciencedirect.com/science/article/pii/S0017931018317307",
author = "Aamir Hamid and  Hashim and Masood Khan and Abdul Hafeez",
keywords = "Multiple solutions, Stretching/shrinking sheet, Magnetohydrodynamic, Williamson fluid, Viscous dissipation, Ohmic heating",
abstract = "On account of engineering applications and its complexity, the flows of non-Newtonian fluid caused by stretching/shrinking surface has turned out to be standout amongst the difficult research fields of fluid mechanics. Therefore, the central concern of this article is to identify the simultaneous consequences of viscous dissipation and Ohmic heating on the flow of an electrically conducting Williamson fluid generated by stretching/shrinking sheet. Flow is subject to a time-dependent magnetic field which is applied in transverse direction. The dimensionless variables are used to transform the governing equations including mass, momentum and energy conservation into ordinary differential equations. The reduced equations subject to the given boundary conditions have been solved by employing Runge-Kuta Fehlberg method followed by shooting technique. In addition, the multiple solutions for dimensionless fluid velocity and temperature distributions are captured when certain amount of mass suction is employed through the porous shrinking surface. The dynamic thermo-physical dimensionless parameters controlling the flow and heat transfer features are the magnetic parameter, viscosity ratio parameter, Weissenberg number, suction parameter, Eckert number and Prandtl number. It is visualized from multiple branches that the skin friction enhances with magnetic parameter for the upper branch solution and it reduces for lower branch solution. On the other side, obtained numerical results illustrated that the rate of heat transfer shows an accelerating trend with higher unsteadiness parameter for both the solutions. The obtained results show a better agreement of this model with respect to experimental data, compared to the homogeneous model."
}
@article{PERISSINOTTO2019881,
title = "Experimental and numerical study of oil drop motion within an ESP impeller",
journal = "Journal of Petroleum Science and Engineering",
volume = "175",
pages = "881 - 895",
year = "2019",
issn = "0920-4105",
doi = "https://doi.org/10.1016/j.petrol.2019.01.025",
url = "http://www.sciencedirect.com/science/article/pii/S092041051930035X",
author = "Rodolfo Marcilli Perissinotto and William {Monte Verde} and Mariana Gallassi and Gabriel F.N. Gonçalves and Marcelo Souza de Castro and João Carneiro and Jorge Luiz Biazussi and Antonio Carlos Bannwart",
keywords = "Centrifugal pump, Two-phase liquid-liquid flow, Flow visualization, Oil drop dynamics, CFD",
abstract = "The Electrical Submersible Pump (ESP) is a multistage centrifugal pump used in the petroleum industry as an artificial lift method. The ESP usually works with the presence of two-phase liquid-liquid flows that constitute dispersions and emulsions, causing performance losses and operational problems. This research aims to investigate the behavior and evaluate the dynamics of individual oil drops in an oil-in-water dispersion within an ESP impeller. The study adopts experimental and numerical approaches. Initially, experiments were performed using an experimental facility with a high-speed camera and an ESP prototype working at 600 rpm and 900 rpm, for water flows around the Best Efficiency Point (BEP) and with the injection of oil drops at a low flow rate. The acquired images were processed, and a drop sample was tracked, enabling the analysis of the size, shape, path, velocity, and acceleration of the oil drops. Numerical simulations were executed in ANSYS® software to define relevant parameters related to water and oil drops, such as velocities, accelerations, forces, turbulent dissipation, and residence time. The images reveal a unique flow pattern of dispersed drops in a continuous water phase. The oil drops' diameters vary from tenths of a millimeter to around 3 mm. The drops' trajectories can be classified into three different regions within the impeller channels. The drops’ velocities stay in the order of 1 m/s, while accelerations can reach hundreds of m/s2. The velocity profiles show that the oil drops tend to decelerate during their trajectory, while the acceleration profiles suggest peaks at the channel inlet and outlet. High intense turbulence is present in the impeller entrance zone. The evaluation of the residence time and the particle Reynolds number suggest that smaller oil drops follow the water streamlines, while larger oil drops tend to be affected by external forces. The main forces that govern the oil drop motion are the drag, the pressure gradient, and the virtual mass forces. The force from the pressure gradient is tenfold greater than the force from the drag. The virtual mass effect is significant only in the impeller inlet. In general, in this research, numerical results show a satisfactory agreement with the experimental data."
}
@article{TOYOKUNI2017610,
title = "Iron and thiol redox signaling in cancer: An exquisite balance to escape ferroptosis",
journal = "Free Radical Biology and Medicine",
volume = "108",
pages = "610 - 626",
year = "2017",
issn = "0891-5849",
doi = "https://doi.org/10.1016/j.freeradbiomed.2017.04.024",
url = "http://www.sciencedirect.com/science/article/pii/S0891584917302289",
author = "Shinya Toyokuni and Fumiya Ito and Kyoko Yamashita and Yasumasa Okazaki and Shinya Akatsuka",
keywords = "Iron, Cancer, Carcinogenesis, Redox signaling, Ferroptosis",
abstract = "Epidemiological data indicate a constant worldwide increase in cancer mortality, although the age of onset is increasing. Recent accumulation of genomic data on human cancer via next-generation sequencing confirmed that cancer is a disease of genome alteration. In many cancers, the Nrf2 transcription system is activated via mutations either in Nrf2 or Keap1 ubiquitin ligase, leading to persistent activation of the genes with antioxidative functions. Furthermore, deep sequencing of passenger mutations is clarifying responsible cancer causative agent(s) in each case, including aging, APOBEC activation, smoking and UV. Therefore, it is most likely that oxidative stress is the principal initiating factor in carcinogenesis, with the involvement of two essential molecules for life, iron and oxygen. There is evidence based on epidemiological and animal studies that excess iron is a major risk for carcinogenesis, suggesting the importance of ferroptosis-resistance. Microscopic visualization of catalytic Fe(II) has recently become available. Although catalytic Fe(II) is largely present in lysosomes, proliferating cells harbor catalytic Fe(II) also in the cytosol and mitochondria. Oxidative stress catalyzed by Fe(II) is counteracted by thiol systems at different functional levels. Nitric oxide, carbon monoxide and hydrogen (per)sulfide modulate these reactions. Mitochondria generate not only energy but also heme/iron sulfur cluster cofactors and remain mostly dysfunctional in cancer cells, leading to Warburg effects. Cancer cells are under persistent oxidative stress with a delicate balance between catalytic iron and thiols, thereby escaping ferroptosis. Thus, high-dose L-ascorbate and non-thermal plasma as well as glucose/glutamine deprivation may provide additional benefits as cancer therapies over preexisting therapeutics."
}
@article{FARIA2019646,
title = "Assessing electric mobility feasibility based on naturalistic driving data",
journal = "Journal of Cleaner Production",
volume = "206",
pages = "646 - 660",
year = "2019",
issn = "0959-6526",
doi = "https://doi.org/10.1016/j.jclepro.2018.09.217",
url = "http://www.sciencedirect.com/science/article/pii/S0959652618329500",
author = "Marta Faria and Gonçalo Duarte and Patrícia Baptista",
keywords = "Naturalistic driving data, Electric vehicles, Feasibility analysis, Mobility characterization, Street level",
abstract = "In a context where electric mobility is gaining increasing importance as a more sustainable solution for urban environments, this work presents an analysis of electric mobility feasibility and adequacy based on private users' naturalistic driving data. Several scenarios were tested to evaluate different charging event opportunities and their impacts on electric mobility feasibility. In more detail, scenario 1 considered that vehicles would recharge whenever they are stopped for 2, 4 or 6 h, either on weekdays or weekend days; scenario 2 tested the hypothesis of recharging only during the night period; and scenario 3 assumed that vehicles would recharge during the day on weekdays. Furthermore, the potential energy impacts of electric mobility at a city level, by applying a driver and street level approach, were also estimated. Results revealed that electric mobility is highly feasible for weekday urban trips, while weekend trips due to their higher average distance are less suitable to be performed by EVs. Scenario 1, due to its higher recharging opportunities was found to be the best-case scenario. In this case, the percentage of eligible trips was found to be equal to or higher than 94% and 88% on weekdays and weekend days, respectively. Results showed also the lower electric mobility feasibility if considering only daytime charging, on weekdays (scenario 3). However, if considering night charging (scenario 2), the electric mobility eligibility was found to improve significantly. When considering a street level analysis, the potential reduction in energy consumption ranges in average from −60 to −70%, enabling the visualization of higher EV potential, with increasing potential for reducing energy consumption for increasing road grades. Concluding, since electric mobility is particularly suited for urban driving and most households detain 2 or more vehicles, there is a high potential to replace at least one ICEV by an EV. In this case, people may adapt their driving behavior, using the EV for their day-to-day urban driving while the ICEV would be used for longer trips. Nonetheless, the capacity to recharge during night plays a significant role on trips eligibility. Therefore, the availability of home-charge set-ups or a much higher deployment of public charging stations at residential locations are required in order to incentivize drivers to shift towards electric mobility."
}
@article{REARDEN2015130,
title = "Monte Carlo capabilities of the SCALE code system",
journal = "Annals of Nuclear Energy",
volume = "82",
pages = "130 - 141",
year = "2015",
note = "Joint International Conference on Supercomputing in Nuclear Applications and Monte Carlo 2013, SNA + MC 2013. Pluri- and Trans-disciplinarity, Towards New Modeling and Numerical Simulation Paradigms",
issn = "0306-4549",
doi = "https://doi.org/10.1016/j.anucene.2014.08.019",
url = "http://www.sciencedirect.com/science/article/pii/S0306454914004046",
author = "B.T. Rearden and L.M. Petrie and D.E. Peplow and K.B. Bekar and D. Wiarda and C. Celik and C.M. Perfetti and A.M. Ibrahim and S.W.D. Hart and M.E. Dunn and W.J. Marshall",
keywords = "SCALE, Monte Carlo, Criticality safety, Shielding, Sensitivity analysis, Depletion",
abstract = "SCALE is a widely used suite of tools for nuclear systems modeling and simulation that provides comprehensive, verified and validated, user-friendly capabilities for criticality safety, reactor physics, radiation shielding, and sensitivity and uncertainty analysis. For more than 30years, regulators, licensees, and research institutions around the world have used SCALE for nuclear safety analysis and design. SCALE provides a “plug-and-play” framework that includes three deterministic and three Monte Carlo radiation transport solvers that can be selected based on the desired solution, including hybrid deterministic/Monte Carlo simulations. SCALE includes the latest nuclear data libraries for continuous-energy and multigroup radiation transport as well as activation, depletion, and decay calculations. SCALE’s graphical user interfaces assist with accurate system modeling, visualization, and convenient access to desired results. SCALE 6.2 will provide several new capabilities and significant improvements in many existing features, especially with expanded continuous-energy Monte Carlo capabilities for criticality safety, shielding, depletion, and sensitivity and uncertainty analysis. An overview of the Monte Carlo capabilities of SCALE is provided here, with emphasis on new features for SCALE 6.2."
}
@article{BERGER2016134,
title = "The molecular relationship between antigenic domains and epitopes on hCG",
journal = "Molecular Immunology",
volume = "76",
pages = "134 - 145",
year = "2016",
issn = "0161-5890",
doi = "https://doi.org/10.1016/j.molimm.2016.06.015",
url = "http://www.sciencedirect.com/science/article/pii/S0161589016301171",
author = "Peter Berger and Adrian J. Lapthorn",
keywords = "hCG, Variants, Antigenic domains, Number of epitopes, Harmonizing immunoassays, Pregnancy testing",
abstract = "Antigenic domains are defined to contain a limited number of neighboring epitopes recognized by antibodies (Abs) but their molecular relationship remains rather elusive. We thoroughly analyzed the antigenic surface of the important pregnancy and tumor marker human chorionic gonadotropin (hCG), a cystine knot (ck) growth factor, and set antigenic domains and epitopes in molecular relationships to each other. Antigenic domains on hCG, its free hCGα and hCGβ subunits are dependent on appropriate inherent molecular features such as molecular accessibility and protrusion indices that determine bulging structures accessible to Abs. The banana-shaped intact hCG comprises ∼7500Å2 of antigenic surface with minimally five antigenic domains that encompass a continuum of overlapping non-linear composite epitopes, not taking into account the C-terminal peptide extension of hCGβ (hCGβCTP). Epitopes within an antigenic domain are defined by specific Abs, that bury nearly 1000Å2 of surface accessible area on the antigen and recognize a few up to 15 amino acid (aa) residues, whereby between 2 and 5 of these provide the essential binding energy. Variability in Ab binding modes to the contact aa residues are responsible for the variation in affinity and intra- and inter-species specificity, e.g. cross-reactions with luteinizing hormone (LH). Each genetically distinct fragment antigen binding (Fab) defines its own epitope. Consequently, recognition of the same epitope by different Abs is only possible in cases of genetically identical sequences of its binding sites. Due to combinatorial V(D)J gene segment variability of heavy and light chains, Abs defining numerous epitopes within an antigenic domain can be generated by different individuals and species. Far more than hundred Abs against the immuno-dominant antigenic domains of either subunit at both ends of the hCG-molecule, the tips of peptide loops one and three (Ł1+3) protruding from the central ck, encompassing hCGβŁ1+3 (aa 20–25+64+68–81) and hCGαŁ1 (aa 13–22; Pro16, Phe17, Phe18) plus hCGαŁ3 (Met71, Phe74), respectively, have been identified in the two “ISOBM Tissue Differentiation-7 Workshops on hCG and Related Molecules” and in other studies. These Abs recognize distinct but overlapping epitopes with slightly different specificity profiles and affinities. Heterodimeric-specific epitopes involve neighboring αŁ1 plus βŁ2 (hCGβ44/45 and 47/48). Diagnostically important Abs recognize the middle of the molecule, the ck (aa Arg10, Arg60 and possibly Gln89) and the linear hCGβCTP “tail” (aa 135–145; Asp139, Pro144, Gln145), respectively. Identification of antigenic domains and of specific epitopes is essential for harmonization of Abs in methods that are used for reliable and robust hCG measurements for the management of pregnancy, pregnancy-related disease and tumors."
}
@article{BAKER201796,
title = "An energy-aware service composition algorithm for multiple cloud-based IoT applications",
journal = "Journal of Network and Computer Applications",
volume = "89",
pages = "96 - 108",
year = "2017",
note = "Emerging Services for Internet of Things (IoT)",
issn = "1084-8045",
doi = "https://doi.org/10.1016/j.jnca.2017.03.008",
url = "http://www.sciencedirect.com/science/article/pii/S1084804517301108",
author = "Thar Baker and Muhammad Asim and Hissam Tawfik and Bandar Aldawsari and Rajkumar Buyya",
keywords = "IoT, Multi-cloud, Service composition, Energy efficiency",
abstract = "There has been a shift in research towards the convergence of the Internet-of-Things (IoT) and cloud computing paradigms motivated by the need for IoT applications to leverage the unique characteristics of the cloud. IoT acts as an enabler to interconnect intelligent and self-configurable nodes “things” to establish an efficient and dynamic platform for communication and collaboration. IoT is becoming a major source of big data, contributing huge amounts of streamed information from a large number of interconnected nodes, which have to be stored, processed, and presented in an efficient, and easily interpretable form. Cloud computing can enable IoT to have the privilege of a virtual resources utilization infrastructure, which integrates storage devices, visualization platforms, resource monitoring, analytical tools, and client delivery. Given the number of things connected and the amount of data generated, a key challenge is the energy efficient composition and interoperability of heterogeneous things integrated with cloud resources and scattered across the globe, in order to create an on-demand energy efficient cloud based IoT application. In many cases, when a single service is not enough to complete the business requirement; a composition of web services is carried out. These composed web services are expected to collaborate towards a common goal with large amount of data exchange and various other operations. Massive data sets have to be exchanged between several geographically distributed and scattered services. The movement of mass data between services influences the whole application process in terms of energy consumption. One way to significantly reduce this massive data exchange is to use fewer services for a composition, which need to be created to complete a business requirement. Integrating fewer services can result in a reduction in data interchange, which in return helps in reducing the energy consumption and carbon footprint. This paper develops a novel multi-cloud IoT service composition algorithm called (E2C2) that aims at creating an energy-aware composition plan by searching for and integrating the least possible number of IoT services, in order to fulfil user requirements. A formal user requirements translation and transformation modelling and analysis is adopted for the proposed algorithm. The algorithm was evaluated against four established service composition algorithms in multiple cloud environments (All clouds, Base cloud, Smart cloud, and COM2), with the results demonstrating the superior performance of our approach."
}
@article{JAFARI2017137,
title = "Geological CO2 sequestration in saline aquifers: Implication on potential solutions of China’s power sector",
journal = "Resources, Conservation and Recycling",
volume = "121",
pages = "137 - 155",
year = "2017",
note = "Environmental Challenges and Potential Solutions of China’s Power Sector",
issn = "0921-3449",
doi = "https://doi.org/10.1016/j.resconrec.2016.05.014",
url = "http://www.sciencedirect.com/science/article/pii/S0921344916301240",
author = "Mohammad Jafari and Shuang Cindy Cao and Jongwon Jung",
keywords = "CCS, GCS, Geological CO sequestration, Saline aquifer, Carbon capture and storage",
abstract = "The rapid growth of energy demands in China surpasses the progress of introducing new clean energy sources. China has relied upon fossil fuel for several decades, which caused China to produce the largest CO2 emission and to influence climate change in the world. Thus, China’s fossil fuel-dependent power sector needs to reduce CO2 emission. Carbon capture and storage (CCS) is one of the solutions to decrease CO2 emission, and geological CO2 sequestration (GCS) is recommended considering its high potential and effectiveness. In this study, the efforts to implement geological CO2 sequestration in China are reviewed, and current technical issues are addressed. The potential storage candidates including depleted oil and gas reservoirs, unminable coal seams, saline aquifers, and hydrate bearing sediments are introduced with the data collected from the pilot, demonstration, and large-scale projects in China. Among potential sites, saline aquifers have been considered as sites with the highest potential for CO2 storage in China because of their enormous capacity. Main trapping mechanisms including structural-, capillary residual-, solubility- and mineral-trappings support saline aquifers as the most possible CO2 storage site. Also, CO2 injectivity and CO2-brine displacement efficiency in saline aquifers are explored to improve the efficiency of CO2 injection with current techniques including visualizing experimental testing method for two-phase immiscible flow such as microfluidic model and X-ray computed tomography (X-CT) method. Finally, regulatory acts in China are explained as the potential rules for monitoring the safety of the GCS projects in China."
}
@article{XI2010245,
title = "MONICA: a compact, portable dual gamma camera system for mouse whole-body imaging",
journal = "Nuclear Medicine and Biology",
volume = "37",
number = "3",
pages = "245 - 253",
year = "2010",
issn = "0969-8051",
doi = "https://doi.org/10.1016/j.nucmedbio.2009.12.003",
url = "http://www.sciencedirect.com/science/article/pii/S0969805109002935",
author = "Wenze Xi and Jurgen Seidel and John W. Kakareka and Thomas J. Pohida and Diane E. Milenic and James Proffitt and Stan Majewski and Andrew G. Weisenberger and Michael V. Green and Peter L. Choyke",
keywords = "MONICA, Small animal imaging, Cancer drug development, Mouse whole-body imaging, Miniature gamma cameras, Single photon imaging",
abstract = "Introduction
We describe a compact, portable dual-gamma camera system (named “MONICA” for MObile Nuclear Imaging CAmeras) for visualizing and analyzing the whole-body biodistribution of putative diagnostic and therapeutic single photon emitting radiotracers in animals the size of mice.
Methods
Two identical, miniature pixelated NaI(Tl) gamma cameras were fabricated and installed “looking up” through the tabletop of a compact portable cart. Mice are placed directly on the tabletop for imaging. Camera imaging performance was evaluated with phantoms and field performance was evaluated in a weeklong In-111 imaging study performed in a mouse tumor xenograft model.
Results
Tc-99m performance measurements, using a photopeak energy window of 140 keV±10%, yielded the following results: spatial resolution (FWHM at 1 cm), 2.2 mm; sensitivity, 149 cps (counts per seconds)/MBq (5.5 cps/μCi); energy resolution (FWHM, full width at half maximum), 10.8%; count rate linearity (count rate vs. activity), r2=0.99 for 0–185 MBq (0–5 mCi) in the field of view (FOV); spatial uniformity, <3% count rate variation across the FOV. Tumor and whole-body distributions of the In-111 agent were well visualized in all animals in 5-min images acquired throughout the 168-h study period.
Conclusion
Performance measurements indicate that MONICA is well suited to whole-body single photon mouse imaging. The field study suggests that inter-device communications and user-oriented interfaces included in the MONICA design facilitate use of the system in practice. We believe that MONICA may be particularly useful early in the (cancer) drug development cycle where basic whole-body biodistribution data can direct future development of the agent under study and where logistical factors, e.g., limited imaging space, portability and, potentially, cost are important."
}
@incollection{PAWADE20201,
title = "1 - Introduction to electronic spectroscopy of lanthanide, properties, and their applications",
editor = "S.J. Dhoble and Vijay B. Pawade and Hendrik C. Swart and Vibha Chopra",
booktitle = "Spectroscopy of Lanthanide Doped Oxide Materials",
publisher = "Woodhead Publishing",
pages = "1 - 20",
year = "2020",
series = "Woodhead Publishing Series in Electronic and Optical Materials",
isbn = "978-0-08-102935-0",
doi = "https://doi.org/10.1016/B978-0-08-102935-0.00001-0",
url = "http://www.sciencedirect.com/science/article/pii/B9780081029350000010",
author = "Vijay B. Pawade and Vibha Chopra and S.J. Dhoble",
keywords = "Ln ions, Properties, Energy levels, Energy transfer, Applications",
abstract = "The element shown in the bottom of the periodic table is classified as lanthanide series element. There are total 15 element occurred in this series including La3+, Ce3+, Pr3+, Nd3+, Pm3+, Sm3+, Eu3+, Gd3+, Tb3+, Dy3+, HO3+, Er3+, Tm3+, Yb3+, and Lu3+. These elements are arranged on the basis of atomic number. From the long years, back lanthanide elements are used as dopant ions in many inorganic host lattices such as aluminates, borates, silicates, tungstate, fluoride, phosphate, and vanadates. In past few years, a lot of study had been carried out on the location of lanthanide level into band structure of several inorganic host compounds. This type of study is very much useful to predicting the exact location of lanthanide level and 4f-electron binding energies based on some theoretical model to calculate red shift, centroid shift, charge transfer, and chemical shift. These are studied on the basis of the observed spectroscopic data to draw the host referred binding energy level scheme in which the 4fn and 4fn−1 5d levels for all divalent and trivalent lanthanides ions are placed relative to the top of the V·B; however, in case of vacuum referred binding energy scheme, all the energy levels are relative to that of an electron at rest in vacuum. Therefore, such energy level scheme provided methods for the visualization of the relevant location of lanthanide levels in relative in the band structure of the inorganic host lattice. Also, we know that the application of lanthanide-doped inorganic host depends upon the observed photophysical properties. Among the 15 lanthanide element, Ce3+, Pr3+, Sm3+, Eu3+, Tb3+, and Dy3+ are potentially used in LEDs and display devices. However, the other elements such as Nd3+, HO3+, Er3+, Tm3+, and Yb3+ show near visible-NIR emission spectra and exhibit the emission bands in visible to NIR range. Recently, upconversion luminescent nanoparticle is proposed for wastewater treatment. In past few years, researcher reported many types of lanthanide-doped luminescent host materials for lighting devices. But only few of them are commercially used. Currently, QD-based LEDs attract the attention of many researchers worldwide due to their high brightness and high quantum yield. Only few materials are reported for such types of LED applications. Hence, the search for new luminescent materials and QDs is not end in near future due to continuous innovation and development in exiting indoor and outdoor lighting technology. Recently, lanthanide-doped (Ce3+, Pr3+, Sm3+, Yb2+, Nd3+, etc.) oxide materials show excellent luminescence properties in UV visible to NIR range and also applicable as an active layer on the front surface to improve conversion efficiency of Si solar cell. Among the different types of renewable and sustainable source of energy, solar energy is a prominent and a clean source for energy generation. Thus, PV solar cells are the nonpolluting energy source of the 21st century. And it is well known that Si solar cell is everywhere accepted for the PV application than the other PV devices like organic solar cell, perovskite solar cell, Tandem solar cell, and CdTe cell. It is most commercially used from few years. Now, researchers are focused on finding other alternatives for Si cell and also they are taking an effort on how to increase the efficiency of existing Si solar cell. Therefore, the luminescent materials with UC/DC phosphor mechanism were theoretically and practically proposed by many research groups to enhance the PV response of Si solar cell. Hence, the lanthanide ions play an important role in modifying the optical, electronic, electrical, and catalytic activity when it doped with some host lattices. In the following section, we have discussed the some important properties of lanthanides ions."
}
@article{NA2010432,
title = "Interaction of Rayleigh surface waves with a tightly closed fatigue crack",
journal = "NDT & E International",
volume = "43",
number = "5",
pages = "432 - 439",
year = "2010",
issn = "0963-8695",
doi = "https://doi.org/10.1016/j.ndteint.2010.04.003",
url = "http://www.sciencedirect.com/science/article/pii/S0963869510000411",
author = "Jeong K. Na and James L. Blackshire",
keywords = "Interdigital transducer (IDT), Rayleigh surface waves, Fatigue crack, Laser interferometry",
abstract = "A miniature interdigital transducer (IDT) tuned to resonate at 3.1MHz of frequency is designed, fabricated, and used to generate narrow band Rayleigh waves on aircraft grade metallic alloys. Electrodes of the IDT are precisely machined by a laser machining technique and its acoustical wave properties are characterized by using a scanning laser interferometry system with a spatial resolution of less than 10 microns. Unlike the conventional contact surface wave generation methods using wedges or combs, the biggest advantage of an IDT are its miniature physical size and its high efficiency in converting electrical signals to mechanical vibrations. Narrow band surface waves with finite amplitudes generated by an IDT are used to investigate the wave interaction with a tightly closed fatigue crack on a metallic test specimen. High-resolution images for the time-resolved ultrasonic field and time-averaged amplitude displacement are generated to understand the interaction of the wave with a surface-breaking fatigue crack. From the amplitude displacement data, three-dimensional surface contour plots of the wave energy are generated to find out how the elastic energy interacts with the tight fatigue crack interfaces. Results show that a tightly closed fatigue crack can transmit Rayleigh waves through the crack due to interfacial contact within the crack, where transmission of elastic energy was found to be more dominant towards the crack tip as expected. The sum of transmitted and reflected energy at the crack interface suggests that there is an additional energy loss mechanism which is proved experimentally by visualizing part of the incident Rayleigh wave energy propagating along the interfacial surfaces of the crack and continues its propagation along the surface of the opposite side of the test specimen."
}
@article{SILERYTE2017130,
title = "Automated generation of versatile data model for analyzing urban architectural void",
journal = "Computers, Environment and Urban Systems",
volume = "66",
pages = "130 - 144",
year = "2017",
issn = "0198-9715",
doi = "https://doi.org/10.1016/j.compenvurbsys.2017.08.008",
url = "http://www.sciencedirect.com/science/article/pii/S0198971517301412",
author = "Rusne Sileryte and Ljiljana Cavic and Jose Nuno Beirao",
keywords = "Spatial data modeling, Urban architectural void, Convex partitioning, Urban computing",
abstract = "Urban environments are defined and modeled in a variety of ways depending on the scientific approach to analyze them. Even though a number of analysis could benefit from using a single model and re-using results of one for the sake of the other, so far no single data model is available. Moreover, the existing standardized models focus on describing objects in and around urban architectural void rather than the spaces themselves. Nevertheless, a number of phenomena such as heat, energy, pollution, also including social and mobility aspects would undoubtedly benefit from using a model that is explicitly focused on defining the urban architectural void and its characteristics as continuous field, interconnected network or series of spatial units. Therefore, this paper aims to suggest a versatile data model that would allow to separate, interpret, analyze and visualize the urban architectural void using a standardized automated procedure. The model relies on Gestalt theories for space compartmentalization. It allows performing various kinds of analysis and storing their results in a unified format using core concepts of GIS. The model can be rendered both as a 2D and 3D representation. Finally, user intervention and parameter calibration is allowed at every principal step of an automated procedure."
}
@article{SIMBEYE201431,
title = "Design and deployment of wireless sensor networks for aquaculture monitoring and control based on virtual instruments",
journal = "Computers and Electronics in Agriculture",
volume = "102",
pages = "31 - 42",
year = "2014",
issn = "0168-1699",
doi = "https://doi.org/10.1016/j.compag.2014.01.004",
url = "http://www.sciencedirect.com/science/article/pii/S0168169914000064",
author = "Daudi S. Simbeye and Jimin Zhao and Shifeng Yang",
keywords = "Aquaculture, Environmental parameters, Wireless sensor network, LabWindows/CVI, Monitoring and control, ZigBee",
abstract = "Aquaculture is moving toward an intensive controlled environment production with a significant increase in production, but at a cost of increased risk of catastrophic loss due to equipment or management failures. In addition, managers of intensive production facilities need accurate, real-time information on system status and performance in order to maximize their potential. This work has developed and deployed low cost short-range modules of wireless sensor network based on ZigBee standard and virtual instruments technology in order to monitor and control an aquaculture system in real time. The system consists of smart sensor nodes, coordinator/gateway node and personal computer (PC). The smart sensor nodes monitor environmental parameters such as dissolved oxygen, water temperature, pH and water level using relevant sensors, transmit this information to the coordinator/gateway node through ZigBee network and receive control signals for actuator control. The coordinator/gateway node receives data acquired and sends command to PC in order to achieve human–computer visualization interface. The graphical user interface (GUI) was designed by LabWindows/CVI software platform so that users can observe, investigate and analyze the related scientific and accuracy of parameters in aquaculture environment. We have implemented our method for two sensor network nodes deployed in fish ponds and monitored the results for six months indicating that the power management and networking solutions adopted to work in practice, lead to maximize monitoring, control as well as the recording of the aquaculture system. It effectively reduces the probability of high risk of fish mortality through enabling constant monitoring of the critical parameters in the aquaculture environment. This situation in effect increases economic benefit for aquaculture, consumer confidence and safety while reducing labor cost and energy consumption."
}
@article{CARASCO20102210,
title = "MCNP output data analysis with ROOT (MODAR)",
journal = "Computer Physics Communications",
volume = "181",
number = "12",
pages = "2210 - 2211",
year = "2010",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2010.08.028",
url = "http://www.sciencedirect.com/science/article/pii/S001046551000322X",
author = "C. Carasco",
keywords = "ROOT, C++, Object-oriented programming, MCNP, Data processing",
abstract = "MCNP Output Data Analysis with ROOT (MODAR) is a tool based on CERN's ROOT software. MODAR has been designed to handle time-energy data issued by MCNP simulations of neutron inspection devices using the associated particle technique. MODAR exploits ROOT's Graphical User Interface and functionalities to visualize and process MCNP simulation results in a fast and user-friendly way. MODAR allows to take into account the detection system time resolution (which is not possible with MCNP) as well as detectors energy response function and counting statistics in a straightforward way.
New version program summary
Program title: MODAR Catalogue identifier: AEGA_v1_1 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEGA_v1_1.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 150 927 No. of bytes in distributed program, including test data, etc.: 4 981 633 Distribution format: tar.gz Programming language: C++ Computer: Most Unix workstations and PCs Operating system: Most Unix systems, Linux and windows, provided the ROOT package has been installed. Examples where tested under Suse Linux and Windows XP. RAM: Depends on the size of the MCNP output file. The example presented in the article, which involves three two dimensional 139×740 bins histograms, allocates about 60 MB. These data are running under ROOT and include consumption by ROOT itself. Classification: 17.6 Catalogue identifier of previous version: AEGA_v1_0 Journal reference of previous version: Comput. Phys. Comm. 181 (2010) 1161 External routines: ROOT version 5.24.00 (http://root.cern.ch/drupal/) Does the new version supersede the previous version?: Yes Nature of problem: The output of a MCNP simulation is an ascii file. The data processing is usually performed by copying and pasting the relevant parts of the ascii file into Microsoft Excel. Such an approach is satisfactory when the quantity of data is small but is not efficient when the size of the simulated data is large, for example when time-energy correlations are studied in detail such as in problems involving the associated particle technique. In addition, since the finite time resolution of the simulated detector cannot be modeled with MCNP, systems in which time-energy correlation is crucial cannot be described in a satisfactory way. Finally, realistic particle energy deposit in detectors is calculated with MCNP in a two step process involving type-5 then type-8 tallies. In the first step, the photon flux energy spectrum associated to a time region is selected and serves as a source energy distribution for the second step. Thus, several files must be manipulated before getting the result, which can be time consuming if one needs to study several time regions or different detectors performances. In the same way, modeling counting statistics obtained in a limited acquisition time requires several steps and can also be time consuming. Solution method: In order to overcome the previous limitations, the MODAR C++ code has been written to make use of CERN's ROOT data analysis software. MCNP output data are read from the MCNP output file with dedicated routines. Two dimensional histograms are filled and can be handled efficiently within the ROOT framework. To keep a user friendly analysis tool, all processing and data display can be done by means of ROOT Graphical User Interface. Specific routines have been written to include detectors finite time resolution and energy response function as well as counting statistics in a straightforward way. Reasons for new version: For applications involving the Associate Particle Technique, a large number of gamma rays are produced by the fast neutrons interactions. To study the energy spectra, it is useful to identify the gamma-ray energy peaks in a straightforward way. Therefore, the possibility to show gamma rays corresponding to specific reactions has been added in MODAR. Summary of revisions: It is possible to use a gamma ray database to better identify in the energy spectra gamma ray peaks with their first and second escapes. Histograms can be scaled by the number of source particle to evaluate the number of counts that is expected without statistical uncertainties. Additional comments: The possibility of adding tallies has also been incorporated in MODAR in order to describe systems in which the signal from several detectors can be summed. Moreover, MODAR can be adapted to handle other problems involving two dimensional data. Running time: The CPU time needed to smear a two dimensional histogram depends on the size of the histogram. In the presented example, the time-energy smearing of one of the 139×740 two dimensional histograms takes 3 minutes with a DELL computer equipped with INTEL Core 2."
}
@article{NGUYEN20143344,
title = "CPMC-Lab: A Matlab package for Constrained Path Monte Carlo calculations",
journal = "Computer Physics Communications",
volume = "185",
number = "12",
pages = "3344 - 3357",
year = "2014",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2014.08.003",
url = "http://www.sciencedirect.com/science/article/pii/S0010465514002707",
author = "Huy Nguyen and Hao Shi and Jie Xu and Shiwei Zhang",
keywords = "Quantum Monte Carlo, Auxiliary field quantum Monte Carlo, Constrained Path Monte Carlo, Sign problem, Hubbard model, Pedagogical software",
abstract = "We describe CPMC-Lab, a Matlab program for the constrained-path and phaseless auxiliary-field Monte Carlo methods. These methods have allowed applications ranging from the study of strongly correlated models, such as the Hubbard model, to ab initio calculations in molecules and solids. The present package implements the full ground-state constrained-path Monte Carlo (CPMC) method in Matlab with a graphical interface, using the Hubbard model as an example. The package can perform calculations in finite supercells in any dimensions, under periodic or twist boundary conditions. Importance sampling and all other algorithmic details of a total energy calculation are included and illustrated. This open-source tool allows users to experiment with various model and run parameters and visualize the results. It provides a direct and interactive environment to learn the method and study the code with minimal overhead for setup. Furthermore, the package can be easily generalized for auxiliary-field quantum Monte Carlo (AFQMC) calculations in many other models for correlated electron systems, and can serve as a template for developing a production code for AFQMC total energy calculations in real materials. Several illustrative studies are carried out in one- and two-dimensional lattices on total energy, kinetic energy, potential energy, and charge- and spin-gaps.
Program summary
Program title: CPMC-Lab Catalogue identifier: AEUD_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEUD_v1_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 2850 No. of bytes in distributed program, including test data, etc.: 24838 Distribution format: tar.gz Programming language:Matlab. Computer: The non-interactive scripts can be executed on any computer capable of running Matlab with all Matlab versions. The GUI requires Matlab R2010b (version 7.11) and above. Operating system: Windows, Mac OS X, Linux. RAM: Variable. Classification: 7.3. External routines:Matlab Nature of problem: Obtaining ground state energy of a repulsive Hubbard model in a supercell in any number of dimensions. Solution method: In the Constrained Path Monte Carlo (CPMC) method, the ground state of a many-fermion system is projected from an initial trial wave function by a branching random walk in an overcomplete basis of Slater determinants. Constraining the determinants according to a trial wave function |ΨT〉 removes the exponential decay of the signal-to-noise ratio characteristic of the sign problem. The method is exact if |ΨT〉 is exact. Unusual features: Direct and interactive environment with a Graphical User Interface for beginners to learn and study the Constrained Path Monte Carlo method with minimal overhead for setup. Running time: The sample provided takes a few seconds to run, the batch sample a few minutes."
}
@article{SEIDEL2013321,
title = "Performance characteristics of a positron projection imager for mouse whole-body imaging",
journal = "Nuclear Medicine and Biology",
volume = "40",
number = "3",
pages = "321 - 330",
year = "2013",
issn = "0969-8051",
doi = "https://doi.org/10.1016/j.nucmedbio.2012.12.003",
url = "http://www.sciencedirect.com/science/article/pii/S0969805112003149",
author = "Jurgen Seidel and Wenze Xi and John W. Kakareka and Thomas J. Pohida and Elaine M. Jagoda and Michael V. Green and Peter L. Choyke",
keywords = "Pre-clinical imaging, Cancer drug development, Mouse whole-body imaging, Positron projection imaging",
abstract = "Introduction
We describe a prototype positron projection imager (PPI) for visualizing the whole-body biodistribution of positron-emitting compounds in mouse-size animals. The final version of the PPI will be integrated into the MONICA portable dual-gamma camera system to allow the user to interchangeably image either single photon or positron-emitting compounds in a shared software and hardware environment.
Methods
A mouse is placed in the mid-plane between two identical, opposed, pixelated LYSO arrays separated by 21.8-cm and in time coincidence. An image of the distribution of positron decays in the animal is formed on this mid-plane by coincidence events that fall within a small cone angle perpendicular to the two detectors and within a user-specified energy window. We measured the imaging performance of this device with phantoms and in tests performed in mice injected with various compounds labeled with positron-emitting isotopes.
Results
Representative performance measurements yielded the following results (energy window 250–650keV, cone angle 3.5°): resolution in the image mid-plane, 1.66-mm (FWHM), resolution ±1.5-cm above and below the image plane, 2.2-mm (FWHM), sensitivity: 0.237-cps/kBq (8.76-cps/μCi) 18F (0.024% absolute). Energy resolution was 15.9% with a linear-count-rate operating range of 0–14.8MBq (0–400μCi) and a corrected sensitivity variation across the field-of-view of <3%. Whole-body distributions of [18F] FDG and [18F] fluoride were well visualized in mice of typical size.
Conclusion
Performance measurements and field studies indicate that the PPI is well suited to whole-body positron projection imaging of mice. When integrated into the MONICA gamma camera system, the PPI may be particularly useful early in the drug development cycle where, like MONICA, basic whole-body biodistribution data can direct future development of the agent under study and where logistical factors (e.g., available imaging space, non-portability, and cost) may be limitations."
}
@article{BURKHARD201217,
title = "Mapping ecosystem service supply, demand and budgets",
journal = "Ecological Indicators",
volume = "21",
pages = "17 - 29",
year = "2012",
note = "Challenges of sustaining natural capital and ecosystem services",
issn = "1470-160X",
doi = "https://doi.org/10.1016/j.ecolind.2011.06.019",
url = "http://www.sciencedirect.com/science/article/pii/S1470160X11001907",
author = "Benjamin Burkhard and Franziska Kroll and Stoyan Nedkov and Felix Müller",
keywords = "Ecosystem functions, Ecological integrity, Ecosystem service footprint, CORINE land cover, Ecosystem service maps, GIS",
abstract = "Among the main effects of human activities on the environment are land use and resulting land cover changes. Such changes impact the capacity of ecosystems to provide goods and services to the human society. This supply of multiple goods and services by nature should match the demands of the society, if self-sustaining human–environmental systems and a sustainable utilization of natural capital are to be achieved. To describe respective states and dynamics, appropriate indicators and data for their quantification, including quantitative and qualitative assessments, are needed. By linking land cover information from, e.g. remote sensing, land survey and GIS with data from monitoring, statistics, modeling or interviews, ecosystem service supply and demand can be assessed and transferred to different spatial and temporal scales. The results reveal patterns of human activities over time and space as well as the capacities of different ecosystems to provide ecosystem services under changing land use. Also the locations of respective demands for these services can be determined. As maps are powerful tools, they hold high potentials for visualization of complex phenomena. We present an easy-to-apply concept based on a matrix linking spatially explicit biophysical landscape units to ecological integrity, ecosystem service supply and demand. An exemplary application for energy supply and demand in a central German case study region and respective maps for the years 1990 and 2007 are presented. Based on these data, the concept for an appropriate quantification and related spatial visualization of ecosystem service supply and demand is elaborated and discussed."
}
@article{CARASCO20101161,
title = "MCNP Output Data Analysis with ROOT (MODAR)",
journal = "Computer Physics Communications",
volume = "181",
number = "6",
pages = "1161 - 1166",
year = "2010",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2010.02.016",
url = "http://www.sciencedirect.com/science/article/pii/S001046551000055X",
author = "C. Carasco",
keywords = "ROOT, C++, Object-oriented programming, MCNP, Data processing",
abstract = "MCNP Output Data Analysis with ROOT (MODAR) is a tool based on CERN's ROOT software. MODAR has been designed to handle time–energy data issued by MCNP simulations of neutron inspection devices using the associated particle technique. MODAR exploits ROOT's Graphical User Interface and functionalities to visualize and process MCNP simulation results in a fast and user-friendly way. MODAR allows to take into account the detection system time resolution (which is not possible with MCNP) as well as detectors energy response function and counting statistics in a straightforward way.
Program summary
Program title: MODAR Catalogue identifier: AEGA_v1_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEGA_v1_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 155 373 No. of bytes in distributed program, including test data, etc.: 14 815 461 Distribution format: tar.gz Programming language: C++ Computer: Most Unix workstations and PC Operating system: Most Unix systems, Linux and windows, provided the ROOT package has been installed. Examples where tested under Suse Linux and Windows XP. RAM: Depends on the size of the MCNP output file. The example presented in the article, which involves three two-dimensional 139×740 bins histograms, allocates about 60 MB. These data are running under ROOT and include consumption by ROOT itself. Classification: 17.6 External routines: ROOT version 5.24.00 (http://root.cern.ch/drupal/) Nature of problem: The output of an MCNP simulation is an ASCII file. The data processing is usually performed by copying and pasting the relevant parts of the ASCII file into Microsoft Excel. Such an approach is satisfactory when the quantity of data is small but is not efficient when the size of the simulated data is large, for example when time–energy correlations are studied in detail such as in problems involving the associated particle technique. In addition, since the finite time resolution of the simulated detector cannot be modeled with MCNP, systems in which time–energy correlation is crucial cannot be described in a satisfactory way. Finally, realistic particle energy deposit in detectors is calculated with MCNP in a two-step process involving type-5 then type-8 tallies. In the first step, the photon flux energy spectrum associated to a time region is selected and serves as a source energy distribution for the second step. Thus, several files must be manipulated before getting the result, which can be time consuming if one needs to study several time regions or different detectors performances. In the same way, modeling counting statistics obtained in a limited acquisition time requires several steps and can also be time consuming. Solution method: In order to overcome the previous limitations, the MODAR C++ code has been written to make use of CERN's ROOT data analysis software. MCNP output data are read from the MCNP output file with dedicated routines. Two-dimensional histograms are filled and can be handled efficiently within the ROOT framework. To keep a user friendly analysis tool, all processing and data display can be done by means of ROOT Graphical User Interface. Specific routines have been written to include detectors finite time resolution and energy response function as well as counting statistics in a straightforward way. Additional comments: The possibility of adding tallies has also been incorporated in MODAR in order to describe systems in which the signal from several detectors can be summed. Moreover, MODAR can be adapted to handle other problems involving two-dimensional data. Running time: The CPU time needed to smear a two-dimensional histogram depends on the size of the histogram. In the presented example, the time–energy smearing of one of the 139×740 two-dimensional histograms takes 3 minutes with a DELL computer equipped with INTEL Core 2."
}
@article{TAGLIABUE2016119,
title = "Probabilistic behavioural modeling in building performance simulation—The Brescia eLUX lab",
journal = "Energy and Buildings",
volume = "128",
pages = "119 - 131",
year = "2016",
issn = "0378-7788",
doi = "https://doi.org/10.1016/j.enbuild.2016.06.083",
url = "http://www.sciencedirect.com/science/article/pii/S0378778816305722",
author = "Lavinia Chiara Tagliabue and Massimiliano Manfren and Angelo Luigi Camillo Ciribini and Enrico {De Angelis}",
keywords = "Probabilistic modeling, Behavioural modeling, Behavioural learning, Building performance simulation, Uncertainty propagation, Energy efficiency, Energy management",
abstract = "Occupant’s behavioural patterns determine a significant level of uncertainty in building energy performance evaluation. It is difficult to account for this uncertainty in the design phase when operational and occupancy profiles are unknown. The relevant “performance gap” usually encountered between simulated and measured energy performance is clearly connected to biased assumptions in modeling, especially in the initial design phase. A probabilistic modeling approach is proposed to improve simulation reliability and robustness with respect to variability in occupancy patterns. The case study presented is the eLUX lab of the “Smart Campus” of Brescia University in Italy. Occupancy dependent input parameters such as air change rates (i.e. mechanically controlled ventilation) and internal heat gains (i.e. due to people, lighting and appliances) are described by means of probability distributions to obtain probabilistic thermal demand and load profiles as output. Probabilistic results enables a more reliable identification of energy saving strategies (operational and environmental settings) with respect to highly variable operating conditions. Further, simulation data are processed to obtain a weather-adjusted energy demand visualization, suitable for establishing a continuity between modeling in design and operation phases, with calibration purpose. Calibrated energy models can be used for several specific tasks in the operation phase, in particular condition monitoring, fault detection and diagnosis, supervisory control and energy management. For the case study presented, a detailed data acquisition scheme has been designed to enable an effective monitoring activity in the operation phase, aimed at experimenting model-based approaches for the tasks reported. The proposed research is the point-of-departure for a general activity aimed at assessing critically the issues of reliability and robustness of simulation results obtained with conventional modeling approaches, in particular with respect to occupants’ behaviour, exploiting at the same time the possibility of using measured data as a direct feedback to promote behavioural change."
}
@article{KHAN2018178,
title = "Computer-aided drug design and virtual screening of targeted combinatorial libraries of mixed-ligand transition metal complexes of 2-butanone thiosemicarbazone",
journal = "Computational Biology and Chemistry",
volume = "75",
pages = "178 - 195",
year = "2018",
issn = "1476-9271",
doi = "https://doi.org/10.1016/j.compbiolchem.2018.05.008",
url = "http://www.sciencedirect.com/science/article/pii/S1476927117303134",
author = "Tahmeena Khan and Rumana Ahmad and Iqbal Azad and Saman Raza and Seema Joshi and Abdul Rahman Khan",
keywords = " Design, Docking, Drug interaction, Bioactivity prediction, Pharmacokinetics, Principal component analysis, Anticancer, Antioxidant",
abstract = "The present paper deals with in silico evaluation of 32 virtually designed transition metal complexes of 2-butanone thiosemicarbazone and N,S,O containing donor hetero-ligands namely py, bpy, furan, thiophene, 2-picoline, 1,10-phenanthroline, piperazine and liquid ammonia. The complexes were designed with a view to assess their potential anticancer, antioxidant and antibacterial activity. The absorption, distribution, metabolism, excretion and toxicity (ADMET) properties of the chosen ligands were calculated by admetSAR software. Metabolic sites of different ligands likely to undergo metabolism were predicted using Metaprint 2D. The proposed complexes were also evaluated for their drug-like quality based on Lipinski’s, Veber, Ghose and leadlikeness filters. Druglikeness and toxicity potential were predicted by OSIRIS property explorer. The pharmacokinetic properties and bioactivity scores were calculated by Molinspiration tool. Bioactivity scores of the complexes were predicted for drug targets including enzymes, nuclear receptors, kinase inhibitors, G-protein coupled receptor ligands and ion channel modulators. Molecular docking of selected Fe(II) mixed-ligand complexes was performed using AutoDock version 4.2.6 and i-GEMDOCK version 2.1 with two target proteins namely Ribonucleotide reductase (RR) and Topoisomerase II (Topo II). The results were compared with three standard reference drugs viz. Doxorubicin HCl, Letrozole (anticancer) and Tetracycline (antibiotic). Multivariate data obtained were analyzed using principal component analysis (PCA) for visualization and projection as scatter and 3D plots. Positive results obtained for hetero-ligands using admetSAR version 1.0 indicated good absorption and transport kinetics of the hetero-ligand complexes through the human intestine and blood-brain barrier. The hetero-ligands were predicted to have no associated mutagenic effect(s) also. However, none of the hetero-ligands was predicted to be Caco-2 (human colon cancer cell line) permeable. Most of the hetero-ligands and the parent ligand (2-butanone thiosemicarbazone) were predicted to undergo Phase-I metabolism prior to excretion using MetaPrint2D software. Pharmacokinetic evaluation of the proposed complexes revealed that all complexes displayed drug-like character and were predicted to have no apparent toxic side-effects. All the proposed complexes showed moderate to good biological activity scores (−5.0 to 0.0). Mixed complexes with bpy, 2-picoline and 1,10-phenanthroline showed significant bioactivity scores (as enzyme inhibitors) in the range 0.02–0.13. Likewise, good docking scores were obtained for Fe (II) complexes with the same ligands. [FeL(2-pic)2] displayed the lowest binding energy (−6.47 kcal/mol) with respect to Topo II followed by [FeL(py)2] (−6.14 kcal/mol) as calculated by AutoDock version 4.2.6. With respect to binding with RR, [FeL(2--pic)2] again displayed the lowest binding energy (−7.21 kcal/mol) followed by [FeL(py)2] (−5.96 kcal/mol). On the basis of docking predictions and various other computational evaluations, four mixed-ligand complexes of Fe in +2 oxidation state with py, bpy, 2--picoline and 1,10-phenanthroline were synthesized with 2-butanone thiosemicarbazone. All the synthesized Fe complexes were characterized using various spectroscopic techniques and tested for their potential anticancer activity in vitro against human breast carcinoma cell line MDA-MB 231 and human lung carcinoma cell line A549 cell line using MTT assay. [FeL(2-pic)2], [FeL(bpy)], and [FeL(py)2] were found to exhibit significant antiproliferative activity with IC50 values in the range of 80–100 μM against breast and lung cancer cells. The synthesized Fe complexes also displayed mild antioxidant activities. The synthesized and studied Fe complexes have the potential for development into future anticancer agents if analyzed and modified further for improvement of their ADMET, solubility and permeability criteria set for potential drug-candidates."
}
@article{GARCIADURANONA2016178,
title = "Life Cycle Assessment of a coniferous wood supply chain for pallet production in Catalonia, Spain",
journal = "Journal of Cleaner Production",
volume = "137",
pages = "178 - 188",
year = "2016",
issn = "0959-6526",
doi = "https://doi.org/10.1016/j.jclepro.2016.07.032",
url = "http://www.sciencedirect.com/science/article/pii/S0959652616309209",
author = "Lucía García-Durañona and Ramon Farreny and Pere Navarro and Jesús Boschmonart-Rives",
keywords = "Biogenic carbon, Climate change, Life Cycle Assessment (LCA), Pallet, Wood products, Wood value chain",
abstract = "In Catalonia, northwest of Spain, the forest and timber industry are an active part of the economy. About 63.5% of the territory is forested area (more than 2 million Ha), from which 60.5% is wooded land. The main activity of the timber industry is the manufacture of wood packaging using 85% of the sawnwood to manufacture pallets, which is the most common wood product manufactured in the region. In the case of Europe, around 20% of all sawnwood consumption is used for wooden pallets and packaging. This research analyzes the impact of the conifers wood supply chain for the sawnwood and pallet production, from forest operations to the gate of the production facility. The Life Cycle Assessment (LCA) methodology was applied to both products (sawnwood and pallet) using primary data provided by the industry in order to identify the most impactful points of the life cycle. Six midpoint impact categories using Recipe methodology were assessed: Climate change, Ozone depletion, Terrestrial acidification, Freshwater eutrophication, Human toxicity; Agricultural land occupation and Water depletion. Also, Cumulative Energy Demand was assessed. It was found that electricity consumption is the most impacting input in the sawnwood and pallet production, followed by pesticide used in the production. For pallet case, steel plays a key role. Possible improvements were identified with a big potential of impacts reduction. LCA results from this research can be applied for the assessment of other wood products and also to increase the knowledge and debate about the biogenic carbon sequestration of wooden products."
}
@article{CIFUENTE202089,
title = "The allosteric control mechanism of bacterial glycogen biosynthesis disclosed by cryoEM",
journal = "Current Research in Structural Biology",
volume = "2",
pages = "89 - 103",
year = "2020",
issn = "2665-928X",
doi = "https://doi.org/10.1016/j.crstbi.2020.04.005",
url = "http://www.sciencedirect.com/science/article/pii/S2665928X2030009X",
author = "Javier O. Cifuente and Natalia Comino and Cecilia D'Angelo and Alberto Marina and David Gil-Carton and David Albesa-Jové and Marcelo E. Guerin",
keywords = "Glycogen biosynthesis, Glycogen regulation, Nucleotide sugar biosynthesis, Enzyme allosterism",
abstract = "Glycogen and starch are the major carbon and energy reserve polysaccharides in nature, providing living organisms with a survival advantage. The evolution of the enzymatic machinery responsible for the biosynthesis and degradation of such polysaccharides, led the development of mechanisms to control the assembly and disassembly rate, to store and recover glucose according to cell energy demands. The tetrameric enzyme ADP-glucose pyrophosphorylase (AGPase) catalyzes and regulates the initial step in the biosynthesis of both α-polyglucans. AGPase displays cooperativity and allosteric regulation by sensing metabolites from the cell energy flux. The understanding of the allosteric signal transduction mechanisms in AGPase arises as a long-standing challenge. In this work, we disclose the cryoEM structures of the paradigmatic homotetrameric AGPase from Escherichia coli (EcAGPase), in complex with either positive or negative physiological allosteric regulators, fructose-1,6-bisphosphate (FBP) and AMP respectively, both at 3.0 Å resolution. Strikingly, the structures reveal that FBP binds deeply into the allosteric cleft and overlaps the AMP site. As a consequence, FBP promotes a concerted conformational switch of a regulatory loop, RL2, from a “locked” to a “free” state, modulating ATP binding and activating the enzyme. This notion is strongly supported by our complementary biophysical and bioinformatics evidence, and a careful analysis of vast enzyme kinetics data on single-point mutants of EcAGPase. The cryoEM structures uncover the residue interaction networks (RIN) between the allosteric and the catalytic components of the enzyme, providing unique details on how the signaling information is transmitted across the tetramer, from which cooperativity emerges. Altogether, the conformational states visualized by cryoEM reveal the regulatory mechanism of EcAGPase, laying the foundations to understand the allosteric control of bacterial glycogen biosynthesis at the molecular level of detail."
}
@article{SOLANOALTAMIRANO2015362,
title = "DensToolKit: A comprehensive open-source package for analyzing the electron density and its derivative scalar and vector fields",
journal = "Computer Physics Communications",
volume = "196",
pages = "362 - 371",
year = "2015",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2015.07.005",
url = "http://www.sciencedirect.com/science/article/pii/S001046551500274X",
author = "J.M. Solano-Altamirano and Julio M. Hernández-Pérez",
keywords = "Electron density, Quantum Theory of Atoms in Molecules, Spatial grids, Momentum space, Reactivity indices, Reduced density matrix of order one",
abstract = "DensToolKit is a suite of cross-platform, optionally parallelized, programs for analyzing the molecular electron density (ρ) and several fields derived from it. Scalar and vector fields, such as the gradient of the electron density (∇ρ), electron localization function (ELF) and its gradient, localized orbital locator (LOL), region of slow electrons (RoSE), reduced density gradient, localized electrons detector (LED), information entropy, molecular electrostatic potential, kinetic energy densities K and G, among others, can be evaluated on zero, one, two, and three dimensional grids. The suite includes a program for searching critical points and bond paths of the electron density, under the framework of Quantum Theory of Atoms in Molecules. DensToolKit also evaluates the momentum space electron density on spatial grids, and the reduced density matrix of order one along lines joining two arbitrary atoms of a molecule. The source code is distributed under the GNU-GPLv3 license, and we release the code with the intent of establishing an open-source collaborative project. The style of DensToolKit’s code follows some of the guidelines of an object-oriented program. This allows us to supply the user with a simple manner for easily implement new scalar or vector fields, provided they are derived from any of the fields already implemented in the code. In this paper, we present some of the most salient features of the programs contained in the suite, some examples of how to run them, and the mathematical definitions of the implemented fields along with hints of how we optimized their evaluation. We benchmarked our suite against both a freely-available program and a commercial package. Speed-ups of ∼2×, and up to 12× were obtained using a non-parallel compilation of DensToolKit for the evaluation of fields. DensToolKit takes similar times for finding critical points, compared to a commercial package. Finally, we present some perspectives for the future development and growth of the suite.
Program summary
Program title: DensToolKit Catalogue identifier: AEXI_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEXI_v1_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: GNU, General Public License, version 3 No. of lines in distributed program, including test data, etc.: 142037 No. of bytes in distributed program, including test data, etc.: 5517409 Distribution format: tar.gz Programming language: C++, bash. Computer: Any. Operating system: Linux, MacOSX, Windows (cygwin). RAM: The memory requirements grow quadratically with the number of primitives describing the wavefunction. A wavefunction with 1,500 primitives uses ∼17MB, and 2GB RAM are enough to process wavefunctions of around 10,000 primitives. A few more MB may be needed by some of the most demanding programs of the package if the number of primitives is large. Classification: 6.5, 7.3, 16, 16.1. External routines: (optional) gnuplot, povray, epstool, Graphics-Magick, epstopdf Nature of problem: The study of the electron density of molecules, some reactivity indices, and the topology of the electron density can be used to analyze the chemical nature, stability and reactivity of those molecules. Furthermore, the study of the electron density and functionals of it may help us in gaining a better understanding of the chemical bond. Reactivity indices and the molecular topological properties may also aid in molecular design. Solution method: The suite provides several programs in order to compute scalar and vector fields derivatives of the electron density. Those fields are obtained from a wavefunction file, which is in turn obtained from programs such as Nwchem, MolPro, etc. The functions, whereby the fields are computed, are implemented following mathematically standard but computationally optimized and parallelized algorithms built upon the Density Matrix. The suite provides several small but efficient programs, easily scriptable, for evaluation of the fields upon spatial grids. Regarding the topology analysis, the suite uses the algorithm proposed by Popelier, which uses the eigen-values of the Hessian of the electron density for locating the critical points. Bond paths are traced using a fifth-order Runge–Kutta–Dormand–Prince algorithm. Optional visualization of the produced data can be carried out by scripts generated by the suite, which can be parsed later to gnuplot, or povray. In addition, DensToolKit provides an open door for the user to program new scalar or vector fields, with almost complete functionality for evaluating such fields upon the same spatial grids as those implemented for the fields already provided in the suite. Restrictions: Wavefunctions with more than 99 nuclei must be input in wfx format. In the current version, wavefunctions that use pseudopotentials must have only one Additional Electron Density Function (EDF) entry in the wfx file, which in this case is the only accepted input format, and pseudopotential support is provided only in non-parallel compilation. Additional comments: A simple method for implementing new indices (derived from any of the implemented fields) is provided. In this manner, the final user may easily program his/her own scalar or vector field with a few code lines. Running time: Strongly dependent on the number of primitives used for approximating the wavefunction (∼N2p). It also depends on the evaluated number of points and type of field. Wavefunctions comprised of 1,500 primitives may take several hours to complete, while small molecules described by two or three hundred primitives take a few seconds. Typical running times are at least as fast as the times taken by some commercial or freely available codes. In many cases, the programs perform the computations with a speed-up of 2× (with respect to other available programs), and in some cases 10× speed-ups or more can be attained."
}
@article{LI201991,
title = "Integration of Building Information Modeling and Web Service Application Programming Interface for assessing building surroundings in early design stages",
journal = "Building and Environment",
volume = "153",
pages = "91 - 100",
year = "2019",
issn = "0360-1323",
doi = "https://doi.org/10.1016/j.buildenv.2019.02.024",
url = "http://www.sciencedirect.com/science/article/pii/S0360132319301271",
author = "Jingming Li and Nianping Li and Kereshmeh Afsari and Jinqing Peng and Zhibin Wu and Haijiao Cui",
keywords = "Building surroundings, Site assessment, Data visualization, Dynamo BIM, Web service APIs",
abstract = "Many studies and green building rating systems have addressed the social and environmental importance of site planning. Tools based on BIM and Location Based Services (LBSs) have been developed to estimate energy consumption for material transportation and the surrounding density of the sites. However, the tools are not programmable and limited by their serving phases. This requires solutions that have the flexibility to run site analysis on social surroundings and the compatibility of user programming in the early design stage. Integrating visual programming and web service Application Programming Interface (API) can fulfill the requirements of evaluating publicly available diverse uses of sites and custom coding. This study introduces the method for integrating Dynamo BIM and Amap web service APIs for the evaluations of publicly available diverse uses and transportations. Additionally, implementations of use cases are demonstrated including assessments of Access to Quality Transit and Diverse Uses in LEED v4. Results from the integrated tool are analyzed and validated with survey results. The analysis of results indicates that the integration method introduced in this paper is effective. The limitations, potentials, and future developments are also discussed. The integration of Dynamo BIM and web service APIs might be useful for site assessments in the early design stage or even earlier."
}
@article{ZAKHNINI2013183,
title = "Monte Carlo simulations of GeoPET experiments: 3D images of tracer distributions (18F, 124I and 58Co) in Opalinus clay, anhydrite and quartz",
journal = "Computers & Geosciences",
volume = "57",
pages = "183 - 196",
year = "2013",
issn = "0098-3004",
doi = "https://doi.org/10.1016/j.cageo.2013.03.023",
url = "http://www.sciencedirect.com/science/article/pii/S0098300413000988",
author = "Abdelhamid Zakhnini and Johannes Kulenkampff and Sophie Sauerzapf and Uwe Pietrzyk and Johanna Lippmann-Pipke",
keywords = "Monte Carlo simulation, PET, Dense material, Radiotracer, Process monitoring",
abstract = "Understanding conservative fluid flow and reactive tracer transport in soils and rock formations requires quantitative transport visualization methods in 3D+t. After a decade of research and development we established the GeoPET as a non-destructive method with unrivalled sensitivity and selectivity, with due spatial and temporal resolution by applying Positron Emission Tomography (PET), a nuclear medicine imaging method, to dense rock material. Requirements for reaching the physical limit of image resolution of nearly 1mm are (a) a high-resolution PET-camera, like our ClearPET scanner (Raytest), and (b) appropriate correction methods for scatter and attenuation of 511keV—photons in the dense geological material. The latter are by far more significant in dense geological material than in human and small animal body tissue (water). Here we present data from Monte Carlo simulations (MCS) reflecting selected GeoPET experiments. The MCS consider all involved nuclear physical processes of the measurement with the ClearPET-system and allow us to quantify the sensitivity of the method and the scatter fractions in geological media as function of material (quartz, Opalinus clay and anhydrite compared to water), PET isotope (18F, 58Co and 124I), and geometric system parameters. The synthetic data sets obtained by MCS are the basis for detailed performance assessment studies allowing for image quality improvements. A scatter correction method is applied exemplarily by subtracting projections of simulated scattered coincidences from experimental data sets prior to image reconstruction with an iterative reconstruction process."
}
@article{CANDANEDO2018250,
title = "Reconstruction of the indoor temperature dataset of a house using data driven models for performance evaluation",
journal = "Building and Environment",
volume = "138",
pages = "250 - 261",
year = "2018",
issn = "0360-1323",
doi = "https://doi.org/10.1016/j.buildenv.2018.04.035",
url = "http://www.sciencedirect.com/science/article/pii/S0360132318302476",
author = "Luis M. Candanedo and Véronique Feldheim and Dominique Deramaix",
keywords = "Learning curves, Multiple linear regression, Random forest, Passive house, Temperatures, Sample size",
abstract = "Whenever the long term monitoring of a building is attempted it is likely that specific sensors or the whole monitoring system used may experience long-term failure therefore creating important gaps in one or more variables of special interest. These long gaps may not be addressed using simple linear interpolation. The option of only using the available data for descriptive statistics would produce results that are biased towards the season of measurement. In addition discarding the incomplete data represents a significant waste of time and effort in the research study. A work around to reduce the bias problem is to predict the missing data from other measured variables using machine-learning techniques. Some questions that follow are: How much data is necessary to be able to train a regression model? What is the expected error of such prediction? What is the best model for such a task? This paper addresses the problem of completing a data set for the interior temperatures inside a passive house using different monitored predictors such as exterior temperature, humidity, wind speed, visibility, pressure and electrical energy use inside the building. Two regression models, multiple linear regression and random forest are compared using learning curves for the training and testing sets for visualizing the so-called bias-variance trade off. The learning curves help to answer the question of optimal sample size for training, model selection and expected error. Finally, descriptive statistics such as median, maximum, minimum, and room temperature averages are presented before and after completing the data sets."
}
@article{DAEICHIN20162017,
title = "Frequency Analysis of the Photoacoustic Signal Generated by Coronary Atherosclerotic Plaque",
journal = "Ultrasound in Medicine & Biology",
volume = "42",
number = "8",
pages = "2017 - 2025",
year = "2016",
issn = "0301-5629",
doi = "https://doi.org/10.1016/j.ultrasmedbio.2016.03.015",
url = "http://www.sciencedirect.com/science/article/pii/S030156291600154X",
author = "Verya Daeichin and Min Wu and Nico {De Jong} and Antonius F.W. {van der Steen} and Gijs {van Soest}",
keywords = "Photoacoustic, Broadband receiver, Acoustic spectra, Frequency content, Hydrophone, Atherosclerosis, Lipid",
abstract = "The identification of unstable atherosclerotic plaques in the coronary arteries is emerging as an important tool for guiding percutaneous coronary interventions and may enable preventive treatment of such plaques in the future. Assessment of plaque stability requires imaging of both structure and composition. Spectroscopic photoacoustic (sPA) imaging can visualize atherosclerotic plaque composition on the basis of the optical absorption contrast. It is an established fact that the frequency content of the photoacoustic (PA) signal is correlated with structural tissue properties. As PA signals can be weak, it is important to match the transducer bandwidth to the signal frequency content for in vivo imaging. In this ex vivo study on human coronary arteries, we combined sPA imaging and analysis of frequency content of the PA signals. Using a broadband transducer (−3-dB one-way bandwidth of 10–35 MHz) and a 1-mm needle hydrophone (calibrated for 1–20 MHz), we covered a large frequency range of 1–35 MHz for receiving the PA signals. Spectroscopic PA imaging was performed at wavelengths ranging from 1125 to 1275 nm with a step of 2 nm, allowing discrimination between plaque lipids and adventitial tissue. Under sPA imaging guidance, the frequency content of the PA signals from the plaque lipids was quantified. Our data indicate that more than 80% of the PA energy of the coronary plaque lipids lies in the frequency band below 8 MHz. This frequency information can guide the choice of the transducer element used for PA catheter fabrication."
}
@article{FAIK2018117,
title = "The equation of state package FEOS for high energy density matter",
journal = "Computer Physics Communications",
volume = "227",
pages = "117 - 125",
year = "2018",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2018.01.008",
url = "http://www.sciencedirect.com/science/article/pii/S0010465518300122",
author = "Steffen Faik and Anna Tauschwitz and Igor Iosilevskiy",
keywords = "Equation of state, QEOS model, MPQeos code, Thomas–Fermi eos, Liquid–vapor two-phase region, Homogeneous mixtures of elements",
abstract = "Adequate equation of state (EOS) data is of high interest in the growing field of high energy density physics and especially essential for hydrodynamic simulation codes. The semi-analytical method used in the newly developed Frankfurt equation of state (FEOS) package provides an easy and fast access to the EOS of – in principle – arbitrary materials. The code is based on the well known QEOS model (More et al., 1988; Young and Corey, 1995) and is a further development of the MPQeos code (Kemp and Meyer-ter Vehn, 1988; Kemp and Meyer-ter Vehn, 1998) from Max-Planck-Institut für Quantenoptik (MPQ) in Garching Germany. The list of features contains the calculation of homogeneous mixtures of chemical elements and the description of the liquid–vapor two-phase region with or without a Maxwell construction. Full flexibility of the package is assured by its structure: A program library provides the EOS with an interface designed for Fortran or C/C++ codes. Two additional software tools allow for the generation of EOS tables in different file output formats and for the calculation and visualization of isolines and Hugoniot shock adiabats. As an example the EOS of fused silica (SiO2) is calculated and compared to experimental data and other EOS codes.
Program summary
Program Title: FEOS — Frankfurt equation of state Program Files doi: http://dx.doi.org/10.17632/6vjsv6v48p.1 Licensing provisions: GNU General Public License version 3 Programming language: C++ Supplementary material: Documentation/manual, exemplary input files for aluminum (Al) and fused silica (SiO2) Nature of problem: The description of a thermodynamic system — e.g. by the solution of the hydrodynamic conservation equations — presumes in the most cases reliable equation of state (EOS) data for different materials and for a wide range in temperature–density space. The FEOS code provides the required thermodynamic quantities like the pressure or the specific internal energy per unit mass as functions of density and temperature for, in principle, arbitrary materials, for single elements as well as for homogeneous mixtures of elements. FEOS is based on the well known QEOS model [2] and is a further development of the MPQeos code [1] from Max-Planck-Institut für Quantenoptik in Garching. Since the model was designed for the high energy density matter regimes, it can be applied e.g. to high-power laser or ion beam and inertial fusion science applications. The most important advantage of the FEOS package is an easy and fast access to materials which may not be available by more complex EOS codes. Solution method: In the QEOS model the thermodynamic quantities are derived from the specific Helmholtz free energy f=ϵ−Ts, which is composed of three contributions f=fe+fi+fb: (1) the uncorrected electronic part fe, calculated by a numerical scheme based on the simple Thomas–Fermi statistical model, (2) the ionic part fi, using the Cowan model [2], which employs analytical formulas to smoothly interpolate between the Debye solid, the normal solid and the liquid states, and (3) the semi-empirical bonding correction fb, which is included to compensate for the negligence of bonding forces in the simple Thomas–Fermi model. Although the total EOS is calculated for a single temperature T=Te=Ti, the user is free to calculate the ionic and the corrected electronic contributions independently with different temperatures. For homogeneous mixtures of elements the partial volumes of all element species k are iteratively adjusted in order to equilibrate the Thomas–Fermi pressures pe,k and to fulfill an additive volume rule for the electronic contribution. Furthermore, in the liquid–vapor two-phase region the model provides the (metastable) EOS with its characteristic features like van-der-Waals loops. The fully equilibrium EOS inside the two-phase region can be calculated by an iterative Maxwell construction scheme. Finally, despite the existence of the bonding correction, pressures near the critical point are often overestimated. Therefore, an improved cold curve can be applied to fit the location of the critical point to theoretical or experimental data. Additional comments: Besides the material’s composition, the user must specify a reference density ρo and the bulk modulus Ko=ρ∂p∕∂ρs at a reference point (p,T)=(0,To) for a new material. The reference temperature To is usually chosen such that at p=0 the studied material is in the solid state. All fixed material parameters are stored in a material parameter database file which can be easily exchanged between users. The code is designed to calculate the equation of state within the following density and temperature limits: 10−7≤ρ∕ρo≤106, 10−4eV≤T≤106eV. Homogeneous mixtures with more than three elements may implicate numerical difficulties and/or uneconomical computing times. For temperatures close to and below the critical point one must be careful to check the accuracy of the model. If available, a more complex EOS in this regime is preferable. The FEOS package was designed to provide the best possible flexibility and therefore consists of three parts: (1) the FEOS library which contains all the routines for the calculation of the EOS and which provides a C/C++ as well as a Fortran interface for this purpose, (2) the FEOS table generation tool which accesses the FEOS library in order to generate EOS table files (e.g. in the SESAME database [3] format), and (3) the SHOWEOS table visualization tool which was developed to provide isotherms, isochores, isentropes, and Hugoniot curves from the FEOS or SESAME tables. Application of the code was first demonstrated in a publication on liquid–vapor metastable states in volumetrically heated matter [4]. [1] A. J. Kemp, J. Meyer-ter Vehn, An equation of state code for hot dense matter, based on the QEOS description, Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment 415 (3) (1998) 674–676. doi:10.1016/S0168-9002(98)00446-X. [2] R. M. More, K. H. Warren, D. A. Young, G. B. Zimmerman, A new quotidian equation of state (QEOS) for hot dense matter, Physics of Fluids 31 (1988) 3059. doi:10.1063/1.866963. [3] S. P. Lyon, J. D. Johnson, SESAME: The Los Alamos National Laboratory equation of state database, Tech. Rep. LA-UR-92-3407, Los Alamos National Laboratory (1992). [4] S. Faik, M. M. Basko, A. Tauschwitz, I. Iosilevskiy, J. A. Maruhn, Dynamics of volumetrically heated matter passing through the liquid–vapor metastable states, High Energy Density Physics 8 (4) (2012) 349–359. doi:10.1016/j.hedp.2012.08.003."
}
@article{JIANG2019105,
title = "Microvascular networks in the area of the auditory peripheral nervous system",
journal = "Hearing Research",
volume = "371",
pages = "105 - 116",
year = "2019",
issn = "0378-5955",
doi = "https://doi.org/10.1016/j.heares.2018.11.012",
url = "http://www.sciencedirect.com/science/article/pii/S0378595518303988",
author = "Han Jiang and Xiaohan Wang and Jinhui Zhang and Allan Kachelmeier and Ivan A. Lopez and Xiaorui Shi",
keywords = "Mouse cochlea, Tissue clearing, Vasculature, Spiral limbus, Spiral ganglion neuron",
abstract = "Using transgenic fluorescent reporter mice in combination with an established tissue clearing method, we detail heretofore optically opaque regions of the spiral lamina and spiral limbus where the auditory peripheral nervous system is located and provide insight into changes in cochlear vascular density with ageing. We found a relatively dense and branched vascular network in young adults, but a less dense and thinned network in aged adults. Significant reduction in vascular density starts early at the age of 180 days in the region of the spiral limbus (SL) and continues into old age at 540 days. Loss of vascular volume in the region of spiral ganglion neurons (SGN) is delayed until the age of 540 days. In addition, we observed that two vascular accessory cells are closely associated with the microvascular system: perivascular resident macrophages and pericytes. Morphologically, perivascular resident macrophages undergo drastic changes from postnatal P7 to young adult (P30). In postnatal animals, most perivascular resident macrophages exhibit a spherical or nodular shape. In young adult mice, the majority of perivascular resident macrophages are elongated and display an orientation parallel to the vessels. In our imaging, some of the perivascular resident macrophages are caught in the act of transmigrating from the blood circulation. Pericytes also display morphological heterogeneity. In the P7 mice, pericytes are prominent on the capillary walls, relatively large and punctate, and less uniform. In contrast, pericytes in the P30 mice are relatively flat and uniform, and less densely distributed on the vascular network. With triple fluorescence labeling, we did not find obvious physical connection between the two systems, unlike neuronal-vascular coupling found in brain. However, using a fluorescent (FITC-conjugated dextran) tracer and the enzymatic tracer horseradish peroxidase (HRP), we observed robust neurovascular exchange, likely through transcytotic transport, evidenced by multiple vesicles present in the endothelial cells. Taken together, our data demonstrate the effectiveness of tissue-clearing methods as an aid in imaging the vascular architecture of the SL and SGNs in whole mounted mouse cochlear preparations. Structure is indicative of function. The finding of differences in vascular structure in postnatal and young adult mice may correspond with variation in hearing refinement after birth and indicate the status of functional activity. The decrease in capillary network density in the older animals may reflect the decreased energy demand from peripheral neural activity. The finding of active transcytotic transport from blood to neurons opens a potential therapeutic avenue for delivery of various growth factors and gene vectors into the inner ear to target SGNs."
}
@article{WU20192037,
title = "Define, process and describe the intersectoral embedded carbon flow network in China",
journal = "MethodsX",
volume = "6",
pages = "2037 - 2045",
year = "2019",
issn = "2215-0161",
doi = "https://doi.org/10.1016/j.mex.2019.08.003",
url = "http://www.sciencedirect.com/science/article/pii/S2215016119302055",
author = "Kaiyao Wu and Tinggan Yang and Xiaoyan Wei",
keywords = "Input–output, Carbon intensity, Life cycle assessment, Social network analysis",
abstract = "This article focuses on defining the intersectoral embedded carbon flow network as a matrix to mimic the complex economic-energy-environment symbiotic system in China. We propose a set of synthetical methodologies, which combines life cycle assessment (LCA) and social network analysis (SNA) in the input–output framework. The nodes and relations between nodes in the network are delicately designed such that these relations, which represent the carbon intensity of total intersectoral input between sectors, can be comparable among sectors and over time. Subsequently, based on longitudinal data of input–output tables in China, we derive, sequentialize and dichotomize matrices in order to apply the SNA method to describe the evolution of the intersectoral embedded carbon flow network. The SNA methods used include network visualization, triad census, cohesion metrics, position metrics and core–periphery modeling. Our synthetical methodologies provide a potential systematic solution to carbon reduction in China and help policy makers determine policy priorities rationally. •By constructing an intersectoral embedded carbon flow network matrix, we provide an easily explicable map to aid in the investigation and research in human derived CO2 emissions embedded in the network.•By describing the longitudinal network matrices with SNA, the evolution of the complex economic-energy-environment symbiotic system in China can be mapped out, such as an example illustrated in Wu et al. [1]."
}
@article{GIORGINO20141109,
title = "PLUMED-GUI: An environment for the interactive development of molecular dynamics analysis and biasing scripts",
journal = "Computer Physics Communications",
volume = "185",
number = "3",
pages = "1109 - 1114",
year = "2014",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2013.11.019",
url = "http://www.sciencedirect.com/science/article/pii/S0010465513004128",
author = "Toni Giorgino",
keywords = "Graphical user interface, VMD, PLUMED, Molecular dynamics, Collective variables, Metadynamics",
abstract = "PLUMED-GUI is an interactive environment to develop and test complex PLUMED scripts within the Visual Molecular Dynamics (VMD) environment. Computational biophysicists can take advantage of both PLUMED’s rich syntax to define collective variables (CVs) and VMD’s chemically-aware atom selection language, while working within a natural point-and-click interface. Pre-defined templates and syntax mnemonics facilitate the definition of well-known reaction coordinates. Complex CVs, e.g. involving reference snapshots used for RMSD or native contacts calculations, can be built through dialogs that provide a synoptic view of the available options. Scripts can be either exported for use in simulation programs, or evaluated on the currently loaded molecular trajectories. Script development takes place without leaving VMD, thus enabling an incremental try–see–modify development model for molecular metrics.
Program summary
Program title: PLUMED-GUI (Collective variable analysis plugin) Catalogue identifier: AERU_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AERU_v1_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: 3-clause BSD Open Source No. of lines in distributed program, including test data, etc.: 2651 No. of bytes in distributed program, including test data, etc.: 32359 Distribution format: tar.gz Programming language: TCL/TK. Computer: Workstations, PCs. Operating system: Linux/Unix, OSX, Windows. RAM: Sufficient to run PLUMED [1] and VMD [2]. Classification: 3, 23. Subprograms used:Cat IdTitleReferenceAEEE_v2_0PLUMEDCPC 185 (2014) 604Nature of problem: Compute and visualize values of collective variables on molecular dynamics trajectories from within VMD, and interactively develop biasing scripts for the estimation of free-energy surfaces in PLUMED. Solution method: A graphical user interface is integrated in VMD and allows the user to interactively develop and run analysis scripts. Menus and dialogs provide mnemonics and documentation on the syntax to define complex CVs. Restrictions: Tested on systems up to 100,000 atoms. Unusual features: VMD–PLUMED is not a standalone program but a plugin that provides access to PLUMED’s analysis features from within VMD. Additional comments: Distributed with VMD since version 1.9.0. Manual update may be required to access the latest features. Running time: Computations of the values of collective variables, performed by the underlying PLUMED code, depends on the size of the system and the length of the trajectory; it is generally negligible with respect to simulation time. References:[1]G. A. Tribello, M. Bonomi, D. Branduardi, C. Camilloni, G. Bussi, PLUMED 2: New feathers for an old bird, Computer Physics Communications 185 (2014) 604.[2]W. Humphrey, A. Dalke, K. Schulten, VMD: visual molecular dynamics, J Mol Graph 14 (1996) 33–38."
}
@article{POTLA2020128070,
title = "An analysis of spectroscopic, computational and biological activity studies of L-shaped sulfamoylbenzoic acid derivatives: A third order nonlinear optical material",
journal = "Journal of Molecular Structure",
volume = "1210",
pages = "128070",
year = "2020",
issn = "0022-2860",
doi = "https://doi.org/10.1016/j.molstruc.2020.128070",
url = "http://www.sciencedirect.com/science/article/pii/S0022286020303951",
author = "Krishna Murthy Potla and Nuthalapati Poojith and Francisco A.P. Osório and Clodoaldo Valverde and Sampath Chinnam and P.A. Suchetan and Suneetha Vankayalapati",
keywords = "NLO properties, Reactivity analysis, SCXRD studies, Biological activity, BDE calculations, Sulfamoylbenzoic acid",
abstract = "The current article focuses mainly on investigation of structural, reactivity, topology studies, and third order nonlinear optical properties of the synthesized 2-(benzylamino)-4-chloro-5-sulfamoylbenzoic acid (BACSBA) with the aid of spectroscopic techniques and computational methods. The structure in the solid state was obtained unambiguously by SCXRD study that revealed that BACSBA has L-shaped structure stabilized by N–H⋯O intramolecular hydrogen bond. Further, the two dimensional sheet like architecture formation by linking of molecules via O–H⋯O and N–H⋯O hydrogen bonds, were visualized both qualitatively and quantitatively by Hirshfeld surface analysis. Also, a topological analysis made through Quantum Theory of Atoms In Molecules (QTAIM) highlights the observations N–H⋯O bonds on solid state. The quantum chemical calculation was performed at DFT/6–311++G (d,p) level of basis set. The analysis of each vibrational wave number was performed with the help of potential energy distribution (PED) using VEDA4 software and correlation with experimental data shows good concurrence. The reactive sites have been predicted and visualized by molecular electrostatic surface potential (MESP) and Fukui function calculation, together with hydrogen bond dissociation energy (H-BDE) for the BACSBA compound. Frontier molecular orbitals (FMO), global reactivity parameters, natural bond orbital analysis (NBO), localized orbital locator (LOL) and electron localization function (ELF) properties have also been studied for the titled compound. The super molecule (SM) approach with 372,680 atoms at the DFT/CAM-B3LYP/6–311++G (d,p) level was used for calculating the nonlinear optical properties of the crystal. The electrical parameters such as total dipole moment, average linear polarizability and average second IDRI hyperpolarizability were calculated. In addition, the linear refractive index and the nonlinear third order macroscopic susceptibility of the crystal was estimated as a function of the frequency of the applied electric field. The value of third order nonlinear susceptibility for the BACSBA crystal at 532 nm was found to be 45.57 times greatest than the experimentally measured result of organic crystal (2E)-1-(3-bromophenyl)-3-[4 (methylsulfanyl)phenyl]prop-2-en-1-one (3Br4MSP) demonstrating that BACSBA crystal could be a good potential candidate for nonlinear optical applications. In addition, the thermal stability was studied showing that the crystal as potential optical devices at temperature up to 234 °C. Furthermore, preliminary studies revealed that BACSBA compound displayed promising antifungal and antioxidant activities compared with the standard drugs."
}
@article{SMISTAD20151,
title = "Medical image segmentation on GPUs – A comprehensive review",
journal = "Medical Image Analysis",
volume = "20",
number = "1",
pages = "1 - 18",
year = "2015",
issn = "1361-8415",
doi = "https://doi.org/10.1016/j.media.2014.10.012",
url = "http://www.sciencedirect.com/science/article/pii/S1361841514001819",
author = "Erik Smistad and Thomas L. Falch and Mohammadmehdi Bozorgi and Anne C. Elster and Frank Lindseth",
keywords = "Medical, Image, Segmentation, GPU, Parallel",
abstract = "Segmentation of anatomical structures, from modalities like computed tomography (CT), magnetic resonance imaging (MRI) and ultrasound, is a key enabling technology for medical applications such as diagnostics, planning and guidance. More efficient implementations are necessary, as most segmentation methods are computationally expensive, and the amount of medical imaging data is growing. The increased programmability of graphic processing units (GPUs) in recent years have enabled their use in several areas. GPUs can solve large data parallel problems at a higher speed than the traditional CPU, while being more affordable and energy efficient than distributed systems. Furthermore, using a GPU enables concurrent visualization and interactive segmentation, where the user can help the algorithm to achieve a satisfactory result. This review investigates the use of GPUs to accelerate medical image segmentation methods. A set of criteria for efficient use of GPUs are defined and each segmentation method is rated accordingly. In addition, references to relevant GPU implementations and insight into GPU optimization are provided and discussed. The review concludes that most segmentation methods may benefit from GPU processing due to the methods’ data parallel structure and high thread count. However, factors such as synchronization, branch divergence and memory usage can limit the speedup."
}
@article{XIONG2020107013,
title = "Efficient learning of personalized visual preferences in daylit offices: An online elicitation framework",
journal = "Building and Environment",
volume = "181",
pages = "107013",
year = "2020",
issn = "0360-1323",
doi = "https://doi.org/10.1016/j.buildenv.2020.107013",
url = "http://www.sciencedirect.com/science/article/pii/S0360132320303930",
author = "Jie Xiong and Nimish M. Awalgaonkar and Athanasios Tzempelikos and Ilias Bilionis and Panagiota Karava",
keywords = "Visual preferences, Online learning, Elicitation framework, Daylighting, Personalized control",
abstract = "Human preference-based control in buildings may achieve maximum occupant satisfaction as well as energy savings. Adaptive and online learning methods are needed for learning human preferences, using the minimum amount of data possible with quantified uncertainty. This paper presents an online visual preference elicitation learning framework, developed for efficiently learning occupants' visual preference profiles and hidden satisfaction utility functions in daylit offices. A combination of Thompson sampling and pure exploration (uncertainty learning) methods was used in the sequential elicitation framework, to determine the set of successive visual preference queries (for visual conditions) with most information gain. In this way, a balance between exploration and exploitation is realized and the area around the satisfaction utility maximum is learned with minimum number of steps. Experiments with human subjects were conducted in private sidelit offices to demonstrate the feasibility and performance of the learning framework. A single-variable model (vertical illuminance) was used to demonstrate the method and visualize results. The entropy of the distribution of the most preferred visual condition is computed for each learned preference profile to quantify the certainty and evaluate the learning efficiency. Our method learns most individual visual preferences to an acceptable certainty level within one day, and indicates the need for personalization. Finally, we discuss the integration of visual preferences into control applications. A switching algorithm is proposed, shifting iteratively between the learning and control modes depending on the certainty of the learned preference model. This work contributes to developing comprehensive online learning methods towards preference-based tuned indoor environments."
}
@article{SCHUETRUMPF2018211,
title = "The TDHF code Sky3D version 1.1",
journal = "Computer Physics Communications",
volume = "229",
pages = "211 - 213",
year = "2018",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2018.03.012",
url = "http://www.sciencedirect.com/science/article/pii/S0010465518300845",
author = "B. Schuetrumpf and P.-G. Reinhard and P.D. Stevenson and A.S. Umar and J.A. Maruhn",
keywords = "Hartree–Fock, BCS, Density-functional theory, Skyrme energy functional, Giant resonances, Heavy-ion collisions",
abstract = "The nuclear mean-field model based on Skyrme forces or related density functionals has found widespread application to the description of nuclear ground states, collective vibrational excitations, and heavy-ion collisions. The code Sky3D solves the static or dynamic equations on a three-dimensional Cartesian mesh with isolated or periodic boundary conditions and no further symmetry assumptions. Pairing can be included in the BCS approximation for the static case. The code is implemented with a view to allow easy modifications for including additional physics or special analysis of the results.
New version program summary
Program title: Sky3D Program Files doi: http://dx.doi.org/10.17632/vzbrzvyrn4.1 Licensing provisions: GPLv3 Programming language: Fortran 90. The OpenMP version requires a relatively recent compiler; it was found to work using gfortran 4.6.2 or later and the Intel compiler version 12 or later. Journal reference of previous version: J. A. Maruhn, P.-G. Reinhard, P. D. Stevenson, and A. S. Umar, “The TDHF Code Sky3D”, Comp. Phys. Comm. 185, 2195 (2014). Does the new version supersede the previous version?: Yes. Nature of problem: The time-dependent Hartree–Fock equations can be used to simulate nuclear vibrations and collisions between nuclei for low energies. This code implements the equations based on a Skyrme energy functional and also allows the determination of the ground-state structure of nuclei through the static version of the equations. For the case of vibrations the principal aim is to calculate the excitation spectra by Fourier-analyzing the time dependence of suitable observables. In collisions, the formation of a neck between nuclei, the dissipation of energy from collective motion, processes like charge transfer and the approach to fusion are of principal interest. Solution method: The nucleonic wave function spinors are represented on a three-dimensional Cartesian mesh with no further symmetry restrictions. The boundary conditions are always periodic for the wave functions, while the Coulomb potential can also be calculated for an isolated charge distribution. All spatial derivatives are evaluated using the finite Fourier transform method. The code solves the static Hartree–Fock equations with a damped gradient iteration method and the time-dependent Hartree–Fock equations with an expansion of the time-development operator. Any number of initial nuclei can be placed into the mesh with arbitrary positions and initial velocities. Reasons for the new version: A few bugs were fixed and a number of enhancements added concerning faster convergence, better stability, and more sophisticated analysis of some results. Summary of revisions: The following is a brief summary. A more complete documentation can be found as update.pdf in the Documentation subdirectory. New documentation: It was decided to switch the documentation to using the Doxygen system (available from www.doxygen.org), which can generate the documentation in a variety of formats. To generate the documentation, go into the Doc-doxygen subdirectory and execute make html, make latex, or make all to produce the corresponding version or both of them. The documentation inserted into the source files accounts for most of the formal changes in them. The general documentation is also updated and present as “Documentation.pdf”. Bug fixes: 1.In the force database forces.data two digits were interchanged in the definition of SLy4d, leading to wrong results for that force.2.If a restart is done for a two-body collision, the code changed the number of fragments to nof =1. The restart is then initialized like a single-nucleus case with nof =1. But two-body analysis was activated only for nof =1 such that it was absent after restart.3.In the time-dependent mode, the wave functions were only save at intervals of mprint and mrest, respectively. If a calculation stops because of reaching the final distance or fulfilling the convergence criterion, this may lead to a loss of information, so that now both are done also in this event before the job finishes.4.The external field parameters were calculated directly from the input in getin_external. Since this is called before the fragment initialization is done, coefficients depending on proton or neutron number will not be calculated correctly. For this reason, the calculation of these coefficients is separated into a new routine init_external, which is called directly before the dynamic calculation starts. Array allocation: It turned out that having the working arrays as automatic variables could cause problems, as they are allocated on the stack and the proper stack size must be calculated. Therefore in all cases where a larger array is concerned, it is now changed to ALLOCATABLE and allocated and deallocated as necessary. Elimination of “guru” mode of FFTW3 While the guru mode as defined in the FFTW3 package (see fftw.org) offers an elegant formulation of complicated multidimensional transforms, it is not implemented in some support libraries like the Intel® MKL. There is not much loss in speed when this is replaced by standard transforms with some explicit loops added where necessary. This affects the wave function transforms in the y and z direction. Enhancement of the makefile In the previous version there were several versions of the makefile, which had to be edited by hand to use different compilers. This was reformulated using a more flexible file with various targets predefined. Thus to generate the executable code, it is sufficient to execute “make target” in the Code subdirectory, where target is one of the following: •seq : simple sequential version with the gfortran compiler.•ifort, ifort_seq : sequential version using the Intel compiler.•omp and ifort_omp produce the OpenMP version for the gfortran and Intel compiler, respectively.•mpi : MPI version, which uses the compiler mpif90.•mpi-omp : MPI version also using OpenMP on each node.•debug, seq_debug, omp_debug, mpi_debug : enable debugging mode for these cases. The first three use the gfortran compiler.•clean : removes the generated object and module files.•clean-exec : same as clean but removes the executable files as well.The generated executable files are called sky3d.seq, sky3d.ifort.seq, sky3d.mpi, sky3d.omp, sky3d.ifort.omp, and sky3d.mpi-omp, which should be self-explanatory. Thus several versions may be kept in the code directory, but a make clean should be done before producing a new version to make sure the object and module files are correct. Skyrme-force compatibility for static restart: the code normally checks that the Skyrme forces for all the input wave functions agree. It may be useful, however, to initialize a static calculation from results for a different Skyrme force. Therefore the consistency check was eliminated for the static case. Acceleration of the static calculations: The basic parameters for the static iterations are (see Eq. 12 of the original paper) x0 (variable x0dmp), which determines the size of the gradient step, and E0 (variable e0dmp) for the energy damping. These were read in and never changed throughout the calculation, except possibly through a restart. This can cause slow convergence, so that a method was developed to change x0dmp during the iterations. The value from the input is now regarded as the minimum allowed one and saved in x0dmpmin. At the start of the iterations, however, x0dmp is multiplied by 3 to attempt a faster convergence. The change in x0dmp is then implemented by comparing the HF energy ehf and the fluctuations efluct1 and efluct2 to the previous values saved in the variables ehfprev, efluct1prev, and efluct2prev. If the energy decreases or one of the fluctuations decreases by a factor of less than 1−10−5, x0dmp is increased by a factor 1.005 to further speed up convergence. If none of these conditions holds, it is assumed that the step was too large and x0dmp is reduced by a factor 0.8, but is never allowed to fall below x0dmpmin. This whole process is turned on only if the input variable tvaryx_0 in the namelist “static” is .TRUE. The default value is .FALSE. A speedup up to a factor of 3 has been observed. External field expectation value This value, which is printed in the file external.res, was calculated from the spatial field including the (time-independent) amplitude amplq0. The temporal Fourier transform then becomes quadratic in the amplitude, as the fluctuations in the density also grow linearly in amplq0 (provided the perturbation is not strong enough to take it into the nonlinear regime). This may be confusing and we therefore divided the expectation value by this factor. Note that if the external field is composed of a mixture of different multipoles (not coded presently), an overall scale factor should instead be used. Enhanced two-body analysis: the analysis of the final two-body quantities after breakup included directly in the code was very simplified and actually it was superfluous to do this so frequently. This is replaced by a much more thorough analysis, including determination of the internal angular momenta of the fragments and of a quite accurate Coulomb energy. It is done only when the final separation is reached, while a simple determination of whether the fragments have separated and, if so, what their distance is, is performed every time step. Diagonalization In the original program the diagonalization of the Hamiltonian in the subroutine diagstep was carried out employing an eigenvalue decomposition using the LAPACK routine ZHBEVD which is optimized for banded matrices. This routine is replaced in the update by the routine ZHEEVD which is optimized for general hermitian matrices. This change should result in a moderate speed up for very large calculations. Furthermore the array unitary, previously a nstmax × nstmax array has been reduced to a nlin × nlin array, where nlin is the number of wave functions of either neutrons or protons. This array is now used as input and output for ZHEEVD. New formulation of the spin–orbit term: The action of the spin–orbit term has been corrected to comply with a strictly variational form. Starting from the spin–orbit energy (1)Els=tls∫d3rJ→⋅∇ρ,we obtain by variation with respect to the s.p. wavefunction ψ∗ the spin–orbit term in the mean field in the symmetrized form (2)hˆlsψ=i2W→⋅(σ→×∇)ψ+σ→⋅(∇×(W→ψ))where W→=tls∇ρ. In the previous version of the code, this term was simplified by applying the product rule for the ∇ operator yielding (3)i2W→⋅(σ→×∇)ψ+σ→⋅(∇×(W→ψ))=iW→⋅(σ→×∇)ψ.Closer inspection reveals that the product rule is not perfectly fulfilled if the ∇ operator is evaluated with finite Fourier transformation as inevitably done in the grid representation of the code. It turned out that this slight mismatch can accumulate to instabilities in TDHF runs over long times. Thus the variationally correct form (2) has been implemented now, although it leads to slightly longer running times. Supplementary material: Extensive documentation and a number of utility programs to analyze the results and prepare them for graphics output using the Silo library (http://wci.llnl.gov/simulation/computer-codes/silo) for use in VisIT [1] or Paraview (https://www.paraview.org). The code can serve as a template for interfacing to other database or graphics systems. External routines/libraries: LAPACK, FFTW3. Restrictions: The reliability of the mean-field approximation is limited by the absence of hard nucleon–nucleon collisions. This limits the scope of applications to collision energies about a few MeV per nucleon above the Coulomb barrier and to relatively short interaction times. Similarly, some of the missing time-odd terms in the implementation of the Skyrme interaction may restrict the applications to even–even nuclei. Unusual features: The possibility of periodic boundary conditions and the highly flexible initialization make the code also suitable for astrophysical nuclear-matter applications. Acknowledgments This work was supported by DOE under contract numbers DE-SC0013847, DE-NA0002847, DE-SC0013365, and DE-SC0008511, by BMBF under contract number 05P15RDFN1, and by UK STFC under grant number ST/P005314/1. References [1]H. Childs, E. Brugger, B. Whitlock, J. Meredith, S. Ahern, D. Pugmire, K. Biagas, M. Miller, C. Harrison, G. H. Weber, H. Krishnan, T. Fogal, A. Sanderson, C. Garth, E. W. Bethel, D. Camp, O. Rübel, M. Durant, J. M. Favre, P. Navrátil, VisIt: An End-User Tool For Visualizing and Analyzing Very Large Data, in: High Performance Visualization–Enabling Extreme-Scale Scientific Insight, 2012, pp. 357–372."
}
@article{ULUER20163342,
title = "A framework for energy reduction in manufacturing process chains (E-MPC) and a case study from the Turkish household appliance industry",
journal = "Journal of Cleaner Production",
volume = "112",
pages = "3342 - 3360",
year = "2016",
issn = "0959-6526",
doi = "https://doi.org/10.1016/j.jclepro.2015.09.106",
url = "http://www.sciencedirect.com/science/article/pii/S0959652615013347",
author = "Muhtar Ural Uluer and Hakki Ozgur Unver and Gozde Gok and Nilgun Fescioglu-Unver and Sadik Engin Kilic",
keywords = "Energy efficiency, ISO/STEP 10303 AP224, Energy-aware design and process planning, Discrete event simulation",
abstract = "Energy is a major input in the manufacturing sector. Its security and efficiency are of supreme importance to a nation's industrial activities. Energy consumption also has serious environmental impacts in terms of Greenhouse Gas (GHG) emissions. In order to use energy more efficiently, simply designing parts and planning manufacturing processes with an energy-aware mindset is insufficient; it is also necessary to model and assess the energy efficiency of a process chain from a holistic point of view. In this work, we propose an integrated energy reduction framework and the internal methods to implement it. Our framework builds on three pillars. Creating an energy profile of a process chain is the first step in characterizing a manufacturing system in terms of energy demand. Energy-aware part designs and process plans are based on ISO/STEP 10303 AP224 standards in order to estimate the embodied energy of a mechanical part. Finally, using discrete event simulation methods, the energy consumption of a process chain is assessed and reduction scenarios are generated based on design or operational alternatives. A data collection and analytics system visualizing measures and key performance indicators (KPIs) also must be implemented in order to measure real consumption values and track improvement results over time. The energy reduction in manufacturing process chains (E-MPC) framework is unique in that it provides a structured method which enables the embodied energy of a part to be estimated during early design stages and further enables the evaluation of design impacts on process chains, thereby recognizing the dynamic nature of systems. A pilot case study of the framework was implemented at the largest household appliance manufacturer in Turkey, Arçelik A.Ş. In order to evaluate its usefulness and validity, we performed a detailed implementation on a fully automated crankshaft manufacturing line in Arçelik's refrigerator compressor plant. The results reveal that design improvements estimated gains would reach 2%, whereas operational improvements yield up to 10% energy savings per produced part."
}
@article{MONTEIRO20111473,
title = "Experimental study of syngas combustion at engine-like conditions in a rapid compression machine",
journal = "Experimental Thermal and Fluid Science",
volume = "35",
number = "7",
pages = "1473 - 1479",
year = "2011",
issn = "0894-1777",
doi = "https://doi.org/10.1016/j.expthermflusci.2011.06.006",
url = "http://www.sciencedirect.com/science/article/pii/S0894177711001300",
author = "Eliseu Monteiro and Julien Sotton and Marc Bellenoue and Nuno Afonso Moreira and Salvador Malheiro",
keywords = "Syngas, Rapid compression machine, Turbulence, Compression, Expansion",
abstract = "As the world energy demand and environmental concern continue to grow, syngas is expected to play an important role in future energy production. It represents a viable energy source, particularly for stationary power generation, since it allows for a wide flexibility in fossil fuel sources, and since most of the harmful contaminants and pollutants can be removed in the post-gasification process prior to combustion. In this work, two typical mixtures of H2, CO, CH4, CO2 and N2 have been considered as representative of the producer gas coming from wood gasification, and its turbulent combustion at engine-like conditions is made in a rapid compression machine in order to improve current knowledge and provide reference data for modeling and simulation of internal combustion engines. Methane as main constituent of the natural gas is used as reference fuel for comparison reasons. Single compression and compression- expansion events were performed as well direct light visualizations from chemiluminescence emission. There is an opposite behavior of the in-cylinder pressure between single compression and compression–expansion strokes. For single compression, peak pressure decreases as the ignition delay increases. In opposite, for compression–expansion the peak pressure increases as the ignition delay increases. This opposite behavior has to do with the combustion duration under constant volume conditions. Conclusion can be drawn that higher pressures are obtained with methane–air mixture in comparison to both typical syngas compositions. These results could be endorsed to the heat of reaction of the fuels, air to fuel ratio and burning velocity. Another major finding is that syngas typical compositions are characterized by high ignition timings due to its low burning velocities. This could compromise the use of typical syngas compositions on high rotation speed engines."
}
@article{ALGAYYIM20194729,
title = "Butanol–acetone mixture blended with cottonseed biodiesel: Spray characteristics evolution, combustion characteristics, engine performance and emission",
journal = "Proceedings of the Combustion Institute",
volume = "37",
number = "4",
pages = "4729 - 4739",
year = "2019",
issn = "1540-7489",
doi = "https://doi.org/10.1016/j.proci.2018.08.035",
url = "http://www.sciencedirect.com/science/article/pii/S1540748918305777",
author = "Sattar Jabbar Murad Algayyim and Andrew P. Wandel and Talal Yusaf and Saddam Al-Lwayzy",
keywords = "Butanol–acetone mixture, Biodiesel, Spray visualization, Diesel engine performance, Emissions",
abstract = "Increasing energy demands and more stringent legislation relating to pollutants such as nitrogen oxide (NOx) and carbon monoxide (CO) from fossil fuels have accelerated the use of biofuels such as biodiesel. However, current limitations of using biodiesel as an alternative fuel for CI engines include a higher viscosity and higher NOx emissions. This is a major issue that could be improved by blending biodiesel with alcohols. This paper investigates the effect of a butanol–acetone mixture (BA) as an additive blended with biodiesel to improve the latter's properties. Macroscopic spray characteristics (spray penetration, spray cone angle and spray volume) were measured in constant volume vessel (CVV) at two injection pressures. A high-speed camera was used to record spray images. The spray's edge was determined using an automatic threshold calculation algorithm to locate the spray outline (edge) from the binary images. In addition, an engine test was carried out experimentally on a single-cylinder diesel engine. The engine's performance was measured using in-cylinder pressure, brake power (BP) and specific fuel consumption (SFC). Emission characteristics NOx, CO and UHC were also measured. Neat biodiesel and three blends of biodiesel with up to 30% added BA were tested. The experimental data were analyzed via ANOVA to evaluate whether variations in parameters due to the different fuels were significant. The results showed that BA can enhance the spray characteristics of biodiesel by increasing both the spray penetration length and the contact surface area, thereby improving air–fuel mixing. The peak in-cylinder pressure for 30% BA was comparable to neat diesel and higher than that of neat biodiesel. Brake power (BP) was slightly improved for 10% BA at an engine speed of 2000 rpm while SFC was not significantly higher for any of the BA-biodiesel blends because of the smaller heating value of BA. Comparing the effect on emissions of adding BA to biodiesel, increasing the amount of BA reduced NOx and CO (7%) and (40%) respectively compared to neat biodiesel, but increased UHC."
}
@article{UDIAS2018467,
title = "A decision support tool to enhance agricultural growth in the Mékrou river basin (West Africa)",
journal = "Computers and Electronics in Agriculture",
volume = "154",
pages = "467 - 481",
year = "2018",
issn = "0168-1699",
doi = "https://doi.org/10.1016/j.compag.2018.09.037",
url = "http://www.sciencedirect.com/science/article/pii/S0168169918302734",
author = "Angel Udias and Marco Pastori and Céline Dondeynaz and Cesar {Carmona Moreno} and Abdou Ali and Luigi Cattaneo and Javier Cano",
keywords = "Multiobjective optimization, Food security, Best management practices, African agricultural growth, WEFE nexus",
abstract = "We describe in this paper the implementation of E-Water, an open software Decision Support System (DSS), designed to help local managers assess the Water Energy Food Environment (WEFE) nexus. E-Water aims at providing optimal management solutions to enhance food crop production at river basin level. The DSS was applied in the transboundary Mékrou river basin, shared among Benin, Burkina Faso and Niger. The primary sector for local economy in the region is agriculture, contributing significantly to income generation and job creation. Fostering the productivity of regional agricultural requires the intensification of farming practices, promoting additional inputs (mainly nutrient fertilizers and water irrigation) but, also, a more efficient allocation of cropland. In order to cope with the heterogeneity of data, and the analyses and issues required by the WEFE nexus approach, our DSS integrates the following modules: (1) the EPIC biophysical agricultural model; (2) a simplified regression metamodel, linking crop production with external inputs; (3) a linear programming and a multiobjective genetic algorithm optimization routines for finding efficient agricultural strategies; and (4) a user-friendly interface for input/output analysis and visualization. To test the main features of the DSS, we apply it to various real and hypothetical scenarios in the Mékrou river basin. The results obtained show how food unavailability due to insufficient local production could be reduced by, approximately, one third by enhancing the application and optimal distribution of fertilizers and irrigation. That would also affect the total income of the farming sector, eventually doubling it in the best case scenario. Furthermore, the combination of optimal agricultural strategies and modified optimal cropland allocation across the basin would bring additional moderate increases in food self-sufficiency, and more substantial gains in the total agricultural income. The proposed software framework proves to be effective, enabling decision makers to identify efficient and site-specific agronomic management strategies for nutrients and water. Such practices would augment crop productivity, which, in turn, would allow to cope with increasing future food demands, and find a balanced use of natural resources, also taking other economic sectors—like livestock, urban or energy—into account."
}
@article{NIEDERHAUSER2015293,
title = "New Innovative Solar Heating System (Cooling/Heating) Production",
journal = "Energy Procedia",
volume = "70",
pages = "293 - 299",
year = "2015",
note = "International Conference on Solar Heating and Cooling for Buildings and Industry, SHC 2014",
issn = "1876-6102",
doi = "https://doi.org/10.1016/j.egypro.2015.02.126",
url = "http://www.sciencedirect.com/science/article/pii/S1876610215002489",
author = "Elena-Lavinia Niederhäuser and Matthias Rouge and Antoine Delley and Harold Brülhart and Christian Tinguely",
keywords = "heat pump, photovoltaic panel, storage, solar heating system, solar cooling system, artificial intelligence, regulation, energyproduction, solar energy, simulation, economics",
abstract = "This paper presents thedevelopment of a compact and cheap solar heating system based on photovoltaic panels, heat pump and heating storage through a smart regulation from a technical and economical point of view. The heat pump can be standard and have its own regulation. The building's owner may interact with the regulation through a web service to visualize the monitoring of the installation, update the calendar of the presence and change the operation mode. One of the goals of the regulation is to optimize the consumption of his own electricity production and therefore minimize the electricity taken from the grid. Several means are developed to reach the main objective. On one hand, the regulation takes into account the weather forecast to anticipate the future heating's needs and the photovoltaic production. Weather forecast data allows also to optimize the storage in hot water tanks and in the building structure. On the other hand, the project includes the development of an electronic module which collects the monitoring data and gives the commands to the heat pump and the pumps. This is accomplished without any intrusion in the heat pump's own regulation and therefore the user keeps the constructor's warranty. The new regulation was validated numerically. The expected energy gain due to the weather forecast is about 10%, while the one due to the self-consumption and the heat storage management will bring another 10-15%."
}
@article{KORTENBRUCK2017227,
title = "Machine operation profiles generated from ISO 11783 communication data",
journal = "Computers and Electronics in Agriculture",
volume = "140",
pages = "227 - 236",
year = "2017",
issn = "0168-1699",
doi = "https://doi.org/10.1016/j.compag.2017.05.039",
url = "http://www.sciencedirect.com/science/article/pii/S0168169916311589",
author = "Dietrich Kortenbruck and Hans W. Griepentrog and Dimitris S. Paraforos",
keywords = "Machinery use, ISO 11783, Operation profile, CAN, State model, Process optimization",
abstract = "An operation profile is a detailed description of machinery use and provides information about the production process (e.g. time and energy requirements). The high complexity of agricultural machine use over time with its various implements and production processes makes it difficult to automate the operation profile generation for agricultural machinery. Today, modern communication interfaces, like the ISO 11783 (ISOBUS), allow a comfortable data acquisition with comprehensive parameter information of the machine and production process status. This paper describes how the generation of operation profiles can be automated and what kind of machine data and status information are required. Experiments were conducted with a tractor-machine combination during field cultivation. The machine was equipped with a CAN data logger to record ISOBUS messages and GNSS position data during normal operation. A software tool was setup and programmed to drive the data logger. Fields were automatically detected by algorithms to avoid the time consuming manual definition of field boundaries. A graphical user interface simplified the setup and use of the software application. Furthermore, the algorithms analyzed the machine communication regarding the actual machine state based on position, machine activity and work state. The KTBL time classification 2013 was used as a basis for working time analysis as it is designed to divide the overall operational time of agricultural machines into clearly defined, qualitatively different partial time fractions. The machine state could be visualized in the software by a map to show the user where problems occurred e.g. in time losses while waiting or machine breakdown. A diagram presented the timespans spent in different time fractions for comparison to other, similar operations. Combined with other infrastructure data, specific operation profiles could be used to gain more detailed knowledge about machine use or to improve future operations."
}
@article{CARVALHO2015278,
title = "Laser-induced breakdown spectroscopy (LIBS) combined with hyperspectral imaging for the evaluation of printed circuit board composition",
journal = "Talanta",
volume = "134",
pages = "278 - 283",
year = "2015",
issn = "0039-9140",
doi = "https://doi.org/10.1016/j.talanta.2014.11.019",
url = "http://www.sciencedirect.com/science/article/pii/S0039914014009114",
author = "Rodrigo R.V. Carvalho and Jomarc A.O. Coelho and Jozemir M. Santos and Francisco W.B. Aquino and Renato L. Carneiro and Edenir R. Pereira-Filho",
keywords = "Laser-induced breakdown spectroscopy, Waste electrical and electronic equipment, Printed circuit board, Chemometrics, Mobile phones, Scanning electron microscopy with energy-dispersive X-ray spectroscopy",
abstract = "In this study, laser-induced breakdown spectroscopy (LIBS) was combined with chemometric strategies (PCA, Principal Component Analysis) and scanning electron microscopy with energy-dispersive X-ray spectroscopy (SEM–EDS) to investigate the metal composition of a printed circuit board (PCB) sample from a mobile phone. Scanning electron microscopy–EDS was used for two main reasons: it was possible at the same time to visualize the sample surface, craters (made by the laser pulses) and also the chemical composition of the samples. A 30mm×40mm area of the mobile phone PCB sample, which was manufactured in 2011, was investigated. In this case, a matrix with 30 rows and 40 columns (1200 points) was analyzed, and 10 pulses were performed at each point. A total of 12,000 emission spectra were recorded in the wavelength range from 186 to 1040nm. After an initial exploratory investigation using PCA, 18 emission lines were selected (representing the elements Al, Au, Ba, Ca, Co, Cu, Fe, K, Li, Mg, Mn, Na, Ni, Sb, Si, Sn, Ti and Zn) and then normalized by the relative intensities, and a new PCA was calculated with the autoscaled data. For example, Au and Si were mainly observed in the superficial electrical contacts and in the bulk of the PCB, respectively. A second sample (a mouse PCB) was also analyzed and Pb (emission lines 357.273, 363.956, 368.346, 373.994 and 405.780nm) was identified in the solders. In addition, this element was determined using FAAS (flame atomic absorption spectrometry) and the Pb concentration was around 25% (w/w). This study opens the possibility for improved recycling processes and the chemical investigation of solid samples measuring a few millimeters in dimension without sample preparation."
}
@article{SYMONS2018509,
title = "Quarter-millimeter spectral coronary stent imaging with photon-counting CT: Initial experience",
journal = "Journal of Cardiovascular Computed Tomography",
volume = "12",
number = "6",
pages = "509 - 515",
year = "2018",
issn = "1934-5925",
doi = "https://doi.org/10.1016/j.jcct.2018.10.008",
url = "http://www.sciencedirect.com/science/article/pii/S193459251830426X",
author = "Rolf Symons and Yves {De Bruecker} and John Roosen and Laurent {Van Camp} and Tyler E. Cork and Steffen Kappler and Stefan Ulzheimer and Veit Sandfort and David A. Bluemke and Amir Pourmorteza",
abstract = "Purpose
To evaluate the performance and clinical feasibility of 0.25 mm resolution mode of a dual-energy photon-counting detector (PCD) computed tomography (CT) system for coronary stent imaging and to compare the results to state-of-the-art dual-energy energy-integrating detector (EID) CT.
Materials and methods
Coronary stents with different diameters (2.0–4.0 mm) were examined inside a coronary artery phantom consisting of plastic tubes filled with iodine-based and gadolinium-based contrast material diluted to approximate clinical concentrations (n = 18). EID images were acquired using 2nd and 3rd generation dual-source CT systems (SOMATOM Flash and SOMATOM Force, Siemens Healthcare) at 0.60 mm (defined as standard-resolution (SR)) isotropic voxel size. Radiation-dose matched PCD images were acquired using a human prototype PCD system (Siemens Healthcare) at 0.50 mm (SR) and 0.25 mm (HR) imaging modes. Images were reconstructed using optimized convolution kernels.
Results
Dual-energy HR PCD images significantly better stent lumen visualization (median: 69.5%, IQR: 61.2–78.9%) over dual-energy EID, and standard-resolution PCD images (median: 53.2–57.4%, all P < 0.01). HR PCD acquisitions reconstructed at SR image voxel size showed 25.3% lower image noise compared to SR PCD acquisitions (P < 0.001). High-resolution iodine and gadolinium maps, as well as virtual monoenergetic images, were calculated from the PCD data and enabled estimation of contrast agent concentration in the lumen without interference from the coronary stent.
Conclusion
HR spectral PCD imaging significantly improves coronary stent lumen visibility over dual-energy EID. When the PCD-HR data was reconstructed into standard voxel sizes (0.5 mm isotropic) the image noise decreased by 25% compared to SR acquisition of PCD. Both dual-energy systems were consistent in estimating contrast agent concentrations."
}
@article{LE201291,
title = "A graphical user interface for numerical modeling of acclimation responses of vegetation to climate change",
journal = "Computers & Geosciences",
volume = "49",
pages = "91 - 101",
year = "2012",
issn = "0098-3004",
doi = "https://doi.org/10.1016/j.cageo.2012.07.007",
url = "http://www.sciencedirect.com/science/article/pii/S0098300412002324",
author = "Phong V.V. Le and Praveen Kumar and Darren T. Drewry and Juan C. Quijano",
keywords = "Modeling, Multilayer canopy, MATLAB, Ecohydrology, Vegetation",
abstract = "Ecophysiological models that vertically resolve vegetation canopy states are becoming a powerful tool for studying the exchange of mass, energy, and momentum between the land surface and the atmosphere. A mechanistic multilayer canopy–soil–root system model (MLCan) developed by Drewry et al. (2010a) has been used to capture the emergent vegetation responses to elevated atmospheric CO2 for both C3 and C4 plants under various climate conditions. However, processing input data and setting up such a model can be time-consuming and error-prone. In this paper, a graphical user interface that has been developed for MLCan is presented. The design of this interface aims to provide visualization capabilities and interactive support for processing input meteorological forcing data and vegetation parameter values to facilitate the use of this model. In addition, the interface also provides graphical tools for analyzing the forcing data and simulated numerical results. The model and its interface are both written in the MATLAB programming language. Finally, an application of this model package for capturing the ecohydrological responses of three bioenergy crops (maize, miscanthus, and switchgrass) to local environmental drivers at two different sites in the Midwestern United States is presented."
}
@article{HALKOS2017140,
title = "Climate change effects and their interactions: An analysis aiming at policy implications",
journal = "Economic Analysis and Policy",
volume = "53",
pages = "140 - 146",
year = "2017",
issn = "0313-5926",
doi = "https://doi.org/10.1016/j.eap.2017.01.005",
url = "http://www.sciencedirect.com/science/article/pii/S031359261630217X",
author = "George E. Halkos and Kyriaki D. Tsilika",
keywords = "Graph theory, Node centrality, Mathematica, Climate related factors, Environmental economics computation",
abstract = "In this study we provide a computerized graph structure for synthesizing and displaying the data on a region’s ecosystem-economic system. By applying Mathematica-based graph modeling we create a causal network of the synergistic impact mechanism among certain climate related factors. Our computational approach identifies a climate factor that affects most immediately or most strongly the others. Important factors are indicated through the use of graph theoretical tools. Our graph-based approach and its computational aspects allow for factor ranking(s) according to their importance to the network both numerically and visually, for certain settlement types. Our contribution provides quantitative estimates of impacts and adaptation potentials of five potential effects of climate change (migration, flooding-landslides-fire, air and water pollution, human health and energy-water-other resources) which play a substantial role at the synergistic impact mechanism. By using graph visualization techniques, the structure of the synergistic impact mechanism is self-evident. Specifically, graph layouts are created to detect i) the causal relationships of the synergistic mechanism under study ii) the most influential factor(s) in the synergistic mechanism and iii) classify the factor’s roles (based on the degree of their impact) within the coping mechanism. Highlighting graph elements let information for policy implications stand out."
}
@article{LARIONOV2010127,
title = "Spatially pathogenic forms of tau detected in Alzheimer's disease brain tissue by fluorescence lifetime-based Förster resonance energy transfer",
journal = "Journal of Neuroscience Methods",
volume = "192",
number = "1",
pages = "127 - 137",
year = "2010",
issn = "0165-0270",
doi = "https://doi.org/10.1016/j.jneumeth.2010.07.021",
url = "http://www.sciencedirect.com/science/article/pii/S0165027010003961",
author = "Sergey Larionov and Przemyslaw Wielgat and Yiner Wang and Dietmar Rudolf Thal and Harald Neumann",
keywords = "Alzheimer disease, Tau, Immunohistology, FRET, FLIM",
abstract = "In tauopathies including Alzheimer's disease (AD) tau molecules have lost their normal spatial distance to each other and appear in oligomeric or aggregated forms. Conventional immunostaining methods allow detection of abnormally phosphorylated or conformationally altered aggregated tau proteins, but fail to visualize oligomeric forms of tau. Here we show that tau molecules that lost their normal spatial localization can be detected on a subcellular level in postmortem central nervous system (CNS) tissue sections of AD patients by fluorescence lifetime-based Förster resonance energy transfer (FRET). Paraffin sections were co-immunostained with two tau-specific monoclonal antibodies recognizing the same epitope, but labeled with distinct fluorescence dyes suitable for spatial resolution at a nanometer scale by lifetime-based FRET. A FRET signal was detected in neuritic plaques and neurofibrillary tangles of CNS tissue sections of AD patients, showing associated tau proteins typically reflecting either fibrillary, oligomeric or aggregated tau. The ‘pretangle-like’ structures within the neuronal perikarya did not contain spatially pathogenic forms of tau accordingly to this method. Data demonstrate that fluorescence lifetime-based FRET can be applied to human brain tissue sections to detect pathogenic forms of tau molecules that lost their normal spatial distance."
}
@article{CREUTZNACHER2016171,
title = "The Transformable Factory: Adapting Automotive Production Capacities",
journal = "Procedia CIRP",
volume = "41",
pages = "171 - 176",
year = "2016",
note = "Research and Innovation in Manufacturing: Key Enabling Technologies for the Factories of the Future - Proceedings of the 48th CIRP Conference on Manufacturing Systems",
issn = "2212-8271",
doi = "https://doi.org/10.1016/j.procir.2015.12.138",
url = "http://www.sciencedirect.com/science/article/pii/S221282711600010X",
author = "Thomas Creutznacher and Ulrich Berger and Raffaello Lepratti and Steffen Lamparter",
keywords = "Factory, Flexibility, Simulation, Production capacity",
abstract = "In the complex field of fast changing market conditions and decreasing predictability of market development, a factory has to adapt its production capacities quickly and with minimal effort. This paper introduces a novel methodology, recommending the best method of capacity planning to change production volumes and to ensure optimal operation in a car factory with respect to several Key Performance Indicators (KPIs), e.g. OEE or energy efficiency. The plant's volume transformability will be increased by not only one, but rather many different possible responses to be analyzed by simulation: the so-called production variants. An algorithm determines the most dominant, pre-analyzed production variants from a data base and visualizes the responding strategies based on the predicted KPIs. After a user- and scenario-based weighting by a so-called Balanced Performance Indicator (BPI), an optimal production variant will be selected. Above all, this assistance system provides more variety and transparency for adapting capacities and supports the factory planning both for greenfields by improved testing und comparability of production variants and for brownfields by giving recommendations for action during run-time. Additionally, real measured data can be retrieved in a closed-loop into the data base to design a continuous learning system. The concept has been methodically implemented and validated in a virtual material flow simulation of an automotive body shop cell and is linked to a real demonstration facility at the Brandenburg University of Cottbus-Senftenberg. Test runs confirm significant saving potential in energy consumption and OEE losses for the given target capacity."
}
@article{FARINA2020,
title = "Removal of water binding proteins from dentin increases the adhesion strength of low-hydrophilicity dental resins",
journal = "Dental Materials",
year = "2020",
issn = "0109-5641",
doi = "https://doi.org/10.1016/j.dental.2020.07.004",
url = "http://www.sciencedirect.com/science/article/pii/S0109564120301913",
author = "Ana Paula Farina and Doglas Cecchin and Cristina M.P. Vidal and Ariene Arcas Leme-Kraus and Ana K. Bedran-Russo",
keywords = "Dentin, Proteoglycans, Wettability, Swelling, Adhesion",
abstract = "Objectives
To investigate the role of proteoglycans (PGs) on the physical properties of the dentin matrix and the bond strength of methacrylate resins with varying hydrophilicities.
Methods
Dentin were obtained from crowns of human molars. Enzymatic removal of PGs followed a standard protocol using 1 mg/mL trypsin (Try) for 24 h. Controls were incubated in ammonium bicarbonate buffer. Removal of PGs was assessed by visualization of glycosaminoglycan chains (GAGs) in dentin under transmission electron microscopy (TEM). The dentin matrix swelling ratio was estimated using fully demineralized dentin. Dentin wettability was assessed on wet, dry and re-wetted dentin surfaces through water contact angle measurements. Microtensile bond strength test (TBS) was performed with experimental adhesives containing 6% HEMA (H6) and 18% HEMA (H18) and a commercial dental adhesive. Data were statistically analyzed using ANOVA and post-hoc tests (α = 0.05).
Results
The enzymatic removal of PGs was confirmed by the absence and fragmentation of GAGs. There was statistically significant difference between the swelling ratio of Try-treated and control dentin (p < 0.001). Significantly lower contact angle was found for Try-treated on wet and dry dentin (p < 0.002). The contact angle on re-wet dentin was not recovered in Try-treated group (p = 0.9). Removal of PGs significantly improved the TBS of H6 (109% higher, p < 0.001) and H18 (29% higher, p = 0.002) when compared to control. The TBS of commercial adhesive was not affected by trypsin treatment (p = 0.9).
Significance
Changing the surface energy of dentin by PGs removal improved resin adhesion, likely due to more efficient water displacement, aiding to improved resin infiltration and polymerization."
}
@article{PROKHOROV2011715,
title = "Detection of internal cracks and ultrasound characterization of nanostructured Bi2Te3-based thermoelectrics via acoustic microscopy",
journal = "Ultrasonics",
volume = "51",
number = "6",
pages = "715 - 718",
year = "2011",
issn = "0041-624X",
doi = "https://doi.org/10.1016/j.ultras.2011.02.005",
url = "http://www.sciencedirect.com/science/article/pii/S0041624X11000291",
author = "V.M. Prokhorov and G.I. Pivovarov",
keywords = "Acoustic microscopy, Ultrasound characterization, Nanostructured materials, BiTe-based thermoelectric alloys, Elastic properties",
abstract = "The search for thermoelectric (TE) materials for highly efficient devices aims at improving the TE efficiency and broadening their areas of applications. We created nanostructured thermoelectric Bi–Sb–Te-family materials by high energy (ball milling) pre-treatment of the parent materials followed by high-pressure/high-temperature treatment. Bi0.5Sb1.5Te3 compositions with the superfluous maintenance of tellurium was used for the synthesis of the samples with p-type electrical conductivity. Acoustic microscopy was used to study elastic properties and bulk irregularities and to detection of internal cracks both in the parent materials and in the created nanostructured samples. The data has been used for optimization of parameters of synthesis of nanostructured thermoelectrics."
}